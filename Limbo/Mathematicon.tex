\catcode`@=11\relax
\input LD@Header.tex
\input LD@Library.tex
\input LD@Typesetting.tex
\input LD@Exercices.tex
\def\Inferno{}

\def\LD@Exercice@Display@Code{\eightpts}%
\LD@Exo@Label@Hide
\LD@Colors@Hide
\overfullrule=0pt%

	
\ifHtml
	\input LD@Html
	\tikzstyle{every picture}=[tex4ht node/escape=true,tex4ht node css=Mathematicon]%
	\font\LD@Font@ResMathematica=cmr10%
\else
	\LD@AFour@Book
	\font\LD@Font@ResMathematica="Caesar" at 9pt
	\font\LD@Font@Gothic="Metal Macabre"\relax
	\font\LD@Font@Sanctuary="Sanctuary"\relax
	\font\LD@Font@Inferno="Gypsy Curse"\relax
\fi


\def\pgfutil@EveryShipout@Output{%
  \setbox255=\vbox{%
    \setbox0=\hbox{\pgfutil@abe\pgfutil@abc\global\let\pgfutil@abc\pgfutil@empty}%
    \wd0=0pt%
    \ht0=0pt%
    \dp0=0pt%
    \box0%
    \makeheadline
    \unvbox255%
    \makefootline
  }%
  \pgfutil@@EveryShipout@Org@Shipout\box\@cclv%
}


\def\LD@Public@Star{PT*}%

\def\Red#1{#1}%%%% Fix this !
\def\Blue#1{#1}%
\def\pspicture#1\endpspicture{}%
\long\def\IGNORE#1\IGNORE{}%

\font\SvgText=cmr10\relax


\tenrm

\def\TipSection{}%
\def\Tip#1{\ifHtml\Par\fi\EA\sidx\EA{\TipSection!#1}}%

%%%% Index
\def\LD@Index@List{%
	Théorèmes,Théorème,Formules,Formule,Propriétés,Propriété,%
	Inégalités,Inégalité,Identités,Identité,Inégalités,Inégalité,%
	Distance,Intersection,Raisonnement,Récurrence,Relations,Relation,Point,%
	Lien,Espaces vectoriels,Espace vectoriel,Suites,Interprétation,%
	Fonctions,Produit,Loi,Déterminant,Principe,%
	Opérateur logique,Equation,Aire,Volume,Changement%
}%

%% Styles
\tikzstyle{Tableau Operateur}= [matrix of nodes,ampersand replacement=\&,nodes={draw,text width=2cm,text height=10pt,inner ysep=3pt},column sep={2cm,between origins},row sep={16pt,between origins}]
\tikzstyle{Nested Picture}=[rectangle,rounded corners=0pt,anchor=center,text centered,text width=2cm, text height=10pt,font=\SvgText,inner xsep=0pt,inner ysep=4pt]




\def\imageFolder{/media/Main/TeX/Images}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																%
%							Res Mathematica		  				%
%																%
%																%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nopagenumber
\newdimen\LD@dimena\LD@dimena=18cm\relax\advance\LD@dimena by-1cm\relax
\newdimen\LD@dimenb\LD@dimenb=28cm\relax\advance\LD@dimenb by-4cm\relax
 %
\centerline{%
	%\unless\ifHtml\font\SvgText=Papyrus\relax\fi
	\tikzpicture
	\ifHtml 
		\node[text height=2\LD@dimenb,text width=2\LD@dimenb,text only=false] (a) {\noindent\smash{\Image[Height=20\LD@dimenb,Width=20\LD@dimenb]{\imageFolder/Hekathomb4gray.jpg}}}%
	\else
			\node[text width=\LD@dimena,text height=\LD@dimenb%,draw,line width=2pt,inner sep=0pt
	] (a) at (0,-2) {\noindent\smash{\Image[Width=\LD@dimena,Height=\LD@dimenb]{\imageFolder/Hekathomb4gray.jpg}}}%
	\fi
node [below=0.7cm,inner sep=1pt] (b) at (a.north) {};
\node [inner sep=1.5pt,above=2.7cm] (c) at (a.south) {};
\node [inner sep=1pt,below=1cm] (d) at (c) {};
\node [inner sep=1pt,below=1.5cm] (e) at (c) {};
\foreach\angle in {0,15,...,360}
{%
\node[color=black,scale=1.5] at (b.\angle) {\LD@Font@Gothic Olus Livius Bindus};
\node[color=black,scale=5] at (c.\angle) {\unless\ifHtml\LD@Font@ResMathematica\fi Res Mathematica}; 
\node[color=black, scale=1.5] at (d.\angle) {Malédictions, charmes, maléfices et sortilèges};
\node[color=black, scale=1.5] at (e.\angle) {à l'usage de l'apprenti-mathématicien};
}%
\node[color=gray,scale=1.5] at (b) {\LD@Font@Gothic Olus Livius Bindus};
\node[color=gray,scale=5] at (c) {\unless\ifHtml\LD@Font@ResMathematica\fi Res Mathematica};
\node[color=gray, scale=1.5] at (d) {Malédictions, charmes, maléfices et sortilèges};
\node[color=gray, scale=1.5] at (e) {à l'usage de l'apprenti-mathématicien};
\endtikzpicture}%
\eject
\headline={\ifpagetitre\the\hautpagetitre\else\ifodd\pageno\the\hautpagedroite\else\the\hautpagegauche\fi\fi }
\footline={\ifpagetitre\the\baspagetitre\else\ifodd\pageno\the\baspagedroite\else\the\baspagegauche\fi\fi \global\pagetitrefalse}
\hautpagegauche={\ifMidFolio\the\hautpagemilieu\else\tenrm\folio\hfil\the\titrecourant\hfil\fi}



\hautspages{}{}%

\iffalse

\background{Black}
\null\vfill

\centerline{%
\tikzpicture
\node [inner sep=2pt] (a) {};
\node[below of=a,node distance =7cm] (b) {};
\node [inner sep=2pt,below of=b,node distance=7cm] (c) {};
\node [inner sep=2pt,below of=c,node distance=1cm] (d) {};
\node [inner sep=2pt,below of=d,node distance=2cm] (e) {};
\foreach\angle in {0,15,...,360}
{%
\node[color=black,node distance=0.1cm,scale=5] at (a.\angle) {Res Mathematica}; 
\node[color=black, scale=2] at (c.\angle) {Malédictions, charmes, maléfices et sortilèges};
\node[color=black,scale=2] at (d.\angle) {à l'usage de l'apprenti-mathématicien};
\node[color=black,scale=3] at (e.\angle) {par Olus Livius Bindus};
}%
\node[color=red,scale=5] at (a) {Res Mathematica};
\node[color=red, scale=2] at (c) {Malédictions, charmes, maléfices et sortilèges};
\node[color=red,scale=2] at (d) {à l'usage de l'apprenti-mathématicien};
\node[color=red,scale=3] at (e) {par Olus Livius Bindus};
% Lucifer's Star
\draw [line width=4ex,draw=black] (b) circle (5.2cm) ;
\draw [line width=3ex,draw=red] (b) circle (5.2cm) ;
\draw [line width=3.5ex,draw=black] (b) +(-90:4.55cm) -- +(54:4.55cm) -- +(198:4.55cm) -- +(342:4.55cm) -- +(486:4.55cm) -- cycle ;
\draw [line width=2.5ex,draw=red] (b) +(-90:4.55cm) -- +(54:4.55cm) -- +(198:4.55cm) -- +(342:4.55cm) -- +(486:4.55cm) -- cycle ;
\endtikzpicture}%
\def\hautspages#1#2{\auteurcourant={\ninepcap#1}\titrecourant={\nineit#2\qquad\the\pageno}\MidFoliofalse}

%\centerline{
%\ifHtml\Picture+{Luciephel.jpg align="center"}\fi
%\centerline{\epsfbox{Mathematicon.eps}}
%\ifHtml\EndPicture\fi
%}

\vfill\null
\eject\background{White}
\fi

\hautspages{\LD@Font@Gothic Olus Livius Bindus}{Table des matières}%

\centerline{\seventeenbf Table des matières}
\bigskip
\bigskip
%\def\tocchapterentry#1#2{\line{\bf #1}}
%\def\tocsectionentry#1#2{\line{\quad\HCode{&nbsp;&nbsp;}\sl #1}}
%\def\tocsubsectionentry#1#2{\line{\qquad\HCode{&nbsp;&nbsp;&nbsp;&nbsp;}\rm #1}}
	\definecontentsfile{toc}%
	\definecontentsfile{tac}%
	\let\tacbookentry\tocbookentry
	\let\tacchapterentry\tocchapterentry
	\let\tacsectionentry\tocsectionentry
	\let\tacsubsectionentry\tocsubsectionentry
\ifHtml
	\EndP
	\HCode{<div class="toc left" style="float:left;">}%
	\readcontentsfile{toc}
	\HCode{</div><div class="toc left" style="float:left;">}%
	\readcontentsfile{tac}
	\HCode{</div>}%
	\par
\else
	\readcontentsfile{toc}
\fi


\vfill\null
\eject

                                                    \ifHtml   \input LD@Output@Xhtml\fi
                          
%\hautspages{Olus Livius Bindus}{Introduction}

%\pagetitrefalse
\iffalse\Chapter Conv, Introduction.

Ce recueil constitue la première esquisse d'un cours illustré de mathématiques, 
couvrant le programme de la première années de préparation aux concours d'écoles d'ingénieurs 
et~s'addressant esssentiellement aux étudiants en section PTSI. 
\bigskip

Plusieurs constatations s'imposent : les livres traitant du sujet pré-cité abondent dans la~littérature 
et brillent par leur clarté ({\it ahahah}), leur précision, leur exhaustivité, etc...
Constatant qu'il serait vain et prétentieux de chercher à concurencer ces illustres ouvrages, 
l'auteur s'est fixé des objectifs plus modestes : 
\medskip

\Bullet Présenter de fa\c con précise, claire, concise et illustrée les concepts au programme. 
\medskip

\Bullet Distinguer les concepts importants de ceux qui le sont moins à l'aide de couleurs, 
les~concepts fondamentaux apparaissant en coloris plus foncés. 
Le critère retenu pour juger de l'importance est l'utilité, la fréquence d'utilisation en exercice et en concours. 
\bigskip
\halign{
#&\qquad#\cr
& Définition.\cr
& Quantification, hypothèse.\hfill \cr
& Identité, propriété, théorème.\hfill\cr
& Méthode classique, technique à retenir. \hfill\cr
}
\bigskip

\Bullet Prévenir le lecteur de certains dangers et lui signaler les exercices, selon leur importance. 

\bigskip
\halign{
#\hfill&\ #\hfill&\qquad #\hfill\cr
 & & Danger, risque de confusion, piège à éviter. \cr
 & & $\vcenter{\hsize=30em\noindent Exercice d'application, exercice classique, de difficulté proportionnelle au nombre de bestioles.}$\vbox to2em{}\cr
}
\bigskip

\Bullet Amuser le lecteur à l'aide de dessins bizarres et de blagues de mauvais gout pour lui donner envie de lire {\it Res Mathematica} 
et de faire des mathématiques. Pour atteindre cet~objectif, l'auteur compte utiliser $\underline{\mbox{toutes}}$ les armes à sa disposition et ne reculera devant rien (il vendrait même son âme... si cela n'était déjà fait). 
Vous voilà prévenus... 
\bigskip
\vfill
\noindent $\underline{\mbox{Pour information}}$, {\it Res Mathematica} a été élaboré dans sa totalité à l'aide du traitement de texte scientifique \TeX{} et de ses 
extensions Eplain, Epsf et Pstricks. De plus, les logiciels Mathematica, Open Office et Ghostview ont été utilisés pour
l'élaboration des graphiques, l'export de dingbats ({\it I love Dingbats !}) et les conversions vers les formats Ps, Eps et Pdf. 

%\LD@Toc@Insert{\centerline{PTSI}\medskip}%

\fi
%\Book PTSI, {Math. Sup.}.

\hautspages{Olus Livius Bindus}{Logique binaire}%

\Chapter Logique, Logique binaire. 

\Concept Assertion


\Definition 
Une assertion est une affirmation élémentaire. 

\Exemple
\Item{ a. }\qquad\Eqdef{Logique_Assertion_1} ``un et un font deux''.
\Item{ b. }\qquad\Eqdef{Logique_Assertion_2} ``Le carré d'un nombre réel est strictement positif.
\Item{ c. }\qquad\Eqdef{Logique_Assertion_3} ``Cette proposition est vraie''. 
\Item{d. }\qquad\Eqdef{Logique_Assertion_4} ``Cette proposition est fausse''. 

\Concept Proposition logique

\Definition
Une proposition logique est une affirmation élémentaire à laquelle peut être attribuée soit la valeur vraie, soit la valeur fausse. 

\Remarque : s'il est impossible d'attribuer soit la valeur vraie, soit la valeur fausse à une assertion, on dit qu'elle est indécidable. 
\bigskip

\Exemple
\Item{ a. } L'assertion \eqref{Logique_Assertion_1} est une proposition logique vraie. 
\Item{ b. } L'assertion \eqref{Logique_Assertion_2} est une proposition logique fausse, le carré de $0$ n'étant pas strictement positif.
\Item{ c. } L'assertion \eqref{Logique_Assertion_3} n'est pas une proposition logique. Elle est indécidable car  elle  est à la fois vraie et fausse. 
\Item{ d. } L'assertion \eqref{Logique_Assertion_4} n'est pas une proposition logique. Elle est indécidable car  elle  n'est ni vraie, ni fausse. 

\Remarque : On s'intéressera uniquement aux propositions logiques car les assertions indécidables 
sont certainement surprenantes et amusantes mais elles n'ont quasiment aucune utilité pratique. 

\Section CPPplan, Opérateurs logiques. 

\Concept [Title=Opérateur logique ``non''] Opérateur logique ``non'' (de négation)

\Definition
Nier une proposition logique $\sc P$, c'est affirmer son contraire. 

\Remarque : la négation d'une proposition logique $\sc P$ se prononce ``non \sc P'' et se note $\overline{\sc P}$ ou ``non \sc P''. 
\bigskip

\Exemple
\Item{ a. } La négation de \eqref{Logique_Assertion_1} est la proposition ``un et un ne font pas deux''. 
\Item{ b. } La négation de \eqref{Logique_Assertion_2} est la proposition ``Le carré d'un nombre réel n'est pas strictement positif". 

\Definition 
La table de vérité de l'opérateur logique non est le tableau 
\Par
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		$\sc P$ \& Vrai \& Faux\\
		$\overline{\sc P}$ \& Faux \& Vrai \\
	};
	\endtikzpicture
}%

\Propriete 
On ne change pas une proposition en lui appliquant deux fois l'opérateur~de négation 
$$
\mbox{non~}(\mbox{non~}\sc P)=\sc P
$$

\Concept Opérateur logique ``et''

\Definition [$\sc P$ et $\sc Q$ propositions logiques]
$$
\mbox{La proposition ``}\sc P\sbox{ et }\sc Q\mbox{'' est dite }\Q\{\eqalign{&\sbox{vraie si les deux propositions sont vraies}\cr&
\sbox{fausse si l'une au moins des propositions est fausse}}\W.
$$ 

\Exemples. La proposition ($1+1=2$ et $2+2=4$) est vraie alors que ($1> 2$ et $2>0$) est une proposition fausse. 

\Definition La table de vérité de l'opérateur logique ``et'' est le tableau
\Par
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		|[draw=none]	|\& $\sc Q$ Vrai \& $\sc Q$ Faux \\
		$\sc P$ Vrai \& Vrai \& Faux\\
		$\sc P$ Faux \& Faux \& Faux \\
	};
	\draw (tableau-2-1.150) --  (tableau-1-2.west) node [sloped,pos=0.5,above,inner sep=1pt] (t) {$\sc P$ et $\sc Q$};
	\endtikzpicture
}%

\Concept [Title=Opérateur logique ``ou''] Opérateur logique ``ou'' (ou inclusif)

\Definition [$\sc P$ et $\sc Q$ propositions logiques]
$$
\mbox{La proposition ``}\sc P\sbox{ ou }\sc Q\mbox{'' est dite }\Q\{\eqalign{&\sbox{vraie si l'une au moins des propositions est vraie}\cr&
\sbox{fausse si les deux propositions sont fausses}}\W.
$$ 

\Exemple. La proposition $1+1=2$ ou $2+2=5$ est vraie alors que ($1> 1$ ou $0>2$) est une proposition fausse. 
\bigskip

\Definition La table de vérité de l'opérateur logique ``ou'' est le tableau
\PAR
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		|[draw=none]	|\& $\sc Q$ Vrai \& $\sc Q$ Faux \\
		$\sc P$ Vrai  \& Vrai \& Vrai \\
		$\sc P$ Faux \& Vrai \& Faux\\
	};
	\draw (tableau-2-1.150) --  (tableau-1-2.west) node [sloped,pos=0.5,above,inner sep=1pt] (t) {$\sc P$ ou $\sc Q$};
	\endtikzpicture
}%


\Propriete [$\sc P$ et $\sc Q$ deux propositions logiques] 
$$
\eqalign{
&\overline{\sc P\sbox{ et }\sc Q}=\overline{\sc P}\sbox{ ou }\overline{\sc Q}\cr
&\overline{\sc P\sbox{ ou }\sc Q}=\overline{\sc P} \sbox{ et }\overline{\sc Q}\cr
}
$$

\Exemple. Si les solutions réelles $x$ d'une équation $E$ vérifient $x>5$ ou $x<-2$, alors~un nombre réel $x$ qui n'est pas solution de $E$ vérifie $x\le 5$ et $x\ge 2$. autrement dit, il appartient à l'intervalle $[-2,5]$. 
\bigskip

\Concept [Title=Opérateur logique ``xor''] Opérateur logique ``xor'' (ou exclusif)

\Definition [$\sc P$ et $\sc Q$ propositions logiques]~
$$
\mbox{La proposition ``}\sc P\sbox{ xor }\sc Q\mbox{''~est dite~}\Q\{\eqalign{&\mbox{vraie si l'une exactement des propositions est vraie}\cr&
\mbox{fausse si les propositions ont la même valeur logique}}\W.
$$


\Exemple. La proposition $1+1=2$ ou $2+2=5$ est vraie alors que ($1> 1$ ou $0>2$) est une proposition fausse. 
\bigskip

\Definition La table de vérité de l'opérateur logique ``xor'' est le tableau 
\PAR
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		|[draw=none]	|\& $\sc Q$ Vrai \& $\sc Q$ Faux \\
		$\sc P$ Vrai  \& Faux  \& Vrai \\
		$\sc P$ Faux \& Vrai \& Faux  \\
	};
	\draw (tableau-2-1.150) --  (tableau-1-2.west) node [sloped,pos=0.5,above,inner sep=1pt] (t) {$\sc P$ xor $\sc Q$};
	\endtikzpicture
}%

\Remarque : En mathématiques, l'opérateur xor est rarement utilisé, contrairement à l'opérateur ou. 
\bigskip

\Concept [Index=Operateurlogique@Opérateur logique!equivalent@``$\Longleftrightarrow$''] Opérateur logique ``$\Longleftrightarrow$'' (d'équivalence)

\Definition [$\sc P$ et $\sc Q$ propositions logiques]
$$
\mbox{La proposition ``}\sc P\Longleftrightarrow\sc Q\mbox{'' est dite }\Q\{\eqalign{&\sbox{vraie si les propositions }\sc P\sbox{ et }\sc Q\sbox{ ont même valeur}\cr&
\sbox{fausse si les deux propositions ont une valeur différente}}\W.
$$ 

\Exemples. La proposition $1+1=2\Longleftrightarrow 2\times2=4$ est vraie alors que $1> 1\Longleftrightarrow 0=0$ est une proposition fausse. 
\bigskip

\Definition La table de vérité de l'opérateur logique ``$\Longleftrightarrow$'' est le tableau
\PAR
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		|[draw=none]	|\& $\sc Q$ Vrai \& $\sc Q$ Faux \\
		$\sc P$ Vrai \& Vrai \& Faux\\
		$\sc P$ Faux \& Faux \& Vrai \\
	};
	\draw (tableau-2-1.150) --  (tableau-1-2.west) node [sloped,pos=0.5,above,inner sep=1pt] (t) {$\sc P\Longleftrightarrow\sc Q$};
	\endtikzpicture
}%

\Remarque : si $\sc P\Longleftrightarrow \sc Q$ est vrai, on dit que les propositions $\sc P$ et $\sc Q$ sont équivalentes. 
\bigskip

\Propriete Soient $\sc P$ et $\sc Q$ deux propositions logiques. Alors, on a 
$$
\sbox{la proposition }\sc P\Longleftrightarrow\sc Q\sbox{ a la même valeur que la proposition }(\sbox{non }\sc P)\Longleftrightarrow(\sbox{non }\sc Q). 
$$

\Remarque : Pour établir l'équivalence de gauche, on peut utiliser la propriété précédente et établir l'équivalence de droite (ce qui dans certains cas est plus facile) et réciproquement. 
\bigskip

\Concept [Index=Operateurlogique@Opérateur logique!implique@``$\Longrightarrow$''] Opérateur logique ``$\Longrightarrow$'' (d'implication)

\Definition [$\sc P$ et $\sc Q$ propositions logiques]
$$
\mbox{La proposition ``}\sc P\Longrightarrow\sc Q\mbox{'' est dite }\Q\{\eqalign{&\sbox{vraie si }\sc P\sbox{ est fausse ou si }\sc P\sbox{ et }\sc Q\sbox{ sont vraies}\cr&
\sbox{fausse si }\sc P\sbox{ est vraie et si }\sc Q\sbox{ est fausse}}\W.
$$ 
(retenir que le vrai implique le vrai et que le faux implique tout). 

\Exemple. Les propositions $(1=2)\Longrightarrow (2=4)$ et $(1=2)\Longrightarrow (2\neq 4)$ sont vraies de même que la proposition $(1=1)\Longrightarrow(2\neq4)$. Par contre, la proposition $(1=1)\Longrightarrow(0=1)$ est fausse. 
\bigskip

\Definition La table de vérité de l'opérateur logique ``$\Longrightarrow$'' est le tableau 
\PAR
\centerline{%
	\tikzpicture[Nested Picture]
	\matrix (tableau) [Tableau Operateur]{
		|[draw=none]	|\& $\sc Q$ Vrai \& $\sc Q$ Faux \\
		$\sc P$ Vrai \& Vrai \& Faux\\
		$\sc P$ Faux \& Vrai \& Vrai \\
	};
	\draw (tableau-2-1.150) --  (tableau-1-2.west) node [sloped,pos=0.5,above,inner sep=1pt] (t) {$\sc P\Longrightarrow\sc Q$};
	\endtikzpicture
}%

\Remarque : si $\sc P\Longrightarrow \sc Q$ est vrai, on dit que la proposition $\sc P$ implique la proposition $\sc Q$. 
\bigskip

\Remarque : la valeur logique de l'implication $\sc P\Longrightarrow \sc Q$, ne dépend que des valeurs logiques de $\sc P$ et $\sc Q$ et non pas d'une éventuelle relation entre $\sc P$ et $\sc Q$. 
En particulier, les implications suivantes sont toutes vraies
$$
\eqalign{
& \ln(1+2)=\ln(1)+\ln(2)\qquad\Longrightarrow\qquad\mbox{Mon prof de maths est un extraterrestre}\cr
& \mbox{Morbid Angel, c'est quand même mieux que Britney Spears}\qquad\Longrightarrow\qquad 666>69\cr
& n!\sim\Q({n\F e}\W)^n\sqrt{2\pi n}\qquad\Longrightarrow\qquad\mbox{Rome a vaincu Carthage au cours de la seconde guerre punique}}
$$


\Propriete [$\sc P$ et $\sc Q$ propositions logiques] 
$$
\sbox{La proposition logique }\sc P\Longrightarrow\sc Q\sbox{ a la même valeur que la proposition }\overline{\sc Q}\Longrightarrow{\overline{\sc P}}. 
$$
L'implication de droite est la contraposée de celle de gauche (et réciproquement).
\bigskip


\Propriete [$\sc P$ et $\sc Q$ propositions logiques] 
$$
\sbox{La proposition logique }\sc P\Longleftrightarrow\sc Q\sbox{ a la même valeur que la proposition }(\sc P\Longrightarrow\sc Q)\sbox{ et }(\sc Q\Longrightarrow\sc P). 
$$

\Section Raisonnements, Raisonnements. 

\Concept Raisonnement par double implications

\Propriete [$\sc P$ et $\sc Q$ propositions logiques]
Pour établir l'équivalence $\sc P\Longleftrightarrow\sc Q$, il~suffit de démontrer $\sc P\Longrightarrow\sc Q$ et $\sc Q\Longrightarrow\sc P$. 

\Concept Raisonnement par contraposée

\Propriete[$\sc P$ et $\sc Q$ propositions logiques]
Pour établir l'implication $\sc P\Longrightarrow\sc Q$, il suffit d'établir sa contraposée $\mbox{non}(\sc Q)\Longleftrightarrow\mbox{non}(\sc P)$ (ce qui dans certains cas est plus facile). 
Pour cela, on suppose que la proposition $\sc Q$ est fausse puis on en déduit que la proposition $\sc P$ est fausse.  
\bigskip

\Concept Raisonnement par l'absurde

\Propriete [$\sc P$ et $\sc Q$ propositions logiques]
Pour établir une proposition logique $\sc P$, en raisonnant par l'absurde, 
il suffit de supposer son contraire (i.e. on suppose que la proposition ``non $\sc P$'' est vraie) puis de procèder par implications pour aboutir à une proposition fausse (une absurdité, une contradiction). 
$$
\underbrace{\overline{\sc P}}_{\mbox{\sevenrm Supposé}}\mathop{\Longrightarrow}\limits^{\mbox{\sevenrm Vrai}}\sc Q\mathop{\Longrightarrow}\limits^{\mbox{\sevenrm Vrai}}\sc R\mathop{\Longrightarrow}\limits^{\mbox{\sevenrm Vrai}}\cdots\mathop{\Longrightarrow}\limits^{\mbox{\sevenrm Vrai}}\sc W\mathop{\Longrightarrow}\limits^{\mbox{\sevenrm Vrai}}\underbrace{\sc Z}_{\mbox{Faux}}
$$ 

\Remarque : Si les implications pré-citées sont justes et si la proposition $\sc Z$ est fausse, il résulte de la table de vérité de l'implication que les propositions $\sc W,\cdots \sc R$, $\sc Q$, non $(\sc P)$ sont fausses et donc que la proposition $\sc P$ est vraie. 
Par ailleurs, d'après le principe de contraposition, la chaine d'implication précédente est équivalente à la chaine d'implication 
$$
\underbrace{\sc P}_{\mbox{\sevenrm Vrai}}\mathop{\Longleftarrow}\limits^{\mbox{\sevenrm Vrai}}
\overline{\sc Q}\mathop{\Longleftarrow}\limits^{\mbox{\sevenrm Vrai}}\overline{\sc R}
\mathop{\Longleftarrow}\limits^{\mbox{\sevenrm Vrai}}\cdots
\mathop{\Longleftarrow}\limits^{\mbox{\sevenrm Vrai}}\overline{\sc W}
\mathop{\Longleftarrow}\limits^{\mbox{\sevenrm Vrai}}
\underbrace{\overline{\sc Z}}_{\mbox{Vrai}},
$$
qui prouve la proposition $\sc P$ de manière directe. 

\noindent{\bf Moralité : } il est toujours possible de trouver une démonstration directe d'une propriété établie par l'absurde (cela constitue souvent un bon exercice, très formateur, mais pas toujours évident). 
\bigskip

\Section CPPplan, Définitions élémentaires. 

\Concept Ensemble des parties

Tout ensemble $B$ inclus dans un ensemble $A$ est appelé une partie de $A$ (ou un sous-ensemble de $A$). \pn
L'ensemble de tous les sous-ensembles de $A$ est appelé simplement ensemble des parties de $A$ et on le note
$$
\sc P(A):=\{B:B\subset A\}.
$$

\Concept Intersection simple

Soient $B$ et $C$ deux parties de $A$. Alors, l'intersection de $B$ et $C$ est l'ensemble
$$
B\cap C:=\{x\in A:x\in B \mbox{ et }x\in C\}.
$$

\Concept Intersection multiple

Soient $I$, $A$ deux ensembles. Alors, l'intersection des parties $B_i\subset A\ \,(i\in I)$ de $A$ est l'ensemble 
$$
\cap_{i\in I}B_i:=\{x\in A:\forall i\in I:x\in B_i\}.
$$ 


\Concept Réunion simple

Soient $B$ et $C$ deux parties de $A$. Alors, la réunion de $B$ et $C$ est l'ensemble 
$$
B\cup C:=\{x\in A:x\in B \mbox{ ou }x\in C\}.
$$

\Concept Réunion multiple

Soient $I$, $A$ deux ensembles. Alors, la réunion des parties $B_i\subset A\ \,(i\in I)$ de $A$ est l'ensemble 
$$
\cup_{i\in I}B_i:=\{x\in A:\exists i\in I:x\in B_i\}.
$$ 

\Concept Principe du ``dressing-undressing''

Soient $E$ un ensemble non vide et $u$ et $v$ deux bijections de $E$ dans $E$. Alors, on a 
$$
(u\circ v)^{-1}=v^{-1}\circ u^{-1}.
$$


\Concept Loi interne

Soit $E$ un ensemble. Une loi (de composition) interne $\otimes$ de l'ensemble $E$ est une application $\phi:E\times E\to E$. 
Pour simplifier les expressions, on note 
$$
\forall x\in E, \qquad\forall y\in E, \qquad x\otimes y:=\phi(x,y).
$$


\Concept Loi externe

Soit $E$ un ensemble. Une loi (de composition) externe $.$ de l'ensemble $E$ est une application $\phi:A\times E\to E$, où $A$ est un ensemble différent de $E$. 
Pour simplifier les expressions, on note 
$$
\forall \lambda\in A, \qquad\forall y\in E, \qquad x.y:=\phi(x,y).
$$




\hautspages{Olus Livius Bindus}{Nombres complexes}


\Chapter nombres complexes, Nombres complexes.

\Section Introduction C, Introduction. 

Historiquement, la constatation que les nombres réels ne suffisent pas pour résoudre certaines équations polynômiales 
du second degré telle que
$$
x^2=-1
\eqdef{Ceq}
$$ 
a motivé l'invention des nombres complexes : les nombres $a+ib$ pour lesquels $a$ et $b$ sont des nombres réels 
et pour lesquels $i$ désigne une solution de l'équation \eqref{Ceq}. 
\bigskip
L'ensemble $\ob C$ des nombres complexes permet de résoudre toutes les équations po\-ly\-nô\-mia\-les 
du second degré et possède la propriété fondamentale suivante : 

\Theoreme [Index=Theoreme@Théorème!de d'Alembert;Title=Théorème de d'Alembert-Gauss] 
Toute équation polynômiale de degré supérieur ou égal à $1$ admet au moins une solution dans~$\ob C$. 

L'ensemble $\ob C$ des nombres complexes présente des avantages par rapport à l'ensemble $\ob R$ des nombres réels, 
qui ne résident pas uniquement dans ces propriétés liées aux polynômes. D'ailleurs, une méthode classique très employée pour résoudre un problème réel consiste à faire le détour dans l'ensemble $\ob C$ suivant : \medskip
 \noindent
a) transformer le problème réel en problème complexe, en général plus facile. 
\smallskip\noindent
b) résoudre le problème complexe. 
\smallskip\noindent
c) En déduire les solutions du problème réel, souvent par projection. 
\bigskip

Les nombres complexes sont employés quotidiennement par la plupart des ma\-thé\-ma\-ti\-ciens et constituent un des pilliers sur lesquels reposent les mathématiques. 
C'est pourquoi, il est impératif en pratique de maitriser les nombres complexes, c'est à dire en ce qui vous concerne d'apprendre à calculer ou à transformer n'importe quelle expression complexe ou trigonométrique sans faire d'erreur. Et raisonnablement vite si c'est possible...

Les nombres complexes se présentent sous deux formes fondamentales : la forme al\-gè\-bri\-que $z=x+iy$ 
et la forme multiplicative $z=r\e^{i\theta}$. La première forme s'emploie plutôt dans un contexte additif 
et la seconde forme s'emploie plutôt dans un contexte multiplicatif. 



\Section CorpsC, Forme algébrique des nombres complexes. 

\Subsection ConstructionC, Construction de $\ob C$ et lien géométrique. 

De même que $\ob R$ est muni d'une addition et d'une multiplication, 
il est possible de munir de deux opérations l'ensemble $\ob C:=\ob R^2$ des couples de nombres réels $(x,y)$, 
en~posant : 
$$
\eqalign{
(a,b)+(c,d)&:=(a+c,b+d)\qquad\qquad\mbox{\bf Addition}\cr
(a,b)\times(c,d)&:=(ac-bd,ad+bc)\qquad\mbox{\bf Multiplication}\cr
}%\eqdef{Coperation}
$$
Muni de ces opérations $+$ et $\times$, l'ensemble $\ob C=\ob R^2$ forme un corps commutatif 
(une structure algébrique que nous détaillons plus loin) de nouveaux nombres du type $(x,y)$, appelés nombres~complexes. 
Manier ces~nombres sous leur forme $(x,y)$ est peu pratique. 
C'est~pourquoi la notation simplifiée suivante est presque toujours utilisée : 
\medskip
\item{$\bullet$}
Un nombre du type $(x,0)$, où $x$ désigne un nombre réel, sera simplement noté ``$x$'' et sera appelé nombre réel (un abus bien pratique).
\medskip
\item{$\bullet$} 
Le nombre $(0,1)$ sera noté ``$i$''. On remarque qu'il vérifie $i^2=-1$. 
\medskip
\item{$\bullet$} 
Un nombre du type $(0,y)$, où $y$ désigne un nombre réel, sera simplement noté ``$iy$'' et sera appelé nombre imaginaire pur. 
\medskip
\item{$\bullet$}
Plus généralement, un nombre complexe $(x,y)$, où $x$ et $y$ sont deux nombres réels, sera~noté ``$x+iy$''. 
\bigskip

\noindent
Exercice : vérifier que $i\times i=-1$ puis que $x+i\times y$ est bien le nombre $(x,y)$. 
\bigskip

Structurellement, l'ensemble des nombres complexes est étroitement lié au plan affine : 
\bigskip
\Definition [$\sc P$ plan affine muni d'un repère orthonormal $(O,\vec i,\vec j)$] 
\item{$\bullet$} L'affixe d'un point $M$ de coordonnées $(x,y)$ est le nombre complexe $z=x+yi$. 
\item{$\bullet$} L'affixe d'un vecteur $\vec v=x\vec i+y\vec j$ est le nombre complexe $z=x+yi$. 
\item{$\bullet$} L'image d'un nombre complexe $z=x+iy$ est le point $M(z)$ de coordonnées $(x,y)$. 

\centerline{
\tikzpicture
\clip (-0.5,-0.5) rectangle (5.4,3.1);
\draw[blue!20,ultra thin] (-0.5,-0.5) grid (5.4,3.1);
\draw[-] (-0.5,0) -- (5,0) coordinate (x axis);
\draw[-] (0,-0.5) -- (0,3) coordinate (y axis);
\draw[-triangle 60,thick] (0,0) -- node [anchor=south,sloped] {$\vec v=x\vec i+y\vec j$} node [anchor=north,sloped,pos=0.6,color=red] {$z=x+yi$} (4.3,2.4) node [anchor=south] {$M (x,y)$} node [anchor=north west,color=red] {$M (z)$} ;
\draw[-triangle 45,thick] (0,0) -- node [anchor=east,pos=0.65] {$y\vec j$} (0,2.4);
\draw[-triangle 45,thick] (0,0) -- node [anchor=north,pos=0.7] {$x\vec i$} (4.3,0);
\draw[dashed] (4.3,2.4) -- (0,2.4) node [color=red,anchor=south west] {$yi$};
\draw[dashed] (4.3,2.4) -- (4.3,0) node [color=red,anchor=south west] {$x$};
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=east] {$\vec j$} (0,1);
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=north] {$\vec i$} (1,0);
\node [anchor=north east] (O) at (0,0) {$O$};
\endtikzpicture}

\Remarque : Ne pas confondre le nombre complexe $i$ avec le vecteur $\vec i$. 

Géométriquement, l'addition de deux nombres complexes $s=a+ib$ et $z=c+id$ s'interprètre 
par la règle du parallèlogramme : en effet, les points $O$, $M$, $P$ et $Q$, d'affixes~respectives $0$, $s$, $z$ et $s+z$, 
forment un parallèlogramme. 

\centerline{
\tikzpicture[scale=0.8]
\draw[-] (-0.2,0) -- (6.5,0) coordinate (x axis);
\draw[-] (0,-0.2) -- (0,4.5) coordinate (y axis);
\draw[-,thick,dotted] (2,2.5) -- (6,4) ;
\draw[-,thick,dotted] (4,1.5) -- (6,4);
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=east] {$\vec j$} (0,1);
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=north] {$\vec i$} (1,0);
\draw[-triangle 60,thick] (0,0) --  (2,2.5) node [anchor=south east] {$M (s)$};
\draw[dashed] (2,2.5) -- (0,2.5) node [color=red,anchor=east] {$b$};
\draw[dashed] (2,2.5) -- (2,0) node [color=red,anchor=north] {$a$};
\draw[-triangle 60,thick] (0,0) --  (4,1.5) node [anchor=north west] {$P (z)$};
\draw[dashed] (4,1.5) -- (0,1.5) node [color=red,anchor=east] {$d$};
\draw[dashed] (4,1.5) -- (4,0) node [color=red,anchor=north] {$c$};
\draw[-triangle 60,thick] (0,0) --  (6,4) node [anchor=south] {$Q (s+z)$};
\draw[dashed] (6,4) -- (0,4) node [color=red,anchor=east] {$b+d$};
\draw[dashed] (6,4) -- (6,0) node [color=red,anchor=north] {$a+c$};
\node [anchor=north east] (O) at (0,0) {$O$};
\endtikzpicture}

\Section DefinitionC, Définition algébrique de $\ob C$. 

En utilisant la notation $x+iy$, la définition de l'ensemble $\ob C$ des nombres complexes et de ses opérations $+$ et $\times$ 
est légèrement plus simple et plus intuitive : 

\Definition L'ensemble $\ob C$ des nombres complexes est l'ensemble des nombres de la forme $z=x+iy$, 
où $x$ et $y$ sont des nombres réels et où $i^2=-1$, muni des opérations 
$$
\eqalign{(a+ib)+(c+id)&:=(a+c)+i(b+d)\qquad\qquad\mbox{\bf Addition}\cr
(a+ib)\times(c+id)&:=(ac-bd)+i(ad+bc)\qquad\mbox{\bf Multiplication}}
\eqdef{Coperation2}
$$


\noindent
Exercice : Calculer $(1-i)^3$ ainsi que $(1+i)^8$, en essayant de limiter les opérations. 
\bigskip

Ces régles de calcul des nombres complexes sont assez semblables aux règles de calcul des nombres réels (développement, factorisation par $i$) auxquelles on ajouterait la relation 
$$
i^2=-1. \eqdef{Ceqi}
$$ 
Ceci est du au fait que l'ensemble $\ob C$ des nombres complexes et l'ensemble $\ob R$ des nombres réels 
sont régis par la même structure algèbrique : ce sont des corps commutatifs. 

\Theoreme 
L'ensemble $\ob C$ des nombres complexes muni des opérations $+$~et~$\times$ définies par \eqref{Coperation2} 
forme un corps commutatif, que l'on note $(\ob C,+,\times)$. 

Avant de rappeler succintement les structures de corps et de corps commutatifs, prenons le temps d'introduire quelques abbréviations mathématiques standards : 
\medskip

\item{$\bullet$}
Le symbole ``$\in$'' signifie ``élément de", ``dans" ou ``appartient à'' ,
\smallskip
\item{$\bullet$}
Le symbole ``$\forall$'' se lit ``pour chaque" ou``pour tout",
\smallskip
\item{$\bullet$}
Le symbole ``$\exists$'' se traduit par ``il existe'' ou par ``on peut trouver'',
\smallskip
\item{$\bullet$}
Le signe de ponctuation ``:'' signifie très souvent ``tel que'' ou ``pour lequel''. 
\medskip

En particulier, on peut traduire la phrase ``on peut trouver un nombre x dans $\ob R$ tel que la quantité $x^2+3$ soit égale à $4$'' par la proposition mathématique 
$$
\exists\ x\in\ob R:\qquad x^2+3=4. 
$$
Réciproquement, il est possible de prendre une assertion symbolique et de la mettre sous une forme plus intélligible, utilisant la langue fran\c caise. En pratique, 
cela en facilite la compréhension et c'est ce que nous vous engageons à faire pour la définition suivante : 
\bigskip

\Concept [Index=Structure!Corps] Définition d'un Corps

Un ensemble $E$ muni de deux opérations $+$ et $\times$ forme un corps $(E,+,\cdots)$ si, et~seulement si, les onze propriétés suivantes sont satisfaites :
i) L'ensemble $E$ n'est pas vide : 
$$
\exists a\in E.
$$ 
ii) L'opération $+$ est interne à $E$ : 
$$
\forall (a,b)\in E^2, \qquad a+b\mbox{ est défini}\quad\mbox{et}\quad a+b\in E
$$
iii) L'opération $+$ admet un élément neutre, noté $0$, dans $E$ : 
$$
\exists 0\in E:\qquad \forall a\in E,\qquad 0+a=a+0=a
$$
iv) Chaque élément de l'ensemble $E$ admet un inverse pour la loi $+$ : 
$$
\forall a\in E, \qquad \exists b\in E:\qquad a+b=0=b+a
$$
v) L'opération $+$ est associative dans $E$ :
$$
\forall (a,b,c)\in E^3, \qquad(a+b)+c=a+(b+c)
$$ 
vi) L'opération $+$ est commutative dans $E$ : 
$$
\forall (a,b)\in E^2, \qquad a+b=b+a
$$
vii) L'opération $\times$ est distributive sur l'opération $+$ dans $E$ : 
$$
\forall(a,b,c)\in E^3, \qquad a\times(b+c)=a\times b+a\times c\quad\mbox{et}\quad (b+c)\times a=b\times a+c\times a.
$$ 
viii) L'opération $\times$ est interne à $E$ : 
$$
\forall (a,b)\in E^2,\qquad a\times b\mbox{ est défini}\quad \mbox{et}\quad a\times b\in E
$$
ix) L'opération $\times$ admet un élément neutre non nul dans $E$, noté $1$ :
$$
\exists 1\in E, \qquad\mbox{ différent de }0: \qquad \forall a\in E, \qquad a\times1=1\times a=a.
$$ 
x) Chaque élément non nul de $E$ est inversible pour $\times$ : 
$$
\forall a\in E, \qquad\mbox{différent de }0, \qquad \exists b\in E:\qquad a\times b=b\times a=1. 
$$
xi) L'opération $\times$ est associative dans $E$
$$
\forall (a,b,c)\in E^3, \qquad(a\times b)\times c=a\times(b\times c)
$$ 

\Definition 
Un corps $(E,+,\times)$ est commutatif si, et seulement si, 
\bigskip
xii) L'opération $\times$ est commutative dans $E$ : 
$$
\forall (a,b)\in E^2, \qquad a\times b=b\times a
$$

Ces douze propriétés constituent les règles de calculs à appliquer dans chaque corps~commutatif et en particulier dans $\ob R$ et $\ob C$. 
Pour résumer sommairement, on peut retenir que 
\medskip
\centerline
{
On calcule dans $\ob C$ comme dans $\ob R$ en utilisant la relation \eqref{Ceqi}.
}
\bigskip

Exercice. En admettant que $(\ob R,+,\times)$ est un corps commutatif, vérifier que $(\ob C,+,\times)$ en est un également. 
\bigskip

\goodbreak
L'identité suivante est très importante et sert en particulier à calculer la somme d'une suite géomètrique. 
\bigskip

\Propriete [$n\in\ob N$ et $(a,b)\in\ob C^2$]
$$
a^n-b^n=(a-b)\sum_{k=0}^{n-1}a^kb^{n-1-k}=(a-b)\sum_{k+\ell=n-1}a^kb^\ell.
$$
En particulier, l'identité remarquable $a^2-b^2=(a-b)(a+b)$ est une conséquence immédiate 
de la relation précédente pour l'entier $n=2$. 
\bigskip

\Definition 
Pour des entiers $n\ge k\ge 0$, on pose 
$$
{n\choose k}:={n!\F k!(n-k)!}.
$$

Le symbôle ${n\choose k}$ se lit ``$n$ et $k$'' et le nombre qu'il représente se note parfois $c_n^k$ 
et satisfait la propriété suivante : 

\Propriete
Pour $0\le k\le n$, le nombre ${n\choose k}$ est un entier. De plus, si $0\le k<n$, on a 
$$
{n\choose k}+{n\choose k+1}={n+1\choose k+1}.
$$

L'identité suivante est fondamentale. On l'emploie par exemple pour linéariser des expressions trigonométriques. 

\Propriete [Title=Binôme de Newton;$n\in\ob N$ et $(a,b)\in\ob C^2$] 
$$
(a+b)^n=\sum_{k=0}^n{n\choose k}a^kb^{n-k}=\sum_{k+\ell=n}{n\choose k}a^kb^\ell.
$$

\noindent
Les identités remarquables $(a+b)^2=a^2+2ab+b^2$ et $(a-b)^2=a^2-2ab+b^2$ sont des conséquences 
immédiates du binôme de Newton pour l'entier $n=2$. 
\bigskip

\Subsection CProjections, Parties réelles et imaginaires. 

La définition du corps $\ob C$ implique que chaque nombre complexe $z$ peut se mettre sous la forme $z=x+iy$, où $x$ et $y$ sont des nombres réels, 
et que cette écriture est unique, ce~qui nous permet de définir les parties réelles et imaginaires du nombre $z$. 
\bigskip

\Definition [$z=x+iy$ nombre complexe, avec $x$ et $y$ nombres réels] 
La partie réelle $\re(z)$ et la partie imaginaire $\im(z)$ du nombre $z$ sont définis par 
$$
\re(z):=x\qquad\mbox{et}\qquad\im(z):=y. 
$$

\Remarque : malgré son nom, la partie imaginaire est un nombre réel. 

\noindent
Une égalité entre nombres complexes se traduit par un système de deux égalités entre nombres réels. 
\medskip

\Propriete 
Deux nombres complexes $s$ et $z$ sont égaux si et seulement s'ils ont même parties réelles et même parties imaginaires. 
$$
s=z\qquad\Longleftrightarrow\qquad
\Q\{\eqalign{
\re(s)=\re(z),\cr
\im(s)=\im(z).
}\W.
$$


\Propriete La partie réelle et la partie imaginaire sont des applications $\ob R$-linéaires. Autrement dit, elles vérifient 
$$
\forall(s,z)\in\ob C^2, \qquad\forall(\lambda,\mu)\in\ob R^2, \qquad\Q\{
\eqalign{\re(\lambda s+\mu z)&=\lambda \re(s)+\mu\re(z),\cr
\im(\lambda s+\mu z)&=\lambda\im(s)+\mu\im(z).}\W.
$$

\noindent
Exercice : Prouver la propriété précédente. 

\Subsection CConjugaison, Conjugué d'un nombre complexe. 


Le corps $\ob C$ est naturellement muni d'une application remarquable : la conjugaison $z\mapsto\overline z$. 
\medskip

\Definition [$z=x+iy$ nombre complexe avec $x$ et $y$ nombres réels]
Le conjugé du nombre $z$ est le nombre complexe $\overline z$ défini par 
$$
\overline z:=x-iy. 
$$

Géomètriquement, la conjugaison est une symétrie par rapport à la droite réelle. 
Les~points $M$ et $P$ d'affixes respectives $z$ et $\overline z$ sont symétriques par rapport à l'axe $(O,\vec i)$. 

\centerline{
\tikzpicture
\clip (-1,-1.9) rectangle (5.5,1.9);
\draw[-] (-0.5,0) -- (5,0) coordinate (x axis);
\draw[-] (0,-1.7) -- (0,1.7) coordinate (y axis);
\node [anchor=north east] (O) at (0,0) {$O$};
\draw[-triangle 45,thick] (0,0) -- node [anchor=south,sloped,pos=0.4] {$x\vec i+y\vec j$} %node [anchor=north,sloped,color=red,pos=0.6] {$z=x+iy$}
 (4.3,1.4) node [anchor=south west] {$M (z)$};
\draw[dashed] (4.3,1.4) -- (0,1.4) node [anchor=east] {$iy$};
\draw[-triangle 45,thick] (0,0) -- node [anchor=north,sloped,pos=0.4] {$x\vec i-y\vec j$} % node [anchor=north,sloped,color=red,pos=0.4] {$\overline z=x-yi$} 
 (4.3,-1.4) node [anchor=north west] {$P (\overline z)$};
\draw[dashed,color=red] (4.3,-1.4) -- (0,-1.4) node [anchor=east] {$-iy$};
\draw[-triangle 45,thick] (0,0) -- node [anchor=south,pos=0.6] {$x\vec i$} node [color=red,anchor=north,pos=0.6] {$x$} (4.3,0) ;
\draw[-triangle 45,thick] (4.3,0) -- node [anchor=west] {$y\vec j$} (4.3,1.4);
\draw[|-triangle 45,thick] (4.3,0) -- node [anchor=west] {$-y\vec j$} (4.3,-1.4);
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=east] {$\vec j$} (0,1);
\draw[-open triangle 60,thick] (0,0) -- node [anchor=north,pos=0.2] {$\vec i$} (1,0);
\endtikzpicture}%

\noindent
Il est possible d'exprimer un nombre $z$ et son conjugué $\overline z$ en fonction des parties réelles et imaginaires $\re(z)$ et $\im(z)$
$$
\forall z\in\ob C, \qquad z=\re(z)+i\ \im(z)\quad\mbox{et}\quad \overline z=\re(z)-i\ \im(z)
$$
et vice versa
$$
\forall z\in\ob C, \qquad \re(z)={z+\overline z\F 2}\quad\mbox{et}\quad\im(z)={z-\overline z\F2i}. 
$$
La conjugaison et les parties réelles et imaginaires permettent de caractériser 
simplement les nombres complexes qui sont des nombres réels et ceux qui sont imaginaires purs. 
\medskip

\Propriete 
Un nombre complexe $z$ est un nombre réel si, et seulement si, sa partie imagnaire est nulle. 
$$
z\mbox{ est réel}\quad\Longleftrightarrow\quad \overline z=z\quad\Longleftrightarrow\quad z=\re(z)\quad\Longleftrightarrow\quad\im(z)=0
$$

\Propriete 
Un nombre complexe $z$ est imaginaire pur si, et seulement si, sa partie réelle est nulle. 
$$
z\mbox{ est imaginaire pur}\quad\Longleftrightarrow\quad \overline z=-z\quad\Longleftrightarrow\quad z=i\ \im(z)\quad\Longleftrightarrow\quad\re(z)=0.
$$

On ne change pas un nombre complexe en le conjuguant successivement deux fois. 
\medskip

\Bullet Le nombre $\overline z$ est le conjugué de $z$ et le nombre $z$ est le conjugué de $\overline z$. Autrement~dit, les nombres $z$ et $\overline z$ sont conjugués. 
$$
\forall z\in\ob C,\qquad \overline{\thinspace\overline z\thinspace}=z.
$$

Le conjugué d'une somme est la somme des conjugués ; le conjugué d'une différence est la différence des conjugués et 
le conjugué d'un nombre multiplié par une constante réelle est le produit de son conjugué par la constante réelle. 
\medskip

\Bullet
La conjuguaison $z\mapsto\overline z$ est une application $\ob R$-linéaire. Autrement dit 
$$
\forall (s,z)\in\ob C^2,\qquad\forall(\lambda,\mu)\in\ob R^2, 
\qquad \lambda s+\mu z=\lambda\overline s+\mu\overline z.
$$


\Bullet Le conjugué d'un produit est le produit des conjugués. 
$$
\forall (s,z)\in\ob C^2, \qquad \overline{s\times z}=\overline s\times \overline z.
$$


\Bullet Le conjugué d'un quotient est le quotient des conjugués. 
$$
\forall (s,z)\in\ob C\times\ob C^*, \qquad 
\overline{\Q({s\F z}\W)}={\thinspace\overline s\thinspace\F \overline z}.
$$

Une conséquence immédiate de ces propriétés est que le conjugué d'une puissance est la puissance du conjugué : 
$$
\forall z\in\ob C^*, \qquad\forall n\in\ob Z, \qquad \overline{z^n}=\overline z^n.
$$

\Section CModule, Module d'un nombre complexe. 

\Definition 
Pour chaque nombre complexe $z=x+iy$, avec $x$ et $y$ nombres réels, le mo\-du\-le du nombre $z$ est le nombre réel positif $|z|$ défini par 
$$
|z|:=\sqrt{z\overline z}=\sqrt{x^2+y^2}. 
$$
\medskip
Dans un plan affine muni d'un repère orthonormé $(O,\vec i,\vec j)$, la distance de l'origine $O$ à un point $M$ d'affixe $z=x+iy$ 
est le module $|z|=\sqrt{x^2+y^2}$ de l'affixe du point $M$. 

\centerline{
\tikzpicture
\clip (-1,-0.7) rectangle (6.5,2.7);
\draw[-] (-0.5,0) -- (6,0) coordinate (x axis);
\draw[-] (0,-0.5) -- (0,2.7) coordinate (y axis);
\node [anchor=north east] (O) at (0,0) {$O$};
\draw[-triangle 45,thick] (0,0) -- node [anchor=south,sloped,pos=0.4] {$OM=|z|=\sqrt{x^2+y^2}$} 
 (5.3,2.4) node [anchor=west] {$M (z)$};
\draw[-triangle 45,thick] (0,0) -- node [anchor=south] {$x\vec i$} (5.3,0) ;
\draw[-triangle 45,thick] (5.3,0) -- node [anchor=east] {$y\vec j$} (5.3,2.4);
\draw[-open triangle 60,thick] (0,0) -- node [pos=0.4,anchor=east] {$\vec j$} (0,1);
\draw[-open triangle 60,thick] (0,0) -- node [anchor=north] {$\vec i$} (1,0);
\endtikzpicture}%

Le module et la conjugaison étant liés par la relation 
$$
\forall z\in\ob C, \qquad |z|^2=z\overline z. 
$$

\goodbreak
Une méthode classique pour mettre le quotient $s/z$ sous sa forme algébrique est : 

\Methode [Pour mettre l'inverse $1/z$ d'un nombre complexe {$z=x+iy$} non nul sous la~forme $a+ib$] 
On multiplie numérateur et dénominateur par le conjugué de $z$. 
$$
\forall z\in\ob C, \qquad {1\F z}={\overline z\F z\overline z}={\overline z\F |z|^2}={x-iy\F x^2+y^2}={x\F x^2+y^2}-i {y\F x^2+y^2}.
$$

\noindent
Exercice : Mettre sous la forme algèbrique $x+iy$ les nombres complexes suivants :
$$
{2+i\sqrt3\F\sqrt3-2i}\qquad\mbox{et}\qquad {(3+i)(2-i)\F2+i}.
$$

\noindent Le module possède les propriétés suivantes 
\bigskip

\Bullet L'application $z\mapsto|z|$ est bien définie et à valeurs réelles positives :
$$
\forall z\in\ob C, \qquad|z|\ge0.
$$

\Bullet L'application $z\mapsto|z|$ est dite ``définie'' :
$$
\forall z\in\ob C, \qquad|z|=0\ \Longleftrightarrow\ z=0.
$$

\Remarque : Ne pas confondre cette propriété ``définie'' avec la propriété standard ``défini=existe". 
\medskip 

\Bullet Inégalités triangulaires :
$$
\forall (s,z)\in\ob C^2, \qquad\B| |s|-|z|\B| \le |s+z|\le |s|+|z|.\eqdef{eqtriangulaire}
$$

Dans un plan affine muni d'un repère orthonormé $(O,\vec i,\vec j)$, la distance d'un point $M$ d'affixe $z=x_1+iy_1$ 
à un point $Q$ d'affixe $a=x_2+iy_2$ est 
$$
QM=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}=|z-a|.
$$




\Definition [$a$ nombre complexe et $r>0$ nombre réel] 
\item{$\bullet$} Le disque ouvert de centre $a$ et de rayon $r$ est l'ensemble 
$$
\sc D(a,r):=\{z\in\ob C:|z-a|<r\}. $$
\item{$\bullet$} Le disque fermé de centre $a$ et de rayon $r$ est l'ensemble 
$$
\overline{\sc D}(a,r):=\{z\in\ob C:|z-a|\le r\}. $$
\item{$\bullet$} Le cercle de centre $a$ et de rayon $r$ est l'ensemble
$$
\sc C(a,r):=\{z\in\ob C:|z-a|=r\}. 
$$


\centerline{
\tikzpicture
%\clip (-1.3,-0.5) rectangle (7,6);
\node [circle,fill=gray,pattern=north west lines,pattern color=gray,draw=red,thick] at (1,4.5) {$\sc D(a,r)$};
\node [circle,fill=gray,pattern=north west lines,pattern color=gray,draw=gray,thick] at (0,3) {$\overline{\sc D}(a,r)$};
\node [circle,draw=gray,thick] at (1,1.5) {$\sc C(a,r)$};
\draw (4,3) circle (2cm);
\node [label={[label distance=-0.3cm]-45:$a$}] at (4,3) {$\times$};
\draw [->] (4,3) -- node [anchor=south,sloped] {rayon $r$}++(200:2cm);
\draw [->] (4,3) -- node [anchor=south,sloped,pos=0.4] {$|z-a|$}++(50:2.5cm) node [label={[label distance=-0.3cm]-45:$z$}] {$\times$};
\endtikzpicture}


La relation \eqref{eqtriangulaire} étant également satisfaite par le nombre $z'=-z$, elle est équivalente 
à l'inégalité triangulaire
$$
\forall (s,z)\in\ob C^2, \qquad\Q| |s|-|z|\W| \le |s-z|\le |s|+|z|, 
$$
qui s'interprète géométriquement de la fa\c con suivante : le coté d'un triangle est plus petit que la somme 
des deux autres cotés et plus grand que leur différence. 


\medskip
\hfill
\pspicture*[](-1,-0.7)(8.8,3.5)
\psaxes*[labels=none,ticks=none]{-}(0,0)(-.5,-.5)(8.8,3.3)
\psline[linewidth=1pt,arrowsize=6pt]{->}(0,2)
\psline[linewidth=1.5pt,arrowsize=6pt]{->}(2,0)
\rput{0}(-.2,-.2){$O$}
\rput{0}(1,-.3){$\vec i$}
\rput{0}(-.3,1){$\vec j$}
\psline[linewidth=1.5pt,arrowsize=6pt,linecolor=blue]{->}(6,3)
\rput{0}(5.8,3.3){$M(z)$}
\rput{26.5651}(3,1.7){\blue {\bf distance }$OM=|z|$}
\psline[linewidth=1.5pt,arrowsize=6pt,linecolor=red]{->}(8,1)
\rput{0}(8.4,.8){$P(s)$}
\rput{7.12502}(4,0.7){\red {\bf distance }$OP=|s|$}
\psline[linewidth=1.5pt,arrowsize=6pt]{-}(6,3)(8,1)
\rput{-45}(7.1,2.2){{\bf distance}}
\rput{-45}(6.7,2){$MP=|s-z|$}
\endpspicture
\hfill\null
\medskip

\noindent
Le module d'un produit est le produit des modules. 
$$
{
\forall (s,z)\in\ob C^2}, \qquad{|s\times z|=|s|\times|z|}.
$$

\noindent
Un nombre complexe a même module que son conjugué. 
$$
{\forall z\in\ob C}, \qquad
{|z|=|\overline z|}.
$$

\noindent
Le module d'un quotient est le quotient des modules. 
$$
{\forall (s,z)\in\ob C\times \ob C^*}, \qquad
{\Q|{s\F z}\W|={\thinspace|s|\thinspace\F|z|}}.
$$
En particulier, le module d'une puissance est la puissance du module. 
$$
{\forall z\in\ob C^*}, \qquad
{\forall n\in\ob Z}, \qquad 
{\Q|z^n\W|=|z|^n}.
$$
Les parties réelles et imaginaires d'un nombre sont majorées par son module : 
$$
{
\forall z\in\ob C}, \qquad 
{\b|\re(z)\b|\le |z|}\quad\mbox{et}\quad
{\b|\im(z)\b|\le|z|}. 
$$
Le module d'un nombre réel est égal à sa valeur absolue. 
$$
\forall x\in\ob R, \qquad \underbrace{|x|}_{\mbox{module}}=\underbrace{|x|}_{\vtop{\mbox{valeur absolue}}}
$$

\Section CTrigonometrie, Forme trigonométrique des nombres complexes. 

\Subsection CGroupeU, Groupe $\ob U$ des nombres complexes de module $1$. 

Commen\c cons par introduire une nouvelle structure algèbrique : la structure de groupe. 
\medskip


\Definition [] Un ensemble $E$ muni d'une opération $\otimes$ est un groupe si, et seulement, si 
les cinq propriétés suivantes sont satisfaites : 
\item{i)}L'ensemble $E$ n'est pas vide : 
$$
\exists a\in E.
$$ 
\item{ii)}L'opération $\otimes$ est interne à $E$ : 
$$
\forall (a,b)\in E^2, \qquad a\otimes b\mbox{ est défini}\quad\mbox{et}\quad a\otimes b\in E
$$
\item{iii)}L'opération $\otimes$ admet un élément neutre, noté $e$, dans $E$ : 
$$
\exists e\in E:\qquad \forall a\in E,\qquad e\otimes a=a\otimes e=a
$$
\item{iv)}Chaque élément de l'ensemble $E$ admet un inverse pour la loi $\otimes$ : 
$$
\forall a\in E, \qquad \exists b\in E:\qquad a\otimes b=e=b\otimes a
$$
\item{v)}L'opération $\otimes$ est associative dans $E$ :
$$
\forall (a,b,c)\in E^3, \qquad(a\otimes b)\otimes c=a\otimes (b\otimes c)
$$ 

\Definition [] Un groupe $(E,\otimes)$ est commutatif (ou abélien) si, et seulement si, 
\item{vi)}L'opération $\otimes$ est commutative dans $E$ : 
$$
\forall (a,b)\in E^2, \qquad a\otimes b=b\otimes a
$$
\bigskip

%Exemples

Cette nouvelle structure permet de mémoriser plus facilement les propriétés de~$\ob C$. 
Ainsi, au paragraphe \xref{secDefinitionC}, % Fix that
 les propriétés i) à vi) signifient que {$(\ob C,+)$ est un groupe abélien}
et les propriétés viii) à xii) signifient que {$(\ob C^*,\times)$ est un groupe commutatif}. Il reste alors à~mémoriser la~propriété viii) 
affirmant que l'opération $\times$ est distributive sur la loi $+$. 
\medskip

{{ \noindent
{\bf Propriété.} L'ensemble $\ob U:=\{z\in\ob C:|z|=1\}$ des nombres complexes de module $1$ forme un groupe abélien 
pour la multiplication $\times$. 
}}
\bigskip

L'ensemble $\ob U$ forme un cercle de centre $0$ et~de~rayon~$1$, 
appelé cercle unité et parfois appelé cercle trigonométrique pour des raisons, qui seront détaillées plus loin. 
\par\noindent\hfill
\pspicture*[](-2,-2)(2,2)
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-2,-2)(2,2)
\pscircle[linewidth=1pt,linecolor=red](0,0){1.5}
\rput{0}(1.7,-.2){$1$}
\rput{0}(-1.8,-.2){$-1$}
\rput{0}(-.2,1.7,){$i$}
\rput{0}(-.3,-1.7){$-i$}
\rput{0}(-.2,-.2){$0$}
\rput{0}(1.5,0){$+$}
\rput{0}(-1.5,0){$+$}
\rput{0}(0,1.5){$+$}
\rput{0}(0,-1.5){$+$}
\psline[linewidth=.5pt]{-}(1.06066,1.06066)(1.3,1.3)
\rput{0}(1.5,1.5){\red$\ob U$}
\endpspicture
\hfill\null\medskip

{{ \quad Soit $(E,\oplus)$ un groupe. Pour montrer qu'un sous-ensemble $F$ 
de l'ensemble $E$

\noindent 
est un groupe pour la loi $\oplus$, 
c'est-à-dire que $(F,\otimes)$ est un sous-groupe de $(E,\otimes)$, 

\noindent\ 
il suffit de prouver que :

\noindent
\quad $\bullet$ $F$ est non vide

\noindent
\quad $\bullet$ $F$ est inclus dans $E$

\noindent
\quad{$\bullet$} $\forall (x,y)\in F^2,x\otimes y^{-1}\in F$\quad(où $y^{-1}$ désigne l'inverse de $y$ pour la loi $\otimes$ de $E$)}}
\bigskip

\noindent 
Exercice. Prouver que $(\ob Z,+)$ et que $(\ob R^*,\times)$ sont des groupes (abéliens).

\Subsection CArgument, Argument d'un nombre complexe non nul. 

\Definition [] L'argument d'un nombre complexe $z$ de module $1$ est la longueur $\theta$ en~radian 
de l'arc du cercle trigonométrique allant de $1$ à $z$ dans le sens direct. Cette~longueur, qui est unique modulo $2\pi$, 
est notée 
$$
\arg(z)\equiv\theta\quad[2\pi].
$$


\pspicture*[](-2,-2)(2,2)
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-2,-2)(2,2)
\rput{0}(1.7,-.2){$1$}
\rput{0}(-.2,-.2){$0$}
\rput{0}(1.5,0){$+$}
\psarc[linecolor=red,arrowsize=6pt]{->}(0,0){1.5}{0}{110}
\psarc{-}(0,0){1.5}{110}{360}
\rput{110}(-0.51303,1.40954){+}
\rput{0}(-0.7,1.6){z}
\rput{0}(1.3,1.3){\red$\theta$}
\endpspicture


Pour chaque nombre complexe $z$ {\bf non nul}, le nombre $z/|z|$ est bien défini 
et appartient au cercle unité $\ob U$. En effet, il est de module $1$ car 
$$
\Q|{z\F |z|}\W|={|z|\F|z|}=1. 
$$

\Definition [] L'argument d'un nombre complexe $z$ {\bf non nul} est l'argument de $z/|z|$. 
Notant $O$, $P$, $M$ les points du plan complexe d'affixe respective $0$, $1$ et $z$, 
c'est la mesure $\theta$ de l'angle orienté $(\vec {OP},\vec {OM})$ en radian, qui est unique modulo $2\pi$, notée 
$$
\arg(z)\equiv\theta\quad[2\pi].
$$

\medskip
\hfill
\pspicture*[](-.5,-.5)(3,3)
\psaxes*[labels=none,ticks=none]{-}(0,0)(-.5,-.5)(3,3)
\psline[linewidth=1.5pt,arrowsize=6pt]{->}(2,0)
\rput{0}(-.2,-.2){$0$}
\rput{0}(2,-.3){$1$}
\rput{0}(2,0){$+$}
\rput{0}(1,-.3){$\vec{OP}$}
\rput{0}(0,2){$+$}
\rput{0}(-.3,2){$i$}
\psline[linewidth=1.5pt,arrowsize=6pt]{->}(2.7,2.4)
\rput{0}(2.7,2.4){$+$}
\rput{41.6335}(.6,.9){$\vec{OM}$}
\rput{0}(2.9,2.2){$z$}
\psarc[linecolor=red]{->}(0,0){2}{0}{41.6335}
\rput{0}(2.1,.8){\red $\theta$}
\rput{0}(1.5,1.8){\red${ z\F |z|}$}
\rput{41.6335}(1.49482,1.32873){$+$}
\endpspicture
\hfill\null
\medskip

\Propriete Deux nombres complexes non nuls sont égaux si, et seulement s'ils ont même module et même argument. 
$$
{\forall (s,z)\in(\ob C^*)^2}, \qquad {s=z\ \Longleftrightarrow\ \Q\{\eqalign{
|s|&=|z|
\cr
\arg(s)&\equiv\arg(z)\quad[2\pi]
}\W.}
$$

\Propriete [$z$ et $s$ nombres complexes non nuls] 
$$
\arg(z\times s)=\arg(z)+\arg(s)\quad[2\pi]\qquad\mbox{et} \arg\Q({z\F s}\W)=\arg(z)-\arg(s)\quad[2\pi].
$$

La propriété suivante est une conséquence immédiate des relations précédente.
\medskip


\Propriete [$z$ nombre complexe non nul] 
$$
\arg(\overline z)\equiv-\arg(z)\quad[2\pi]\qquad\mbox{et}\qquad \arg\Q({1\F z}\W)\equiv-\arg(z)\quad[2\pi]
$$

\Subsection Ccosinus, Cosinus et sinus. 

Parce qu'il permet de définir les fonctions trigonométriques cosinus et sinus, 
le cercle de centre $0$ et de rayon $1$ est parfois appelé cercle trigonométrique. 
\medskip


\Definition [] Pour chaque nombre réel $\theta$, les parties réelles et imaginaires 
de l'unique nombre complexe $z$ de module $1$ et d'argument 
$\theta$ sont notées $\cos(\theta)$ et $\sin(\theta)$. 

%\pspicture*[](-3,-3)(5.2,3)
%\psaxes*[labels=none,ticks=none]{<->}(0,0)(-3,-3)(3,3)
%\pscircle[linewidth=1pt,fillcolor=white](0,0){2.5}
%\rput{40}(0.7,.8){rayon$=1$}
%\rput{40}(1.91511,1.60697){$+$}
%\rput{0}(3.6,1.8){$z=\cos(\theta)+i\sin(\theta)$}
%\rput{0}(0,1.60697){$+$}
%\rput{0}(-.7,1.60697){\red$i\ \sin(\theta)$}
%\psline[linewidth=1.5pt,linecolor=red,linestyle=dotted]{-}(0,1.60697)(1.91511,1.60697)
%\psline[linewidth=1.5pt,linecolor=red,linestyle=dotted]{-}(1.911511,0)(1.91511,1.60697)
%\psline[linewidth=1.5pt,arrowsize=6pt]{->}(1.91511,1.60697)
%\psarc[linecolor=red]{->}(0,0){1}{0}{40}
%\rput{0}(1.1,0.4){\red $\theta$}
%\rput{0}(1.91511,0){$+$}
%\rput{0}(1.91511,-0.3){\red$\cos(\theta)$}
%\psline[linewidth=.5pt]{-}(-1.76777,1.76777)(-2,2)
%\rput{0}(-2.2,2.2){$\ob U$}
%\rput{0}(-.2,2.7){$i$}
%\rput{0}(0,2.5){$+$}
%\rput{0}(-.2,-.2){$0$}
%\rput{0}(2.5,0){$+$}
%\rput{0}(2.7,.2){$1$}
%\endpspicture

\noindent
Les fonctions cosinus $x\mapsto\cos(x)$ et sinus $x\mapsto\sin(x)$ sont $2\pi$-périodiques 
$$
\forall x\in\ob R, \quad \cos(x+2\pi)=\cos(x)\quad\mbox{et} \quad\sin(x+2\pi)=\sin(x)
$$
et bornées 
$$
\forall x\in\ob R, \qquad-1\le \cos(x)\le 1\quad\mbox{et}\quad-1\le\sin(x)\le1.
$$
De plus, en pivotant le cercle trigonométrique d'un quart de tour, on peut exprimer le cosinus en fonction du sinus et vice versa, 
$$
\forall x\in\ob R, \quad \cos\Q(x+{\pi\F2}\W)=-\sin(x) \quad\mbox{et} \quad
\sin\Q(x+{\pi\F2}\W)=\cos(x), \eqdef{translationFTC}
$$ 
comme on peut le voir sur le graphe $y=\cos(x)$ et $y=\sin(x)$ suivants : 


\pspicture*[](-6.8,-1.2)(6.8,1.2)
%\psset{xunit=1cm, yunit=2cm}
\psplot[linecolor=red,plotpoints=1000]{-6.8}{6.8}{x 3.1415 div 180 mul sin}
\psplot[linecolor=blue,plotpoints=1000]{-6.8}{6.8}{x 3.1415 div 180 mul cos}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-6.8,-1.2)(6.8,1.2)
\rput{0}(3.1415,0){$+$}
\rput{0}(3.18,.2){$\pi$}
\rput{0}(-1.5708,0){$+$}
\rput{0}(-1.79,.2){$-{\pi\F2}$}
\rput{0}(1.5708,0){$+$}
\rput{0}(1.65,.2){${\pi\F2}$}
\rput{0}(-.2,.15){$0$}
\rput{0}(-3.1415,0){$+$}
\rput{0}(-3,.2){$-\pi$}
\rput{0}(4.71239,0){$+$}
\rput{0}(4.5,.2){${3\pi\F2}$}
\rput{0}(-4.71239,0){$+$}
\rput{0}(-4.5,.2){$-{3\pi\F2}$}
\rput{0}(-6.283,0){$+$}
\rput{0}(-6.6,.2){$-2\pi$}
\rput{0}(6.283,0){$+$}
\rput{0}(6.1,.2){$2\pi$}
\rput{0}(0,1){$+$}
\rput{0}(-.15,.9){$1$}
\rput{0}(0,-1){$+$}
\rput{0}(-.4,-1){$-1$}
\rput{0}(.2,1.1){$y$}
\rput{0}(6.7,-.1){$x$}
\psline[linewidth=.5pt]{-}(-4.85,-1)(-4,-.65)
\rput{0}(-5.5,-1){\blue cosinus}
\psline[linewidth=.5pt]{-}(-6,-.6)(-6.7,-.40485)
\rput{0}(-5.5,-.6){\red sinus}
\endpspicture

\centerline{
	\tikzpicture[domain=-4:4,samples=66]
		\draw[very thin,color=gray,step={(1.570796327,1)}] (-4.1,-1.1) grid (4.1,1.1);
		\draw[->] (0,0) -- (4.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,1.2) node[left] {$y$};
		\draw[color=red,smooth] plot (\x,{cos(\x r)}) ;
		\node[rotate=-45,color=red] at (1.45,0.4) {$\cos x$};
		\draw[color=blue,smooth] plot (\x,{sin(\x r)});
		\node[rotate=-45,color=blue] at (3.1,0.4) {$\sin x$};
	\endtikzpicture
}%
\Figure [Index=Courbes!cosinus et sinus] Graphes $y=\cos(x)$ et $y=\sin(x)$.

\noindent
Les fonctions {cosinus et sinus sont continues et indéfiniment dérivables sur $\ob R$} et on a 
$$
{
\forall x\in\ob R,\quad
\cos'(x)=-\sin(x)\quad\mbox{et}\quad \sin'(x)=\cos(x).}\eqdef{deriverFTC}
$$
Il résulte de \eqref{deriverFTC} et \eqref{translationFTC} que l'on peut dériver $n$ fois 
ces fonctions de~la fa\c con suivante : 
$$
{\forall n\in\ob N}, \quad\forall x\in\ob R, \quad {\cos^{(n)}(x)=\cos\Q(x+n{\pi\F2}\W)}
\quad\mbox{et}\quad{\sin^{(n)}(x)=\sin\Q(x+n{\pi\F2}\W)}.
$$
Exercice. Prouver l'identité précédente. 
\bigskip

\Subsection CExp, Exponentielle d'un nombre complexe.

La fonction exponentielle est initialement définie sur l'ensemble $\ob R$, 
en tant que bijection réciproque du logarithme népérien $\ln:x\mapsto\int_1^x{\d t\F t}$. 
On la prolonge dans un premier temps à l'ensemble des nombres imaginaires purs pour la prolonger, 
dans un second temps, à~l'ensemble des nombres complexes. 
\bigskip
 
\Definition [] Pour chaque {$\theta\in\ob R$}, l'exponentielle du nombre imaginaire pur $i\theta$ est 
$$
{\e^{i\theta}:=\cos\theta+i\sin\theta}.
$$
\medskip
\noindent
L'exponentielle transforme un nombre imaginaire pur en nombre complexe de $\ob U$ : 
$$
{\forall \theta\in\ob R}, \qquad {\Q|\e^{i\theta}\W|=1}.
$$
De plus, la fonction $\theta\mapsto\e^{i\theta}$ est un morphisme du groupe $(\ob R,+)$ dans le groupe $(\ob U,\times)$. 
Autrement dit, l'exponentielle satisfait 
$$
{\forall (\theta,\varphi)\in\ob R^2}, \qquad {\e^{i(\theta+\varphi)}=\e^{i\theta}\times\e^{i\varphi}}.
$$

\noindent
Chaque nombre complexe peut se mettre sous la forme trigonométrique (la forme trigonométrique du nombre complexe $0$ n'étant pas unique) : 
\bigskip
\Propriete [] Pour chaque nombre complexe {$z\neq0$}, il existe un unique nombre $r>0$ 
et un nombre réel $\theta$, unique modulo $2\pi$, tels que 
$$
z=r\e^{i\theta}. \eqdef{formtrig}
$$
Cette expression est la forme trigonométrique de $z$ et l'on a 
$r=|z|$ et $\arg z\equiv\theta\ \,[2\pi]$. 
\bigskip


\Definition [] L'exponentielle d'un nombre complexe $z=x+iy$, avec $x$ et $y$ réels, est 
$$
\e^z:=\e^x\times\e^{iy}.
$$

\noindent
En particulier, on a 
$$
{\forall z\in\ob C}, \qquad {|\e^z|=\e^{\re(z)}}\qquad\mbox{et}
\qquad{\arg(\e^z)\equiv\im(z)\quad[2\pi]}.
$$


\bigskip
\noindent
La fonction exponentielle est un morphisme du groupe $(\ob C,+)$ dans le groupe $(\ob C^*,\times)$. Autrement dit, 
l'exponentielle satisfait la propriété fondamentale suivante : 
$$
{\forall (s,z)\in\ob C^2}, \qquad{\e^{s+z}=\e^s\times\e^z}.\eqdef{expfon}
$$


\bigskip
\noindent
L'exponentielle d'un nombre complexe $z$ n'est jamais nulle et son inverse est $\e^{-z}$.  
$$
{\forall z\in\ob C}, \qquad {\e^z\neq0}\qquad\mbox{et}\qquad {{1\F\e^z}=\e^{-z}}.
$$
A fortiori, la puissance positive ou non d'une exponentielle se calcule très simplement : 
$$
{\forall z\in\ob C},\qquad {\forall n\in\ob Z},\qquad {(\e^z)^n=\e^{nz}}. 
$$

Enfin, on résoud facilement les équations du type $\e^z=a$ pour $a\neq0$ à l'aide de \eqref{expfon} et de la propriété suivante. 
$$
{
\e^z=1\quad \Longleftrightarrow\quad \exists k\in\ob Z: z=2\pi i k\quad \Longleftrightarrow\quad z\in2\pi i\ob Z}.
$$


\Propriete [] Pour $z\in\ob C$, la fonction {$x\mapsto \e^{zx}$ est indéfiniment dérivable sur $\ob R$} et on a 
$$
\forall x\in\ob R, \qquad {{\d\F\d x}\e^{zx}=z\e^{zx}}.
$$
En particulier, on a $(\e^x)'=\e^x$ pour chaque nombre réel $x$. 

\Section Trigo, Trigonométrie. 

\Subsection Tan, Tangente.

Les solutions de l'équation trigonométrique $\cos(x)=0$ se caractérisent très simplement. 
Ainsi, pour chaque nombre réel $x$, on a 
$$
{\cos x=0\ \Longleftrightarrow\ x\equiv {\pi\F2}\quad[\pi]}.
$$
La fonction tangente $x\mapsto\tan(x)$ est alors définie 
comme le quotient de la fonction sinus par la fonction cosinus pour les nombres réels n'annulant pas le dénumérateur du quotient. 
$$
{
\forall x\not\equiv {\pi\F2}\quad[\pi]}, \qquad {\tan(x):={\sin(x)\F\cos(x)}}.
$$
La fonction tangente est $\pi$-périodique, autrement dit 
$$
{
\forall x\not\equiv {\pi\F2}\quad[\pi]}, \qquad {\tan(x+\pi)=\tan(x)},
$$ 
et impaire, autrement dit 
$$
{
\forall x\not\equiv {\pi\F2}\quad[\pi]}, \qquad {\tan(-x)=-\tan(x)}, 
$$
comme on peut le voir sur le graphe suivant : 

%% Mathematica : Table[{x,Tan[x]},{x,-Pi/2+0.32,Pi/2-0.32,0.02}]%
%%\readdata{\tangraph}{Graphes/GraphTan.txt}
%%\psset{xunit=.6cm,yunit=.6cm}
%\pspicture*[](-7,-3)(7,3)
%\psaxes*[labels=none,ticks=none]{<->}(0,0)(-7,-3)(7,3)
%\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}
%\rput{0}(6.283,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}}
%\rput{0}(3.1415,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}}
%\rput{0}(-3.1415,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}}
%\rput{0}(-6.283,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}}
%\psline[linestyle=dashed,linewidth=.3pt]{-}(1.5707,-3)(1.5707,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(4.7122,-3)(4.7122,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(-1.5707,-3)(-1.5707,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(-4.7122,-3)(-4.7122,3)
%\rput{0}(1.35,-.3){\scalebox{.5}{${\pi\F2}$}}
%\rput{0}(4.4,-.3){\scalebox{0.5}{${3\pi\F2}$}}
%\rput{0}(-1.9,-.3){\scalebox{.5}{$-{\pi\F2}$}}
%\rput{0}(-5.1,-.3){\scalebox{0.5}{$-{3\pi\F2}$}}
%\rput{0}(0.2,-.2){\scalebox{.5}{$0$}}
%\rput{0}(3.34,-.2){\scalebox{.5}{$\pi$}}
%\rput{0}(6.5,-.2){\scalebox{.5}{$2\pi$}}
%\rput{0}(-2.9,-.2){\scalebox{.5}{$-\pi$}}
%\rput{0}(0,1){\scalebox{.5}{$+$}}
%\rput{0}(0,-1){\scalebox{.5}{$+$}}
%\rput{0}(-0.3,1){\scalebox{.5}{$1$}}
%\rput{0}(-.4,-1){\scalebox{.5}{$-1$}}
%\rput{0}(-.3,2.9){\scalebox{.5}{$y$}}
%\rput{0}(6.9,.3){\scalebox{.5}{$x$}}
%\endpspicture
%$$
\medskip

\centerline{%
	\tikzpicture[scale=0.5]
		\draw[clip] (-4.8,-3.1) rectangle (4.8,3.1);
		\draw[very thin,color=gray,clip,step={(1.570796327,1)}] (-4.8,-3.1) grid (4.8,3.1);
		\draw[->] (0,0) -- (4.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,1.2) node[left] {$y$};
		\draw[domain=-4.55:-1.75,samples=66,color=red,smooth] plot (\x,{tan(\x r)}) ;
		\draw[domain=-1.4:1.4,samples=66,color=red,smooth] plot (\x,{tan(\x r)}) ;
		\draw[domain=1.75:4.55,samples=66,color=red,smooth] plot (\x,{tan(\x r)}) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Tangente] Graphe $y=\tan(x)$. 
\medskip

\noindent
La fonction tangente est indéfiniment dérivable sur son ensemble de définition et 
$$
\forall x\not\equiv{\pi\F2}\quad[\pi], \quad{\tan'(x)=1+\tan(x)^2={1\F\cos(x)^2}}.
$$

\Subsection Cotan, Cotangente. 

Les solutions de l'équation trigonométrique $\sin(x)=0$ se caractérisent très simplement. 
Ainsi, pour chaque nombre réel $x$, on a 
$$
{\sin x=0\ \Longleftrightarrow\ x\equiv 0\quad[\pi]}.
$$
La fonction cotangente $x\mapsto\cotan(x)$ est alors définie 
comme le quotient de la fonction cosinus par la fonction sinus pour les nombres réels n'annulant pas le dénumérateur. 
$$
{
\forall x\not\equiv 0\quad[\pi]}, \qquad {\cotan(x):={\cos(x)\F\sin(x)}}.
$$
La fonction cotangente est $\pi$-périodique, autrement dit 
$$
{
\forall x\not\equiv 0\quad[\pi]}, \qquad {\cot(x+\pi)=\cot(x)}
$$ 
et impaire, autrement dit 
$$
{
\forall x\not\equiv 0\quad[\pi]}, \qquad {\cot(-x)=-\cot(x)}, 
$$ 
comme on peut le voir sur le graphe suivant : 

% Mathematica : Table[{x,Cot[x]},{x,0.32,Pi-0.32,0.02}]
%%\readdata{\cotgraph}{Graphes/GraphCot.txt}
%%\psset{xunit=.6cm,yunit=.6cm}
%\pspicture*[](-7,-3)(7,3)
%\psaxes*[labels=none,ticks=none]{<->}(0,0)(-7,-3)(7,3)
%\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\cotgraph}
%\rput{0}(6.283,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\cotgraph}}
%\rput{0}(3.1415,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\cotgraph}}
%\rput{0}(-3.1415,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\cotgraph}}
%\rput{0}(-6.283,0){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\cotgraph}}
%\psline[linestyle=dashed,linewidth=.3pt]{-}(3.1415,-3)(3.1415,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(6.283,-3)(6.283,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(-3.1415,-3)(-3.1415,3)
%\psline[linestyle=dashed,linewidth=.3pt]{-}(-6.283,-3)(-6.283,3)
%\rput{0}(1.35,-.3){\scalebox{.5}{${\pi\F2}$}}
%\rput{0}(4.4,-.3){\scalebox{0.5}{${3\pi\F2}$}}
%\rput{0}(-1.9,-.3){\scalebox{.5}{$-{\pi\F2}$}}
%\rput{0}(-5.1,-.3){\scalebox{0.5}{$-{3\pi\F2}$}}
%\rput{0}(-0.2,-.2){\scalebox{.5}{$0$}}
%\rput{0}(2.94,-.2){\scalebox{.5}{$\pi$}}
%\rput{0}(6,-.2){\scalebox{.5}{$2\pi$}}
%\rput{0}(-6.65,-.2){\scalebox{.5}{$-2\pi$}}
%\rput{0}(-3.4,-.2){\scalebox{.5}{$-\pi$}}
%\rput{0}(0,1){\scalebox{.5}{$+$}}
%\rput{0}(0,-1){\scalebox{.5}{$+$}}
%\rput{0}(-0.3,1){\scalebox{.5}{$1$}}
%\rput{0}(-.4,-1){\scalebox{.5}{$-1$}}
%\rput{0}(-.3,2.9){\scalebox{.5}{$y$}}
%\rput{0}(6.9,-.3){\scalebox{.5}{$x$}}
%\endpspicture
\medskip

\centerline{%
	\tikzpicture[scale=0.5]
		\draw[clip] (-3.2,-3.1) rectangle (3.2,3.1);
		\draw[very thin,color=gray,clip,step={(1.570796327,1)}] (-3.2,-3.1) grid (3.2,3.1);
		\draw[->] (0,0) -- (3.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,3.2) node[left] {$y$};
		\draw[domain=-3:-0.1,samples=66,color=red,smooth] plot (\x,{cot(\x r)}) ;
		\draw[domain=0.1:3,samples=66,color=red,smooth] plot (\x,{cot(\x r)}) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Cotangente]  Graphe $ y=\cot(x)$. 
\medskip

\noindent
Les fonctions tangentes et cotangentes sont inverses l'une de l'autre : 
$$
{
\forall x\not\equiv 0\quad\Q[{\pi\F2}\W]}, \qquad {\cot(x)={1\F\tan(x)}}. 
$$
La fonction {cotangente est indéfiniment dérivable sur son ensemble de définition} et 
$$
\forall x\not\equiv0\quad[\pi], \quad\cot'(x)=-1-\cot(x)^2=-{1\F\sin(x)^2}
$$

\Subsection FormCos, Propriétes des fonctions trigonométriques.
\bigskip


%\Section Parité
%
%La fonction cosinus est paire et la fonction sinus est impaire, autrement dit 
%$$
%\forall x\in\ob R, \quad {\cos(-x)=\cos(x)}\quad\mbox{et}\quad {\sin(-x)=-\sin(x)}.
%$$


%\Section Périodicité
% 
%$$
%\forall x\in\ob R, \qquad{\cos(x+2\pi)=\cos(x)},\qquad{\sin(x+2\pi)=\sin(x),}
%$$

\Concept [] Anti-période $\pi$
 
$$
\forall x\in\ob R, \qquad{\cos(x+\pi)=-\cos(x)}\qquad\mbox{et}\qquad{\sin(x+\pi)=-\sin(x).}
$$
Symétrie centrale du cercle trigonométrique par rapport à l'origine du repère.
\bigskip

\Concept [] Angles supplémentaires

$$
\forall x\in\ob R, \qquad{\cos(\pi-x)=-\cos(x)}\qquad\mbox{et}\qquad{\sin(\pi-x)=\sin(x).}
$$
Symétrie du cercle trigonométrique par rapport à l'axe des ordonnées. 
\bigskip
\Concept [] Angles complémentaires

$$
\forall x\in\ob R, \qquad{\cos\Q({\pi\F2}-x\W)=\sin(x)}\qquad\mbox{et}
\qquad{\sin\Q({\pi\F2}-x\W)=\cos(x).}
$$
Symétrie du cercle trigonométrique par rapport à la première bissectrice du repère...
\bigskip
\Concept [] Relations d'Euler

$$
{\forall\theta\in\ob R}, \qquad {\ds\cos\theta=\re(\e^{i\theta})={\e^{i\theta}+\e^{-i\theta}\F2}}\quad\mbox{et}\quad
{\ds\sin\theta=\im(\e^{i\theta})={\e^{i\theta}-\e^{-i\theta}\F2i}}
$$
Ces relations fondamentales permettent de transformer un problème tri\-go\-no\-mé\-tri\-que réel 
en problème trigonométrique complexe et réciproquement.
\bigskip


\Concept [] Formules d'addition

\noindent
Soient {$a$ et $b$ des nombres réels}. Alors, on a 
$$
\eqalign{
&{\cos(a+b)=\cos a\cos b-\sin a\sin b},\qquad{\cos(a-b)=\cos a\cos b+\sin a \sin b},
\cr
&{\sin(a+b)=\sin a\cos b+\cos a\sin b},\qquad{\sin(a-b)=\sin a\cos b-\cos a\sin b},
\cr
&{\ds\tan(a+b)={\tan a+\tan b\F1-\tan a\tan b}}\ \qquad\mbox{et}\qquad \tan(a-b)={\tan a-\tan b\F1+\tan a\tan b},
}
$$
si les tangentes sont définies...
\bigskip

\Concept [] Duplication de l'angle

\noindent
Pour chaque {$x\in\ob R$}, on a 
$$
{\cos(2x)=\cos^2x-\sin^2x}, \qquad{\sin(2x)=2\sin x\cos x}\quad\mbox{et}\quad
\tan(2x)={2\tan x\F1-\tan^2x}.
$$
Ces formules permettent de déduire la valeur du cosinus, du sinus ou de la tangente d'un angle doublé de la valeur de ces fonctions 
pour l'angle. 
\bigskip
 
\Concept [] Formule de Moivre

$$
{\forall \theta\in\ob R}, \qquad{\forall n\in\ob Z}, \qquad {\cos(n\theta)+i\sin(n\theta)=(\cos\theta+i\sin\theta)^n}. 
$$
La formule précédente est à utiliser en conjonction avec le binôme de Newton. 
\bigskip

Exercice. Développer $\cos(6\theta)$. 
\bigskip
 

\Concept [] Relation entre cosinus et sinus
 
$$
\forall x\in\ob R,\qquad{\cos^2x+\sin^2x=1}.\eqdef{coscpsinc}
$$
Géométriquement, cette relation signifie que le point d'affixe $\cos x+i\sin x$ appartient 
au~cercle de centre $0$ et de rayon $1$ 
\bigskip

\Concept [] Linéarisation d'un produit 

Pour chaque couple $(a,b)$ de nombres réels, on a 
$$
\eqalign{
\sin a \sin b={\cos(a-b)-\cos(a+b)\F2},&\qquad\sin a\cos b={\sin(a+b)+\sin(a-b)\F2_{\strut}}\cr
\mbox{et }\quad\cos a\cos b^{\strut}=&{\cos(a+b)+\cos(a-b)\F2}.
}\eqdef{lin}
$$
Ces formules permettent de transformer un produit de fonctions trigonométriques en sommes de cosinus ou de sinus. 
C'est très utile pour les intégrer, par exemple. 
\bigskip


\Concept [] Factorisation d'une somme 

\noindent
Soient {$p$ et $q$ des nombres réels}. Alors, on a 
\medskip
\noindent
{$\ds\cos p+\cos q=2\cos\Q(\!{p+q\F2}\!\W)\cos\Q(\!{p-q\F2}\!\W)$}, \hfill
{$\ds\cos p-\cos q=-2\sin\Q(\!{p+q\F2}\!\W)\sin\Q(\!{p-q\F2}\!\W)$},
\medskip
\noindent
{$\ds\sin p+\sin q=2\sin\Q({p+q\F2}\W)\cos\Q({p-q\F2}\W)$},\hfill
{$\ds\sin p-\sin q=2\cos\Q({p+q\F2}\W)\sin\Q({p-q\F2}\W)$}.
\medskip
\noindent
Ces formules permettent de factoriser des expressions trigonométriques. 
C'est très utile pour résoudre des équations trigonométriques. 
\bigskip

\Concept [] Tangente de l'angle moitié 

\noindent
Soit {$x\not\equiv \pi\ \,[2\pi]$} un nombre réel et soit 
{$\ds t:=\tan{x\F 2}$}. Alors, on a 
$$
{\cos x={1-t^2\F1+t^2}}\qquad\mbox{et}\qquad
{\sin x={2t\F1+t^2}}.
$$
Si le nombre $x$ satisfait de plus {$\ds x\not\equiv {\pi\F2}\ \,[\pi]$}, on a 
$$
{\tan x={2t\F1-t^2}}.
$$
Ces formules, qui permettent d'exprimer le cosinus, le sinus et la tangente d'un angle en fonction 
de la tangente de l'angle moitié, servent essentiellement à effectuer des changements de variables dans les intégrales. 
\bigskip

\Subsection LineTrigo, Applications classiques.

\bigskip
\Concept Equation trigonométrique $f(x)=f(a)$
 
\noindent
Pour {$a\in\ob R$}, on a 
$$
\eqalign{
&{\cos(x)=\cos(a)\ \Longleftrightarrow\ x\equiv a\ [2\pi]\ \mbox{ ou }\ x\equiv -a\ [2\pi]}
\cr
&{\sin(x)=\sin(a)\ \Longleftrightarrow\ x\equiv a\ [2\pi]\ \mbox{ ou }\ x\equiv \pi-a\ [2\pi]}
}
$$
Si de plus {$\ds a\not\equiv{\pi\F2}\ \,[\pi]$}, on a 
$$
{\tan(x)=\tan(a)\ \Longleftrightarrow\ x\equiv a\ [\pi]}. 
$$ 

\Concept Equation trigonométrique $a\cos(x)+b\sin(x)=c$ 

Soient $a$ et $b$ des nombres réels tels que $(a,b)\neq(0,0)$. 
\bigskip


{{ \centerline{\bf Technique à retenir}
\medskip
\noindent\hfill$\ds
a\cos(x)+b\sin(x)=\underbrace{\sqrt{a^2+b^2}}_{\ss r}\Bigg(\underbrace{{a\F\sqrt{a^2+b^2}}}_{\cos(\theta)}\cos(x)
+\underbrace{{b\F\sqrt{a^2+b^2}}}_{\sin(\theta)}\sin(x)\Bigg)=r\cos(x-\theta).
$\hfill\null\par
}}
\bigskip

\noindent
Pour résoudre l'équation trigonométrique  
$$
a\cos(x)+b\sin(x)=c, \eqdef{eqtrig}
$$

on la transforme pour la mettre sous la forme $\ds\cos(\theta)\cos(x)+\sin(\theta)\sin(x)={c\F r}$, 
c'est-à-dire sous une forme étudiée au paragraphe précédent : 
$$
\cos(x-\theta)={c\F r}. 
$$ 
Pour cela, on divise \eqref{eqtrig} par le nombre strictement positif $r:=\sqrt{a^2+b^2}$ pour~obtenir~que 
$$
\eqref{eqtrig}\ \Longleftrightarrow\ {a\F\sqrt{a^2+b^2}}\cos(x)+{b\F\sqrt{a^2+b^2}}\sin(x)={c\F r}.
$$
Puis, on remarque alors que le point de coordonnées $\Q({a\F\sqrt{a^2+b^2}},{b\F\sqrt{a^2+b^2}}\W)$ 
appartient au cercle trigonométrique et donc qu'il existe un nombre réel $\theta$, unique modulo $2\pi$, tel que 
$$
\Q\{\eqalign{
\cos(\theta)={a\F\sqrt{a^2+b^2}},
\cr
\sin(\theta)={b\F\sqrt{a^2+b^2}}.}
\W.\eqdef{eqtrig2}
$$
On en déduit alors que 
$$
\eqref{eqtrig}\ \Longleftrightarrow\ \cos(\theta)\cos(x)+\sin(\theta)\sin(x)={c\F r}
$$
puis que 
$$
\eqref{eqtrig}\ \Longleftrightarrow\ \cos(x-\theta)={c\F r}.
$$
Deux cas peuvent se produire : 
\medskip

\qquad$\bullet$ Si $|c|\le r$, il existe un nombre réel $\tau$ tel que 
$$
\cos(\tau)={c\F r}
$$
et les solutions réelles de l'équation \eqref{eqtrig} sont les nombres $x$ vérifiant 
$$
x\equiv\theta+\tau\quad[2\pi] \quad\mbox{ou}\quad x\equiv \theta-\tau\quad[2\pi].
$$
\qquad\quad$\bullet$ Si $|c|>r$, l'équation \eqref{eqtrig} n'admet aucune solution réelle. 
\bigskip


\Concept [] Linéarisation

\noindent
Linéariser une expression trigonométrique, c'est l'écrire comme une somme de constantes, de cosinus et de sinus. Ainsi, 
une linéarisation des expressions $\cos^2(x)$ et $\sin^2(x)$ est 
$$
\forall x\in\ob R, \qquad {\cos^2(x)={1+\cos(2x)\F2}}\quad\mbox{et}\quad
{\sin^2(x)={1-\sin(2x)\F2}}.
$$
Pour linéariser une fonction trigonométrique, on utilise les formules \eqref{lin} ainsi que les~relations d'Euler 
conjointement au binôme de Newton. Ainsi, pour linéariser les expressions $\cos^n(x)$ et $\sin^n(x)$ pour des entiers $n\ge3$, 
on écrit 
$$
{
\eqalign{
\cos^n(x)&=\Q({\e^{ix}+\e^{-ix}\F 2}\W)^n={1\F 2^n}\sum_{0\le k\le n}\Q({n\atop k}\W)\e^{i(n-2k)x}, 
\cr
\sin^n(x)&=\Q({\e^{ix}-\e^{-ix}\F 2i}\W)^n={1\F(2i)^n}\sum_{0\le k\le n}\Q({n\atop k}\W)(-1)^k\e^{i(n-2k)x},
}}
$$
puis on transforme les exponentielles complexes en cosinus et en sinus via les relations d'Euler en remarquant que 
$$
\forall x\in\ob R, \qquad \e^{ix}+\e^{-ix}=2\cos(x)\quad\mbox{et}\quad\e^{ix}-\e^{-ix}=2i\sin(x). 
$$

Exercice. Pour $x\in\ob R$, montrer que $\ds \cos^4(x)={\cos(4x)\F8}+{\cos(2x)\F2}+{3\F8}$. 
\bigskip

\Concept [] Développement d'une expression trigonométrique 

\noindent
Développer une expression trigonométrique, c'est écrire une expression trigonométrique 
en fonction de $\cos(x)$ et de $\sin(x)$. En quelque sorte, c'est l'inverse de la linéarisation. 
somme de constantes, de cosinus et de sinus. Ainsi, les formules suivantes forment 
un~``développement'' des quantités $\cos(2x)$ et $\sin(2x)$. 
$$
\forall x\in\ob R, \qquad\cos(2x)=\cos^2(x)-\sin^2(x)=2\cos^2(x)-1\quad\mbox{et}\quad\sin(2x)=2\sin(x)\cos(x). 
$$
Pour développer une fonction trigonométrique, on utilise les formules d'addition ainsi que la~formule de Moivre 
conjointement au binôme de Newton. Ainsi, pour développer les expressions $\cos(nx)$ et $\sin(nx)$ pour des entiers $n\ge3$, 
on écrit 
$$
{
\cos(nx)+i\sin(nx)=(\cos x+i\sin x)^n=\sum_{0\le k\le n}\Q({n\atop k}\W)\cos^k(x)i^{n-k}\sin^{n-k}(x), 
}
$$
en simplifiant éventuellement les termes de puissance paire via la relation \eqref{coscpsinc}. 

Exercice. Pour chaque nombre réel $x$, démontrer que 
$$
\cos(3x)=4\cos^3(x)-3\cos(x)\qquad\mbox{ et }\qquad \sin(3x)=3\sin(x)-4\sin^3(x).
$$ 
Si $\tan(x)$ et $\tan(3x)$ sont définies, prouver de plus que
$$
\tan(3x)={3\tan(x)-\tan^3(x)\F1-3\tan^2(x)}.
$$

\Section Eq2emedg, Equations polynômiales du second degré. 

\Subsection Raccar, Racines carrées. 

\Definition [] Un nombre complexe {$z$ est une racine carrée d'un nombre complexe $a$} si, et~seulement si, 
$$
{z^2=a}.
$$
\medskip

\Propriete [] Le nombre complexe $0$ l'unique racine carrée de $0$. \par
\noindent Les racines carrées d'un nombre complexe {$a$ non nul, de module $r$ et d'argument $\theta$}, sont 
$$
z_1:={-\sqrt r\ \e^{i\theta/2}}\qquad\mbox{et}\qquad z_2:={\sqrt r\ \e^{i\theta/2}}. \eqdef{raccar}
$$
En particulier, chaque nombre complexe non nul admet exactement deux racines carrées. 
\bigskip
\Remarque : l'identité \eqref{raccar} donne les formes trigonométriques des racines carrées de $a\neq0$. 
On peut également chercher les racines carrées $z=x+iy$ du nombre complexe $a=u+iv$ non nul
en résolvant le système
$$
z^2=a\ \Longleftrightarrow\ \Q\{\eqalign{
x^2-y^2&=u
\cr
2xy&=v
}\W.
$$
Avant de se lancer dans la résolution du sytème, il est bon de savoir qu'il admet toujours deux solutions 
mais que ses solutions n'admettent pas forcément une expression simple. 

En effet, le système précédent étant équivalent au système 
$$
 \Q\{\eqalign{
x^2-y^2&=u,
\cr
2xy&=v,
\cr
x^2+y^2&=\sqrt{u^2+v^2},
}\W.
$$
ses deux solutions sont 
$$
z=\pm{\sqrt{u+\sqrt{u^2+v^2}}+i\sgn(v)\sqrt{\sqrt{u^2+v^2}-u}\F\sqrt2}.
$$


\Subsection Eqsecdeg, Equations polynomiales du second degré. 

\Definition [] Une équation polynomiale du second degré est une équation du type 
$$
az^2+bz+c=0,\eqdef{eqsd}
$$
pour des nombres $a\in\ob C^*$ et $(b,c)\in\ob C^2$. Le {discriminant} d'une telle équation est 
$$
{\Delta:=b^2-4ac}. 
$$
\bigskip

\Theoreme [$a\in\ob C^*$ et ${(a,b)}\in\ob C^2$]
\Bullet Si {$\Delta=0$}, alors l'équation \eqref{eqsd} admet une {unique racine} $\ds z={-b\F2a}$, qui est {double}.
\Bullet Si {$\Delta\neq0$}, alors l'équation \eqref{eqsd} admet {deux racines simples} 
$$
z_1:={-b+\omega\F2a}\qquad\mbox{et}\qquad z_2:={-b-\omega\F2a},
$$
où le symbole $\omega$ désigne une racine carrée du discriminant $\Delta$, c'est-à-dire 
 $$
\omega^2=\Delta.
$$

\Theoreme  [$a\in\ob R^*$ et ${(a,b)}\in\ob R^2$] {\bf\ {(cas réel)}.}
\Bullet Si {$\Delta<0$}, alors l'équation \eqref{eqsd} admet {deux racines simples non-réelles}  
$$
z_1:={-b+i\sqrt{-\Delta}\F2a}\qquad\mbox{et}\qquad z_2:={-b-i\sqrt{-\Delta}\F2a}.
$$
\Bullet Si {$\Delta=0$}, alors l'équation \eqref{eqsd} admet {une racine} $\ds z={-b\F2a}$, {double et réelle}.\smallskip
\Bullet Si {$\Delta>0$}, alors l'équation \eqref{eqsd} admet {deux racines réelles simples} 
$$
z_1:={-b+\sqrt\Delta\F2a}\qquad\mbox{et}\qquad z_2:={-b-\sqrt\Delta\F2a}.
$$

 En pratique on résoud les équations polynomiales du second degré en les factorisant.
\medskip
\centerline{Méthode conseillée.} 
\medskip
\noindent\qquad1) Factoriser le nombre $a$. 

2) Mettre sous la forme canonique en faisant apparaitre un carré
$$
\eqref{eqsd}\ \Longleftrightarrow\ a\Q(z^2+{b\F a}z+{c\F a}\W)=0\ \Longleftrightarrow\ a\Q(\underbrace{\Q(z+{b\F2a}\W)^2}_{\mbox{carré}}+\underbrace{{c\F a}-{b^2\F4 a^2}}_{-{\ss\Delta\F\ss 4a^2}}\W)=0.
$$
\vskip-2em
3) Déterminer une racine carrée de $\Delta$. 

4) Utiliser l'identité remarquable $A^2-B^2=(A-B)(A+B)$ pour factoriser 
$$
\eqref{eqsd}\ \Longleftrightarrow\ a(z-z_1)(z-z_2)=0.
$$
\qquad\quad 5) Conclure en utilisant que, dans $\ob C$, un produit de facteur est nul si, 

\qquad\qquad et seulement si, l'un au moins des facteurs est nul.



\Propriete [Title=Lien coefficients-racines] Soient $S$ et $P$ deux nombres complexes. Alors, 
$$
z_1\mbox{ et } z_2 \mbox{ sont solutions de l'équation }z^2-Sz+P=0\ \Longleftrightarrow\ \Q\{\eqalign{z_1+z_2&=S,\cr z_1\ z_2&=P.}\W.
$$ 

\Subsection Racunite, Racines n$^{\mbox{\sevenrm ièmes}}$ de l'unité. 

\Definition [] Soit $n\ge2$ un entier. {Une racine $n^{\mbox{\sevenrm ième}}$ 
d'un nombre complexe $a$} est un nombre complexe $z$ vérifiant 
$$
{z^n=a}.
$$
On appelle {racines $n^{\mbox{\sevenrm ièmes}}$ de l'unité} les racines $n^{\mbox{\sevenrm ièmes}}$ du nombre $1$. 
\bigskip

\Propriete [] Pour $n\ge2$, les racines $n^{\mbox{\sevenrm ième}}$ d'un nombre complexe $a$ non nul, 
de module $r$ et d'argument $\theta$, sont les $n$ nombres distincts 
$$
\root n\of{r}\ \e^{i\theta+{2\pi ik\F n}}\quad\mbox{ pour }\quad k\in\{0,\cdots,n-1\}.
$$
En particulier, {les racines $n^{\mbox{\sevenrm ième}}$ de l'unité sont les $n$ nombres}
$$
{\e^{{2\pi ik\F n}}\quad\mbox{ pour }\quad k\in\{0,\cdots,n-1\}}.
$$


\Definition [] On note $j$ et $\ol j$ les nombres complexes définis par 
$$
{j=\e^{2\pi i\F3}={-1+i\sqrt3\F2}}\qquad\mbox{et}\qquad{\ol j:=j^2=\e^{-2\pi i\F3}={-1-i\sqrt3\F2}}.
$$
Les nombres {$1$, $j$ et $\ol j$ sont les racines $3^{\mbox{\sevenrm ième}}$ de l'unité} et sont {solutions de} l'équation
$$
{z^2+z+1=0}.
$$
\bigskip

\Propriete [] {La somme des racines $n^{\mbox{\sevenrm ième}}$ de l'unité est égale à $0$}. 

\medskip

\Section GeometrieC, Nombres complexes et géométrie plane. 


La structure du plan euclidien est étroitement liée à celle du plan complexe. Ainsi, 
les nombres complexes permettent de caractériser simplement certaines notions géo\-mé\-tri\-ques, 
ce qui permet de traiter par le calcul 
des problèmes a priori géométriques. 
\bigskip


\Subsection objets, Liens géométriques. 

\Concept [] Rappels

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonormé direct $(O,\vec i,\vec j)$. \pn
{L'affixe d'un point} $M$ de coordonnées $(x,y)$ est le nombre complexe $x+i y$.\pn
{L'affixe d'un vecteur} $x\vec i+y\vec j$ est le nombre complexe $x+iy$. \pn
Inversement, {l'image d'un nombre complexe} $x+iy$ est le point $M$ de coordonnées $(x,y)$. \pn
Soient $M$ un point d'affixe $z$ et $P$ un point d'affixe $a$. \pn
Alors, le module {$|z|$ est la distance $OM$} 
et le module {$|z-a|$ est la distance $MP$}. \pn
De plus, si $z\neq0$, l'argument {$\arg z$ est la mesure de l'angle $(\widehat{\vec i,\vec {OM}})$}.



\Concept [] Mesure d'angle

\noindent
Soient $A$, $B$, $C$ et $D$ des points vérifiant $A\neq B$ et $C\neq D$, d'affixes respectives $a$, $b$, $c$, $d$. \pn
Alors, la mesure de l'angle $(\widehat{\vec{AB},\vec {CD}})$ est l'argument 
$$
{(\widehat{\vec{AB},\vec {CD}})=\arg\Q({d-c\F b-a}\W)}.
$$

\Concept [] Alignement

$$
{\mbox{Les points distincts $A$, $B$ et $C$ sont alignés}\ \Longleftrightarrow\ {a-c\F a-b}\in\ob R}. 
$$
\medskip\noindent
Soient deux points distincts $A$ et $B$ d'affixe respective $a$ et $b$. Alors, la droite $(AB)$ est l'ensemble des points $M$ d'affixe 
$$
z=a+\lambda (b-a)\qquad(\lambda\in\ob R).
$$
La droite passant par le point $A$ d'affixe $a$ admettant le vecteur directeur $\vec u\neq\vec0$ d'affixe~$u$ est l'ensemble des points $M$ d'affixe 
$$
z=a+\lambda u\qquad(\lambda\in\ob R).
$$


\Concept [] Produit scalaire

\noindent
Soit $\sc P$ un plan affine euclidien muni d'un repère orthonormé $(O,\vec i,\vec j)$ et soient $\vec u$ et $\vec v$ deux vecteurs de $\sc P$ d'affixe $u=x_1+y_1$ et $v=x_2+y_2$. 
Alors, le produit scalaire $\vec u.\vec v$ du~vecteur~$\vec u$ par le vecteur $\vec v$ est 
$$
{\vec u.\vec v:=x_1x_2+y_1y_2=\re(u\overline v)}.
$$

\Concept [] Orthogonalité 


\Definition [] On dit que deux vecteurs $\vec u$ et $\vec v$ d'un plan affine muni d'un produit scalaire sont orthogonaux et l'on note {$\vec u\bot\vec v$ si, et seulement si, $\vec u.\vec v=0$}. 
\bigskip

\noindent
Soient $A$, $B$, $C$, $D$ des points d'affixe respective $a$, $b$, $c$ et $d$ tels que $A\neq B$. Alors, 
$$
{\mbox{les vecteurs $\vec{AB}$ et $\vec{CD}$ sont orthogonaux}\ \Longleftrightarrow\ {d-c\F a-b}\in i\ob R}. 
$$

\Concept [] Barycentres.

\Definition [] Soit $\sc P$ un plan affine euclidien muni d'un repère orthonormé $(O,\vec i,\vec j)$. \pn Soit $n\ge1$ un entier, soient $M_1,\cdots,M_n$ des points du plan d'affixe respective~$z_1,\cdots,z_n$ 
et soient $\alpha_1,\cdots,\alpha_n$ des nombres réels vérifiant {$\alpha_1+\cdots+\alpha_n\neq0$}. 
Alors, le {barycentre} du système de points pondérés {$\b\{(M_1,\alpha_1),\cdots,(M_n,\alpha_n)\b\}$} est l'unique point $G$ vérifiant 
$$
\alpha_1\vec{GM_1}+\cdots+\alpha_n\vec{GM_n}={\sum_{1\le k\le n}\alpha_k\vec{GM_k}=\vec0}.
$$


\Propriete [] Le barycentre $G$ du système $\b\{(M_1,\alpha_1),\cdots,(M_n,\alpha_n)\b\}$ vérifie 
$$
{\vec{OG}={\sum_{k=1}^n\alpha_k\vec{OM_k}\F\sum_{k=1}^n\alpha_k}}={\alpha_1\vec{OM_1}+\cdots+\alpha_n\vec{OM_n}\F\alpha_1+\cdots+\alpha_n}
$$
et son affixe est 
$$
{z={\sum_{k=1}^n\alpha_kz_k\F\sum_{k=1}^n\alpha_k}}={\alpha_1z_1+\cdots+\alpha_nz_n\F\alpha_1+\cdots+\alpha_n}.
$$

\Exemple. le {milieu} du segment d'extrémités $A(a)$ et $B(b)$ est le point $M$ d'affixe 
$$
{z={a+b\F2}}.
$$

\Subsection Transformations, Transformations élémentaires.

\Concept [] Translation de vecteur $\vec a$ 

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonormé $(O,\vec i,\vec j)$. 
La translation de~vecteur~$\vec a$, d'affixe $a$, est l'application $T_{\vec a}$ qui à chaque point $M$ associe l'unique point $P$ vérifiant 
$$
\vec{MP}=\vec a.
$$
Elle correspond à la transformation du plan complexe 
$${T_a:z\mapsto z+a}. 
$$ 

\Concept [] Homothétie de rapport $\lambda$ et de centre $O$

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonormé $(O,\vec i,\vec j)$. 
L'homothétie de centre~$A$, d'affixe $a$, et de~rapport $\lambda\in\ob R^*$ est l'application $h_{A,\lambda}$ qui à chaque point $M$ du plan associe l'unique point $P$ vérifiant 
$$
\vec{AP}=\lambda\vec{AM}.
$$
Elle correspond à la transformation du plan complexe 
$$
h_{A,\lambda}:z\mapsto a+\lambda(z-a).
$$
L'homothétie $h_\lambda$ de centre~$O$ et de~rapport $\lambda$ correspond à la transformation 
$$
{h_\lambda:z\mapsto \lambda z}. 
$$ 

\Concept [] Rotation d'angle $\theta$ et de centre $O$

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonormé $(O,\vec i,\vec j)$. 
La rotation $r_{A,\theta}$ de centre~$A$, d'affixe $a$, et d'angle~$\theta$ est l'application qui à chaque point $M$ associe le point $P$ vérifiant 
$$
AP=AM\qquad\mbox{et}\qquad (\widehat{\vec{AP},\vec{AM}})\equiv\theta\quad[2\pi].
$$
Elle correspond à la transformation du plan complexe 
$$
r_{A,\theta}:z\mapsto a+\e^{i\theta}(z-a). 
$$ 
La rotation $r_\theta$ de centre $O$ et d'angle $\theta$ correspond à la transformation 
$$
{r_\theta:z\mapsto\e^{i\theta}z}.
$$

\Concept [] Reflexion d'axe $(O,\vec i)$

\noindent
Soient $\sc P$ un plan affine muni d'un repère orthonoirmé $(O,\vec i,\vec j)$ et $\Delta$ une droite. 
La~reflexion d'axe $\Delta$, autrement dit la symétrie orthogonale d'axe $\Delta$, est l'application~$s_\Delta$ qui à chaque point $M$ associe l'unique point $P$ tel que $\Delta$ soit la médiatrice de $[M,P]$. 
\bigskip
\noindent
La reflexion d'axe $(Ox)$ correspond à la transformation 
$$
{
s_{\sss(Ox)}:z\mapsto \overline z}. 
$$ 
La reflexion d'axe $(Oy)$ correspond à la transformation 
$$
{
s_{\sss(Oy)}:z\mapsto -\overline z}.
$$

\Exercice. Trouver l'expression d'une reflexion quelconque d'axe $\Delta$ passant par un point~$A$ et admettant un vecteur directeur $\vec u$. 
\bigskip

\Concept [] Similitude (directe)

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonoirmé $(O,\vec i,\vec j)$. La~similitude (directe) de centre $A(a)$, de rapport $\lambda>0$ et d'angle $\theta$ est la composée commutative de la rotation~$r_{A,\theta}$, 
de centre $a$ et d'angle $\theta$, avec l'homothétier $h_{A,\lambda}$, de centre $A$ et de rapport~$\lambda$. 
Elle~correspond à la transformation 
$$
{z\mapsto a+r\e^{i\theta}(z-a)}. 
$$

\Remarque. Soient $(a,b)\in\ob C^2$. Si $a\notin\{0,1\}$, la transformation du plan complexe 
$$
z\mapsto az+b
$$
est une similitude (directe) de rapport $|a|$, d'angle $\arg a$ et de centre $C$, d'affixe $\ds c={b\F 1-a}$. 
\bigskip

\noindent
Une similitude de rapport $\lambda$ conserve les angles et multiplie les distances par $\lambda$. De plus, Une similitude est directe si elle conserve l'orientation des angles. 

\Concept [] Inversion

\noindent
Soit $\sc P$ un plan affine muni d'un repère orthonoirmé $(O,\vec i,\vec j)$. L'inversion de centre $A$, d'affixe $a$, est l'application qui correspond à la transformation 
$$
{Inv_A:z\mapsto{1\F\overline z-\overline a}}.
$$
Linversion de centre $O$ est l'application correspondant à la transformation 
$$
{Inv:z\mapsto{1\F\overline z}}.
$$

\Remarque. L'inversion de centre $A$ laisse (globalement) invariantes les ``droites passant par $A$'' et transforme : 
\Bullet une droite ne passant pas par $A$ en cercle passant par $A$.
\Bullet un cercle passant par $A$ en droite ne passant par $A$. 
\Bullet un cercle ne passant pas par $A$ en cercle ne passant pas par $A$. 



















\hautspages{Olus Livius Bindus}{Géométrie élémentaire du plan}


\Chapter GEP, Géométrie élémentaire du plan.

\noindent
Dans ce chapitre, le symbole $\sc P$ désigne un plan affine euclidien, c'est-à-dire un plan affine muni d'un produit scalaire $(\vec u,\vec v)\mapsto\vec u.\vec v$. 



\Section CPPplan, Plan affine. 

\Concept [] Vecteurs
 
\noindent
Pour chaque couple $(M,N)$ de points du plan affine $\sc P$, le~vecteur $\vec{MN}$ est uniquement défini par 
$$
N=M+\vec{MN}.
$$

\Concept [] Colinéarité 

\noindent
Dans un plan affine $\sc P$, deux vecteurs $\vec u$ et $\vec v$ sont dits colinéaires s'il existe un nombre réel $\lambda$ tel que 
$$
\vec u=\lambda\vec v\quad\mbox{ou}\quad\vec v=\lambda\vec u.
$$

\Propriete []  Deux vecteurs $\vec u$ et $\vec v$ du plan $\sc P$ sont colinéaires si, et seulement s'il existe un couple $(\lambda,\mu)\neq(0,0)$ de nombres réels tels que 
$$
\lambda \vec u+\mu\vec v=\vec 0.
$$
Une telle égalité est appelée une relation de dépendance linéaire. 

\Concept [] Repères, Bases

\noindent
Dans un plan affine $\sc P$, un repère est constituée par la donnée d'un point $O$ et de deux vecteurs non-colinéaires $\vec i$ et $\vec j$ (ou par la donnée de trois points $O$, $A$, $B$ non-alignés). 
\medskip

\noindent
Si $\vec i$ et $\vec j$ sont non-colinéaires, on dit que les vecteurs $\{\vec i,\vec j\}$ forment une base de ~$\sc P$. 

\noindent
Pour chaque vecteur $\vec u$, il existe alors un unique couple $(\lambda,\mu)$ de nombres réels tels que 
$$
\vec u=\lambda \vec i+\mu\vec j.
$$ 

\Propriete []  Soit $\sc P$ un plan affine muni d'une base $\{\vec i,\vec j\}$. Alors, 
$$
\vec u=a\vec i+b\vec j \mbox{ et }\vec v=c\vec i+d\vec j\mbox{ sont colinéaires}\Longleftrightarrow ad-bc=0.
$$


\Concept [] Coordonnées cartésiennes

\noindent
Soit $\sc P$ un plan affine muni d'un repère $(O,\vec i,\vec j)$. Pour chaque point $M\in\sc P$, il existe un unique couple $(x,y)$ de nombres réels tels que 
$$
\vec{OM}=x\vec i+y\vec j.
$$
Le couple $(x,y)$ constitue les coordonnées du point $M$ dans le repère $(O,\vec i,\vec j)$. Les nombres $x$ et $y$ sont respectivement appelés abscisse et ordonnée du point $M$. 
 
\Concept [] Changement de repère

\noindent
Soit $\sc P$ un plan affine muni de deux repères $(O,\vec i,\vec j)$ et $(\Omega,\vec u,\vec v)$. Alors, il existe un unique $4-uplet$ de nombres réels $a$, $b$, $c$, $d$ tels que 
$$
\Q\{\eqalign{
\vec u=a\vec i+b\vec j,\cr
\vec v=c\vec i+d\vec j
}\W.\eqdef{sys1}
$$ 
et ce $4-$uplet vérifie nécéssairement $ad-bc\neq0$ (sinon $\vec u$ et $\vec v$ seraient colinéaires). 
\medskip

\noindent
Pour exprimer les vecteurs $\vec i$ et $\vec j$ en fonction de $\vec u$ et $\vec v$, il suffit de résoudre ce système d'inconnues $\vec i$ et $\vec j$ et on trouve alors que 
$$
\Q\{\eqalign{
\vec i={d\vec u-b\vec v\F ad-bc},\cr
\vec j={-c\vec u+a\vec v\F ad-bc}
}\W.\eqdef{sys2}
$$
\medskip

\noindent
Soit $M$ un point du plan $\sc P$ et soient $(x,y)$ et $(X,Y)$ ses coordonnées respectives dans les repères $(O,\vec i,\vec j)$ et $(\Omega,\vec u,\vec j')$. Alors, en écrivant 
$$
x\vec i+y\vec j=\vec{OM}=\vec{O\Omega}+\vec{\Omega M}=\vec{O\Omega}+X\vec u+Y\vec v
$$ 
on exprime $x$ et $y$ en fonction de $X$ et $Y$ en rempla\c cant $\vec u$ et $\vec v$ par les expressions du système \eqref{sys1} et en identitfiant les coefficients de $\vec i$ et de $\vec j$. 
\medskip

\noindent
Réciproquement, on peut exprimer $X$ et $Y$ en fonction de $x$ et $y$ en rempla\c cant $\vec i$ et $\vec j$ par les expressions du système \eqref{sys2} et en identitfiant les coefficients de $\vec u$ et $\vec v$. 

\Section CPPplan, Plan affine euclidien.

 
\Concept [] Produit scalaire

\noindent
Un plan affine $\sc P$ est euclidien si et seulement s'il est muni d'un produit scalaire $\vec u.\vec v$, qui~peut également être noté $\langle\vec u,\vec v\rangle$ ou $\langle\vec u|\vec v\rangle$ ou $(\vec u|\vec v)$. 
\medskip
\noindent
Un produit scalaire est une application $(\vec u,\vec v)\mapsto \langle\vec u|\vec v\rangle$ à valeurs
réelles vérifiant 
$$
\eqalignno{ 
	&\forall\vec u\mbox{ et }\vec v \mbox{vecteurs de $\sc P$}, \qquad
	\langle\vec  u|\vec  v\rangle  =  \langle\vec  v|\vec  u\rangle&\mbox{(symétrie)}  \cr
	&\forall  \vec  u  \mbox{  vecteur  de  $\sc  P$},  \qquad\qquad  \langle\vec  u|\vec
	u\rangle=0\Longleftrightarrow \vec u=\vec 0&\mbox{(définie)} \cr &\forall \vec u \mbox{
	vecteur de $\sc P$}, \qquad\qquad \langle\vec u|\vec u\rangle\ge0&\mbox{(positivité)} 
} 
$$
qui est bilinéaire, c'est à dire linéaire à gauche 
$$
\forall \vec u,\vec v \mbox{ et } \vec w \mbox{ vecteurs de $\sc P$}, 
\forall (\lambda,\mu)\in\ob R^2, \quad\langle\lambda\vec u+\mu\vec v|\vec w\rangle=\lambda\langle\vec u|
\vec w\rangle+\mu\langle\vec v|\vec w\rangle
$$ 
et linéaire à droite 
$$
\forall \vec u, \vec v \mbox{ et } \vec w \mbox{ vecteurs de $\sc P$}, 
\forall  (\lambda,\mu)\in\ob  R^2,  \quad\langle\vec  w|\lambda\vec u+\mu\vec v\rangle
=\lambda\langle\vec w|\vec u\rangle+\mu\langle\vec w|\vec v\rangle 
$$

\noindent
{\it Remarque 1 : }Une application $(\vec u,\vec v)\mapsto\vec u.\vec v$ symétrique et linéaire à gauche (ou à droite) est forcément bilinéaire. 
\bigskip

\noindent
{\it Remarque 2 : }Une application bilinéaire $(\vec u,\vec v)\mapsto\langle\vec u|\vec v\rangle$ se comporte comme un produit pour le développement : 
$$
\langle\vec a+\vec b|\vec c+\vec d\rangle=\langle\vec a|\vec c\rangle+\langle\vec a|\vec d\rangle+\langle\vec b|\vec c\rangle+\langle\vec b|\vec d\rangle.
$$

\noindent{\bf Exemples fondamentaux de produits scalaires. }Dans $\ob R^2$ : l'application qui associe aux vecteurs $(x_1,y_1)$ et $(x_2,y_2)$ de $\ob R^2$ le nombre 
$$
\B\langle (x_1,y_1)\B| (x_2,y_2)\B\rangle=\B\langle\pmatrix{x_1\cr y_1 \B|\pmatrix{x_2\cr y_2} \B\rangle:=x_1x_2+y_1y_2}
$$
Dans $\ob C$ : l'application qui associe aux nombres complexes $z_1=x_1+iy_1$ et $z_2=x_2+iy_2$ le nombre 
$$
{\langle z_1,z_2\rangle:=\re\b(\overline{z_1}z_2\b)}=x_1x_2+y_1y_2.
$$
Dans un plan affine $\sc P$ muni d'un repère orthonormé $(O,\vec i,\vec j)$ : l'application qui associe aux vecteurs
$\vec  u=x_1\vec  i+y_1\vec  j$  et  $\vec  v=x_2\vec  i+y_2\vec  j$  le  nombre  $$
\langle\vec u|\vec v\rangle:=x_1x_2+y_1y_2. $$

\Concept [] Orthogonalité 

\noindent
Deux vecteurs $\vec u$ et $\vec v$ d'un plan affine euclidien $\sc P$ sont orthogonaux si, et~seulement~si
$$
\vec u.\vec v=0
$$
Dans ce cas, on note $\vec u\perp\vec u$. 

\Concept [] Norme d'un vecteur

\noindent
La norme d'un vecteur $\vec u$ du plan affine euclidien $\sc P$ est le nombre réel positif 
$$
\|\vec u\|:=\sqrt{\vec u.\vec u}. 
$$

\Concept [] Distance entre deux points

\noindent
La distance $AB$ séparant deux points $A$ et $B$ du plan est le nombre réel positif
$$
{AB=d(A,B):=\|\vec{AB}\|}. 
$$

\Concept [] Théorème de Pythagore

\noindent
Soit $\sc P$ un plan affine euclidien et $ABC$ un triangle. alors, 
$$
ABC \mbox{ est rectangle en }A \Longleftrightarrow BC^2=AB^2+AC^2
$$

\Concept [] Lien du produit scalaire avec distance et angle

\noindent
Pour chaque vecteur $\vec u$ et chaque vecteur $\vec v$ du plan $\sc P$, on a 
$$
\vec u.\vec v=\|\vec u\|.\|\vec v\|.\cos\Q(\widehat{\vec u,\vec v}\W)
\qquad\mbox{si }\vec u\neq\vec 0\mbox{ et }\vec v\neq\vec 0
$$
et
$$
\vec u.\vec v=0\qquad\mbox{si }\vec u=\vec 0\mbox{ ou }\vec v=\vec 0.
$$

\Concept [] Repères orthonormés

\noindent
Un repère $(O,\vec i,\vec j)$ est orthonormé si, et seulement si, $\|\vec i\|=\|\vec j\|=1$ et $\vec i\perp\vec j$. 
\medskip
\noindent
Deux vecteurs $\vec u$ et $\vec v$ forment une base orthonormée $\Leftrightarrow$ $\|\vec u\|=\|\vec v\|=1$ et $\vec
u\perp\vec v$. \medskip \noindent

\Propriete []  Soit $(O,\vec i,\vec j)$ un repère orthonormé du plan euclidien $\sc P$. Alors, nous avons 
$$
\forall M\in\sc P, \qquad \vec{OM}=\underbrace{\langle \vec{OM}|\vec i\rangle}_{\mbox{abscisse }x}\vec i+\underbrace{\langle \vec{OM}|\vec j\rangle}_{\mbox{ordonnée }y}\vec j.
$$ 
Autrement dit, en faisant le produit scalaire de $\vec{OM}$ avec $\vec i$ (respectivement $\vec j$), on trouve la composante du vecteur $\vec {OM}$ selon l'axe $(O,\vec i)$ \b(respectivement selon l'axe $(O,\vec j)$\b). 


\pspicture*[](-2,-1)(5.5,3.5)
\psaxes*[labels=none,ticks=none]{-}(0,0)(-.5,-.5)(5,3)
\psline[linewidth=1pt,arrowsize=6pt]{->}(0,2)
\psline[linewidth=1.5pt,arrowsize=6pt]{->}(2,0)
\rput{0}(-.2,-.2){$O$}
\rput{0}(1,-.3){$\vec i$}
\rput{0}(-.3,1){$\vec j$}
\psline[linewidth=1.5pt,arrowsize=6pt]{->}(4.3,2.4)
\rput{0}(4.8,2.6){$M$}
\psline[linestyle=dotted,linewidth=0.5pt]{-}(0,2.4)(4.3,2.4)
\psline[linestyle=dotted,linewidth=0.5pt]{-}(4.3,0)(4.3,2.4)
\rput{0}(4.3,-.3){$x=\vec{OM}.\vec i$}
\rput{0}(-1.2,2.4){$y=\vec{OM}.\vec j$}
\rput{30}(3.4,1.5){$\|\vec{OM}\|$}
\psarc[]{->}{1}{0}{30}
\rput{0}(1.6,.4){\scalebox{.7 .7}{$(\widehat{\vec i,\vec{OM}})$}}
\psarc[]{<-}{1.5}{30}{90}
\rput{0}(1.2,1.7){\scalebox{.7 .7}{$(\widehat{\vec j,\vec{OM}})$}}
\endpspicture


\Concept [] Lien entre produit scalaire et repère orthonormé

\Propriete []  Soit $\sc P$ un  plan  affine  euclidien,  muni  d'un produit scalaire $\langle.,.\rangle$, 
et soit $(\vec i,\vec j)$ une base orthonormée.  Alors, on a 
$$
\forall  (x_1,y_1,x_2,y_2)\in\ob  R^4,\qquad  \langle   x_1\vec   i+y_1\vec   j|x_2\vec   i+y_2\vec   j\rangle=x_1x_2+y_1y_2.
\eqdef{cascanonique} 
$$

\Remarque 1 : Le donnée d'un produit scalaire détermine quelles sont les bases orthonormés. Réciproquement, 
la donnée d'une base orthonormée détermine uniquement le produit scalaire. 
\bigskip
\Remarque 2 : Etant donnée un produit scalaire quelconque, il suffit de fixer une base orthonormée $\{\vec i,\vec j\}$ 
pour se ramener au cas ``canonique'' ou le produit scalaire est défini par l'expression \eqref{cascanonique}. 
\bigskip


\Concept [] Identification à $\ob R^2$ et à $\ob C$ 

\noindent
La donnée d'un repère orthonormé $(O,\vec i,\vec j)$ permet d'identifier un plan affine euclidien soit à $\ob R^2$ par l'isométrie affine (bijection conservant les distances/le produit scalaire)
$$
\eqalign{\ob R^2&\to \sc P\cr x+iy&\mapsto M=O+x\vec i+y\vec j\quad(\mbox{i.e. }\vec{OM}=x\vec i+y\vec j)}
$$
soit à $\ob C$ par l'isométrie affine (bijection conservant les distances/le produit scalaire)
$$
\eqalign{\ob C&\to\sc P\cr x+iy&\mapsto M=O+x\vec i+y\vec j\quad(\mbox{i.e. }\vec{OM}=x\vec i+y\vec j)}
$$

\Concept [] Dérivation du produit scalaire

\noindent
Soit $I$ un intervalle, soient $f:I\to\ob R^2$ et $g:I\to\ob R^2$ deux applications dérivables (resp. de classe $\sc C^1$) sur $I$ et soit $\langle.|.\rangle$ un produit scalaire du plan affine euclidien $\sc P=\ob R^2$. Alors, l'application $h:x\mapsto \langle f(x)|g(x)\rangle$ est dérivable (resp. de classe $\sc C^1$) sur $I$ et on a 
$$
\forall x\in I,\qquad {h'(x)={\d\F\d x} \langle f(x)|g(x)\rangle= \langle f'(x)|g(x)\rangle+ \langle f(x)|g'(x)\rangle}.
$$
Pour simplifier, on a la ``même'' formule $\ds\langle f|g\rangle'= \langle f'|g\rangle+ \langle f|g'\rangle$ que pour le produit. 


\Section CPPplan, Déterminant et orientation des plans affines.

\Concept [] Determinant

Soit $\sc B=\{\vec i,\vec j\}$ une base du plan affine $\sc P$. 
Alors, pour chaque couple de vecteur $(\vec u,\vec v)$, il existe un unique $4$-uplet de nombres réels tels que 
$$
\Q\{
\eqalign{
&\vec u=a\vec i+b\vec j,
\cr
&\vec v=c\vec i+d\vec j.
}
\W.
$$
et le déterminant dans la base $\sc B$ du couple $(\vec u,\vec v)$ est le nombre réel défini par 
$$
{\det_{\sc B}(\vec u,\vec v)=\det_{\sc B}(a\vec i+b\vec j,c\vec i+d\vec j):=ad-bc}.\eqdef{déterminant}
$$

\Concept [] Colinéarité et alignement 

Soit $\sc B=\{\vec i,\vec j\}$ une base du plan affine $\sc P$. Alors, 
$$
\eqalign{
\mbox{Les vecteurs $\vec u$ et $\vec v$ sont colinéaires }\Longleftrightarrow \det_{\sc B}(\vec u,\vec v)=0
\cr
\mbox{Les points $A$, $B$ et $C$ sont alignés}\Longleftrightarrow \det_{\sc B}(\vec AB,\vec AC)=0
}
$$

\Concept [] Orientation

\Remarque : pour chaque vecteur $\vec u\neq 0$, le vecteur $\vec U:=\vec u/\|\vec u\|$ est de norme $1$ et il existe $2$ vecteurs $\vec V$ tels que $\{\vec U,\vec V\}$ forme une base orthonormée ($\vec V_1$ et $\vec V_2=-\vec V_1$). 
\bigskip

\noindent
Un plan affine n'est pas naturellement orienté. Pour l'orienter, on fixe une base (respectivement un repère) et l'on convient que cette base particulière (resp. ce repère) est directe. 
L'orientation (directe ou non) des autres bases (resp. repères) découle alors de ce choix :  
\bigskip

\Definition []  Soit $\sc P$ un plan affine orienté par le choix d'une base $\sc B$. Alors, 
$$
\eqalign{
{\mbox{les vecteurs $\{\vec u,\vec v\}$ forment une base directe}\Longleftrightarrow \det_{\sc B}(\vec u,\vec v)>0},
\cr
{\mbox{les vecteurs $\{\vec u,\vec v\}$ forment une base indirecte}\Longleftrightarrow \det_{\sc B}(\vec u,\vec v)<0}.
}
$$
\bigskip

\Concept [] Propriétés du déterminant

\noindent
Le déterminant {$(\vec u,\vec v)\mapsto \det_{\sc B}(\vec u,\vec v)$ est une application à valeurs réelles}~vérifiant
$$
\eqalignno{
&\qquad\qquad\qquad{\forall  \vec  u  \mbox{  et  }  \vec  v  \mbox{  vecteurs de $\sc P$}}, \qquad
{\det_{\sc  B}(\vec  u,\vec  v)   =-\det_{\sc  B}(\vec  v,\vec  u)}&\mbox{(anti-symétrie)}  \cr
&\qquad\qquad\qquad\forall \vec u \mbox{ vecteur de $\sc P$}, \qquad\qquad {\det_{\sc B}(\vec
u,\vec u)=0}&\mbox{(alternée)} } $$ et vérifiant ${\forall \vec u, \vec v \mbox{ et } \vec w
\mbox{ vecteurs de $\sc P$}}$ et  $\forall (\lambda,\mu)\in\ob R^2$ les relations $$\Q\{ \eqalign{
&{\det_{\sc B}(\lambda\vec u+\mu\vec v,\vec w)=\lambda\det_{\sc B}(\vec u,\vec w) +\mu\det_{\sc B}(\vec v,\vec
w)} \cr &{\det_{\sc B}(\vec w,\lambda\vec u+\mu\vec v)=\lambda\det_{\sc B}(\vec w,\vec u)+\mu\det_{\sc B}(\vec
w,\vec v)} }\W.\leqno{\mbox{(bilinéarité)}} $$

\noindent
{\it Remarque : }A l'aide de la bilinéarité, on peut déduire l'anti-symétrie du caractère alterné et réciproquement. 
\bigskip

\Concept [] Déterminant dans les bases orthonormées directes

\Propriete []  Soient deux bases orthonormées directes $\sc B$ et $\sc C$ du plan affine euclidien $\sc P$. Alors, on a 
$$
\mbox{$\forall \vec u$ et $\vec v$ vecteurs de }\sc P, \qquad \det_{\sc B}(\vec u,\vec v)=\det_{\sc C}(\vec u,\vec v). 
$$
Autrement dit, le déterminant est le même dans toutes les bases orthonormées directes. 
\bigskip

\Remarque{ 1 : }En pratique, on utilise le déterminant dans une base orthonormée directe. 
Comme il ne dépend pas de la base orthonormée directe $\sc B$ choisie afin de le calculer, pour simplifier, on le note $\Det(\vec u,\vec v)$ plutôt que $\det_{\sc B}(\vec u,\vec v)$. 
\bigskip

\Remarque{  2  :  }Pour  calculer le déterminant $\Det(\vec u,\vec v)$, il on décompose $\vec u$ et $\vec v$ sur une base
$\sc  B={\vec  i,\vec  j}$  orthonormée  directe  (celle   qui   vous   arrange  le  plus)  puis  on  applique  la  formule
\eqref{déterminant}.  \bigskip

\Propriete []  Pour chaque vecteur $\vec u$ et chaque vecteur $\vec v$ du plan affine euclidien orienté~$\sc P$, on a 
$$
{\Det(\vec u,\vec v)=\|\vec u\|.\|\vec v\|.\sin(\widehat{\mbox{$\vec u\vec v$}})}\qquad{\mbox{si }\vec u\neq\vec 0\mbox{ et }\vec v\neq\vec 0}
$$
et
$$
{\Det(\vec u,\vec v)=0}\qquad{\mbox{si }\vec u=\vec 0\mbox{ ou }\vec v=\vec 0}.
$$

 \Exercice. Prouver que l'aire d'un parallélogramme $ABCD$ est $\b|\Det(\vec{AB},\vec{AC})\b|$.  

\Concept [] Cas particulier de $\ob R^2$ et de $\ob C$. 

Le plan affine $\ob R^2$ est naturellement muni d'un produit scalaire et d'une orientation, pour lesquels la base $\{(1,0),(0,1)\}$ est orthonormée directe. 
Le déterminant est alors l'application qui associe aux vecteurs $(a,b)$ et $(c,d)$ de $\ob R^2$ le nombre 
$$
{\Det\B( (a,b),(c,d)\B)=\Det\Q(\pmatrix{a\cr b},\pmatrix{c\cr d} \W):=ad-bc}.
$$
De même, le plan affine complexe $\ob C$ est naturellement muni d'un produit scalaire et d'une orientation , pour lesquels la base $\{1,i\}$ est orthonormée directe. 
Le déterminant est alors l'application qui associe aux nombres complexes $z_1=a+ib$ et $z_2=c+id$ le nombre 
$$
{\Det(z_1,z_2):=\im\b(\overline{z_1}z_2\b)}=ad-bc.
$$


\Concept [] Dérivation du déterminant

\noindent
Soit $I$ un intervalle, soient $f:I\to\ob R^2$ et $g:I\to\ob R^2$ deux applications dérivables (resp. de classe $\sc C^1$) sur $I$ et soit $\sc B$ une base du plan affine euclidien $\sc P=\ob R^2$. 
Alors, l'application $h:x\mapsto \det_{\sc B}\b(f(x),g(x)\b)$ est dérivable (resp. de classe $\sc C^1$) sur $I$ et on a 
$$
\forall x\in I,\qquad {h'(x)={\d\F\d x} \det_{\sc B}\b(f(x),g(x)\b)= \det\b(f'(x),g(x)\b)+\det_{\sc B}\b(f(x),g'(x)\b)}.
$$
On a la ``même'' formule $\det_{\sc B}(f,g)'=\det_{\sc B}(f',g)+ \det_{\sc B}(f,g')$ que pour le produit. 

\Section Reperage, Droites et cercles.

\Subsection Droites, Droites. 

\Concept [] Ligne de niveau

\Propriete []  Soit $A$ un point et $\vec u$ un vecteur non nul du plan affine euclidien $\sc P$. 
Alors, pour chaque nombre réel $\lambda$, l'ensemble des points $M$ vérifiant 
$$
\vec u.\vec{AM}=\lambda\eqdef{OL}
$$
est la droite $\sc D$ de vecteur normal $\vec u$ passant par un point $D$ vérifiant \eqref{OL}, comme par exemple le point $D$ défini par 
$$
\vec AD={\lambda\F\|\vec u\|^2}\vec u.
$$ 

\Propriete []  Soit $A$ un point et $\vec u$ un vecteur non nul du plan affine euclidien $\sc P$. Alors, pour chaque nombre réel $\lambda$, l'ensemble des points $M$ vérifiant 
$$
\Det(\vec u,\vec{AM})=\lambda\eqdef{OL}
$$
est la droite $\sc D$ de vecteur directeur $\vec u$ passant par un point $D$ vérifiant \eqref{OL}, comme par exemple le point $D$ défini par 
$$
\ds\vec AD={\lambda\F\|\vec u\|^2}\vec u.
$$ 

\Concept [] Droite définie par un point et un vecteur directeur

\noindent
Dans le plan affine $\sc P$, la droite $\sc D$ passant par un point $A$ et de vecteur directeur $\vec u$ admet pour paramétrage 
$$
\forall t\in\ob R, \qquad \vec{AM(t)}=t\vec u.
$$
Soient $(O,\vec i,\vec j)$ un repère orthonormé de $\sc P$ et $a,b,c,d$ les nombres réels tels que $\vec{OA}=a\vec i+b\vec j$ et $\vec u=c\vec i+d\vec j$. Alors, un paramètrage de la droite $\sc D$ est 
$$
\forall t\in\ob R, \qquad 
\Q\{\eqalign{
x(t)=a+ct
\cr
y(t)=b+dt
}\W.
$$
Une équation cartésienne de la droite $\sc D$ est 
$$
M(x,y)\in\sc D\Longleftrightarrow \Det(\vec u,\vec{AM})=0\Longleftrightarrow (y-b)c-(x-a)d=0\Longleftrightarrow -dx+cy+ad-bc=0.
$$ 

\Concept [] Droite définie par deux points distincts $A$ et $B$

\noindent
Se ramener au cas précédent en remarquant que $\vec u:=\vec{AB}$ est un vecteur directeur de $\sc D$. 


\Concept [] Droite définie par un point et un vecteur directeur

\noindent
Soit $(O,\vec i,\vec j)$ un repère orthonormé d'un plan affine euclidien $\sc P$. alors, la droite $\sc D$ passant par un point $A(a,b)$ et de vecteur normal $\vec v=c\vec i+d\vec j$ est dirigée par 
le vecteur $\vec u:=-d\vec i+c\vec j$ et admet pour paramétrage 
$$
\forall t\in\ob R, \qquad 
\Q\{\eqalign{
x(t)=a-bt
\cr
y(t)=b+ct
}\W.
$$
Une équation cartésienne de la droite $\sc D$ est 
$$
M(x,y)\in\sc D\Longleftrightarrow \vec v.\vec{AM}=0\Longleftrightarrow (x-a)c+(y-b)d=0\Longleftrightarrow cx+dy=ac+bd.
$$ 
\Remarque : Dans le cas particulier ou le vecteur $\vec v$ est de norme $1$ et ou $\theta$ est l'angle polaire de $\vec v$, autrement dit $\vec v=\cos\theta\vec i+\sin\theta\vec j$, on a l'équation normale de $\sc D$
$$
M(x,y)\in\sc D\Longleftrightarrow x\cos\theta+y\sin\theta=\underbrace{a\cos\theta+b\sin\theta}_{\ds\alpha}.
$$ 

\Concept [] Distance à une droite

Dans le plan affine euclidien, la distance $d(M,\sc D)$ d'un point $M$ à une droite $\sc D$ est la distance $MH$ ou $H$ est la projection orthogonale de $M$ sur $\sc D$. 
Si $\sc D$ passe par un point $A$ et admet un vecteur directeur $\vec u$, on a 
$$
d(M,\sc D)={|Det(\vec u,\vec{AM})|\F\|\vec u\|}.
$$
Si $\sc D$ passe par un point $A$ et admet un vecteur normal $\vec v$, on a 
$$
d(M,\sc D)={|\vec u.\vec{AM}|\F\|\vec u\|}.
$$
\eject
\Subsection Cercles, Cercles. 

\Concept [] Definition et equation cartésienne

\Definition []  Dans un plan affine euclidien, un cercle $\sc C$ de centre $A$ et de rayon $R>0$ est l'ensemble des points $M$ vérifiant 
$$
CM=\|\vec CM\|=R
$$
Soit $(O,\vec i,\vec j)$ un repère orthonormé du plan $\sc P$ et soit $(a,b)$ les coordonnées du point $C$. alors, le cercle $\sc C$ admet pour équation cartésienne 
$$
M(x,y)\in\sc C\Longleftrightarrow \|CM\|^2=R^2\Longleftrightarrow (x-a)^2+(y-b)^2=R^2.
$$


\Concept [] Ligne de niveau

\Propriete []  Soient $A$ et $B$ deux points distincts du plan affine euclidien $\sc P$. 
Alors, le cercle de diamètre $[AB]$ est l'ensemble des points $M$ vérifiant 
$$
\vec{MA}.\vec{MB}=0. 
$$

\Concept [] Intersection d'un cercle et d'une droite

\noindent
Dans un plan affine euclidien, l'intersection d'un cercle $\sc C$ de centre $A$ et de rayon $R>0$ avec une droite $\sc D$ est :
\medskip

\noindent
\item{$\emptyset$}vide si $d(A,\sc D)>R$. 
\medskip

\noindent
\item{$\{*\}$}réduite à un point $M$ si $d(A,\sc D)=R$. La droite $\sc D$ est alors tangente au cercle $\sc C$ en $M$. 
\medskip

\noindent
\item{$\{**\}$}réduite à deux points si $d(A,\sc D)<R$. 
\medskip


\Concept [] Intersection de deux cercles

\noindent
Dans un plan affine euclidien, l'intersection d'un cercle $\sc C$ de centre $A$ et de rayon $R>0$ avec un cercle $\sc C'$ de centre $A'\neq A$ et de rayon $R'$ est :
\medskip
\noindent
\item{$\emptyset$}vide si $AB<|R-R'|$ ou $AB>R+R'$
\medskip
\noindent
\item{$\{*\}$}réduite à un point $M$ si $AB=|R-R'|$ ou $AB=R+R'$. Les cercles $\sc C$ et $\sc C'$ sont alors tangents en $M$. 
\medskip
\noindent
\item{$\{**\}$}réduite à deux points si $|R-R'|<AB<R+R'$. 







\hautspages{Olus Livius Bindus}{Géométrie élémentaire de l'espace}



\Chapter GEE, Géométrie élémentaire de l'espace.

\Section CPPplan, Espace affine. 

\Concept [] Vecteurs
 
\noindent
Pour chaque couple $(M,N)$ de points de l'espace affine $\sc E$, le~vecteur $\vec{MN}$ est uniquement défini par 
$$
N=M+\vec{MN}.
$$

\Concept [] Colinéarité 

\noindent
Deux vecteurs $\vec u$ et $\vec v$ de l'espace $\sc E$ sont colinéaires si, et seulement s'il existe un couple $(\lambda,\mu)\neq(0,0)$ de nombres réels tels que 
$$
\lambda \vec u+\mu\vec v=\vec 0.
$$
Une telle égalité est appelée une relation de dépendance linéaire entre $\vec u$ et $\vec v$. 
\bigskip

\noindent
Trois vecteurs $\vec u$, $\vec v$ et $\vec w$ de l'espace $\sc E$ sont coplanaires si, et seulement s'il existe un triplet $(\lambda,\mu, \rho)\neq(0,0,0)$ de nombres réels tels que 
$$
\lambda \vec u+\mu\vec v+\rho\vec w=\vec 0.
$$
Une telle égalité est appelée une relation de dépendance linéaire entre $\vec u$, $\vec v$ et $\vec w$. 
\bigskip


\Concept [] Repères, Bases

\noindent
Dans un espace affine $\sc E$, un repère est constituée par la donnée d'un point $O$ et de trois vecteurs non-coplanaires $\vec i$, $\vec j$ et $\vec k$ (ou par quatres points $O$, $A$, $B$ non-coplanaires). 
\medskip

\noindent
Si $\vec i$, $\vec j$ et $\vec k$ sont non-coplanaires, on dit que $\{\vec i,\vec j,\vec k\}$ forme une base de ~$\sc E$. 

\noindent
Pour chaque vecteur $\vec u$, il existe alors un unique couple $(\lambda,\mu,\rho)$ de nombres réels tels que 
$$
\vec u=\lambda \vec i+\mu\vec j+\rho\vec k.
$$ 

\Concept [] Coordonnées cartésiennes

\noindent
Soit $\sc E$ un espace affine muni d'un repère $(O,\vec i,\vec j,\vec k)$. Pour chaque point $M\in\sc E$, il existe un unique couple $(x,y,z)$ de nombres réels tels que 
$$
\vec{OM}=x\vec i+y\vec j+z\vec k.
$$
Le couple $(x,y,z)$ constitue les coordonnées du point $M$ dans le repère $(O,\vec i,\vec j,\vec k)$. Les nombres $x$, $y$ et $z$ sont respectivement appelés abscisse, ordonnée et cote du point $M$. 
 
\Concept [] Changement de repère

\noindent
Soit $\sc E$ un espace affine muni de deux repères $(O,\vec i,\vec j,\vec k)$ et $(\Omega,\vec u,\vec v,\vec w)$. Alors, il~existe un unique $9-uplet$ de nombres réels $(a_1, b_1, c_1,a_2,b_2,c_2,a_3,b_3, c_3)$ tel que 
$$
\Q\{\eqalign{
\vec u=a_1\vec i+b_1\vec j+c_1\vec k,\cr
\vec v=a_2\vec i+b_2\vec j+c_2\vec k,\cr
\vec w=a_3\vec i+b_3\vec j+c_3\vec k
}\W.\eqdef{sys3}
$$ 

\noindent
Pour exprimer les vecteurs $\vec i$, $\vec j$ et $\vec k**att$ en fonction de $\vec u$, $\vec v$ et $\vec w$, il suffit de résoudre ce système d'inconnues $\vec i$, $\vec j$ et $\vec k$ et on trouve alors que 
$$
\Q\{\eqalign{
\vec i=A_1\vec u+B_1\vec v+C_1\vec w,\cr
\vec j=A_2\vec u+B_2\vec v+C_2\vec w, \cr
\vec k=A_3\vec u+B_3\vec v+C_3\vec w.
}\W.\eqdef{sys4}
$$

\noindent
Soit $M$ un point de l'espace $\sc E$ et soient $(x,y,z)$ et $(X,Y,Z)$ ses coordonnées respectives dans les repères $(O,\vec i,\vec j,\vec k)$ et $(\Omega,\vec u,\vec v,\vec w)$. Alors, en écrivant 
$$
x\vec i+y\vec j+z\vec k=\vec{OM}=\vec{O\Omega}+\vec{\Omega M}=\vec{O\Omega}+X\vec u+Y\vec v+Z\vec w
$$ 
on exprime $x$, $y$ et $z$ en fonction de $X$, $Y$ et $Z$ en rempla\c cant $\vec u$, $\vec v$ et $\vec w$ par les expressions du système \eqref{sys3} et en identitfiant les coefficients de $\vec i$, $\vec j$ et $\vec k$. 
\medskip

\noindent
Inversement, on exprime $X$, $Y$ et $Z$ en fonction de $x$, $y$ et $z$ en rempla\c cant $\vec i$, $\vec j$ et $\vec k$ par les expressions du système \eqref{sys4} et 
en identitfiant les coefficients de $\vec u$, $\vec v$ et $\vec w$. 


\Concept [] Coordonnées cylindrique

\noindent
Soit $\sc E$ un espace affine muni d'un repère $(O,\vec i,\vec j,\vec k)$. On appelle coordonnées cylindriques d'un point $M$, tout triplet $(r,\theta,z)$ de nombres réels tel que 
$$
\vec{OM}=r\cos(\theta)\vec i+r\sin(\theta)\vec j+z\vec k\Longleftrightarrow\Q\{\eqalign{
x&=r\cos(\theta)\cr
y&=r\sin(\theta)\cr
z&=z
}\W.
$$
Le nombre $z$ est la cote du point $M$ et le couple $(r,\theta)$ sont les coordonnées polaires du projeté orthogonal de $M$ sur la plan $(O,\vec i,\vec j)$. 
\bigskip

\Remarque : pour que les coordonnées cylindriques associées à un point $M$ soient uniquement définies, on pourra prendre $\theta\ [2\pi]$ et enlever la droite $(O,\vec k)$ de l'espace~$\sc E$. 
\bigskip

\Concept [] Coordonnées sphériques

\noindent
Soit $\sc E$ un espace affine muni d'un repère $(O,\vec i,\vec j,\vec k)$. On appelle coordonnées cylindriques d'un point $M$, tout triplet $(r,\theta,\varphi)$ de nombres réels tel que 
$$
\vec{OM}=r\cos(\varphi)\cos(\theta)\vec i+r\sin(\varphi)\cos(\theta)\vec j+r\sin(\theta)\vec k\Longleftrightarrow\Q\{\eqalign{
x&=r\cos(\varphi)\cos(\theta)\cr
y&=r\sin(\varphi)\cos(\theta)\cr
z&=r\sin(\theta)
}\W.
$$

\Remarque{} 1 : pour que les coordonnées sphériques associées à un point $M$ soient uniquement définies, on pourra prendre $\theta\ [2\pi]$, $-{\pi\F 2}<\varphi<{\pi\F2}$ 
et privé de la droite l'espace~$\sc E$ de la droite $(O,\vec k)$ . 
\bigskip
 
\Remarque{} 2 : pour les cartographes, les nombres $\lambda=\theta$ et $\varphi$ désignent respectivement la latitude (l'angle d'élévation par rapport à l'équateur) et la longitude (l'angle par rapport au méridien de Greenwich). 
\bigskip

 \Exercice. Sachant que le rayon terrestre est d'environ $R=40000 km$, que le Lycée Newton de Clichy est situé à~$48.9^\circ$ de lattitude Nord et à $2.3^\circ$ de longitude Est 
et que le vatican est situé à $41^\circ 54'$ Nord et $12^\circ 27'$ Est, quelle distance un corbeau-voyageur doit il parcourir 
pour apporter une longue lettre d'amour enflammée de S.S.I. Olus Livius Bindus à S.S.L.P Jean-Paul ?
\bigskip\eject


\Remarque{} 3 : Tout le monde ne note pas les coordonnées sphériques de la même fa\c con (tant qu'à faire, autant compliquer...). 
Certains utilisent la colatitude (dans certains cas, c'est plus pratique) : c'est à dire l'angle $\theta=(\widehat{\vec k,\vec{OM}})$ qu'on prendra dans $[0,\pi]$. On a alors
$$
\vec{OM}=r\cos(\varphi)\sin(\theta)\vec i+r\sin(\varphi)\sin(\theta)\vec j+r\cos(\theta)\vec k\Longleftrightarrow\Q\{\eqalign{
x&=r\cos(\varphi)\sin(\theta)\cr
y&=r\sin(\varphi)\sin(\theta)\cr
z&=r\cos(\theta)
}\W.
$$

\Section grrrr, Espace affine Euclidien. 

\noindent
Un espace affine $\sc E$ est euclidien si et seulement s'il est muni d'un produit scalaire $\vec u.\vec v$, qui~peut également être noté $\langle\vec u,\vec v\rangle$ ou $\langle\vec u|\vec v\rangle$ ou $(\vec u|\vec v)$. 
\medskip
\noindent
Un produit scalaire  est  une application  $(\vec  u,\vec  v)\mapsto  \langle\vec  u|\vec v\rangle$ à valeurs
réelles~vérifiant 
$$
\eqalignno{                             
&{\forall  \vec  u  \mbox{  et  }  \vec  v  \mbox{  vecteurs  de  $\sc  E$}},  \qquad
\langle\vec    u|\vec   v\rangle   =   \langle\vec   v|\vec   u\rangle&\mbox{(symétrie)}   
\cr
&\forall   \vec   u   \mbox{   vecteur    de    $\sc   E$},   \qquad\qquad\langle\vec   u|\vec
u\rangle=0\Longleftrightarrow \vec u=\vec  0&
\mbox{(définie)}  
\cr  
&\forall  \vec  u  \mbox{vecteur  de  $\sc  E$}, \qquad\qquad \langle\vec u|\vec u\rangle\ge0&\mbox{(positivité)} 
} 
$$
qui est bilinéaire, c'est à  dire  linéaire  à  gauche $$
\forall \vec u,\vec v \mbox{  et  }  \vec  w  \mbox{  vecteurs  de  $\sc  E$}, 
\forall  (\lambda,\mu)\in\ob  R^2,  \quad\langle\lambda\vec  u+\mu\vec v|\vec w\rangle=
\lambda\langle\vec u|\vec w\rangle+\mu\langle\vec v|\vec w\rangle
$$ 
et linéaire à droite 
$$
\forall \vec u, \vec v \mbox{ et } \vec w \mbox{ vecteurs de $\sc E$},   
\forall   (\lambda,\mu)\in\ob   R^2,   \quad  \langle\vec   w|\lambda\vec  u+\mu\vec
v\rangle=\lambda\langle\vec w|\vec u\rangle+\mu\langle\vec w|\vec v\rangle 
$$

\noindent
On retiendra que le produit scalaire est une forme bilinéaire symétrique, définie et positive. 
\bigskip

\noindent{\bf Exemples fondamentaux de produits scalaires. }Dans $\ob R^3$ : l'application qui associe aux vecteurs $(x_1,y_1,z_1)$ et $(x_2,y_2,z_2)$ de $\ob R^3$ le nombre 
$$
\B\langle  (x_1,y_1,z_1)\B|  (x_2,y_2,z_2)\B\rangle=\B\langle\pmatrix{x_1\cr  y_1\cr   z_1}  \B|\pmatrix{x_2\cr  y_2\cr  z_2}
\B\rangle:=x_1x_2+y_1y_2+z_1z_2 
$$ 
Dans un espace affine $\sc E$ muni d'un repère orthonormé $(O,\vec i,\vec j,\vec k)$ :
l'application qui associe aux vecteurs $\vec u=x_1\vec i+y_1\vec j+z_1\vec k$ et $\vec v=x_2\vec i+y_2\vec  j+z_2\vec  k$  le
nombre 
$$ 
\langle\vec u|\vec v\rangle:=x_1x_2+y_1y_2+z_1z_2. 
$$

\Concept [] Orthogonalité 

\noindent
Deux vecteurs $\vec u$ et $\vec v$ d'un espace affine euclidien $\sc E$ sont orthogonaux si, et~seulement~si
$$
\vec u.\vec v=0
$$
Dans ce cas, on note $\vec u\perp\vec u$. 

\Concept [] Norme d'un vecteur

\noindent
La norme d'un vecteur $\vec u$ du plan affine euclidien $\sc E$ est le nombre réel positif 
$$ouaip
\|\vec u\|:=\sqrt{\vec u.\vec u=\sqrt{x^2+y^2+z^2}}. 
$$

\Concept [] Distance entre deux points

\noindent
La distance $AB$ séparant deux points $A$ et $B$ du plan est le nombre réel positif
$$
AB=d(A,B):=\|\vec{AB}\|. 
$$

\Concept [] Théorème de Pythagore

\noindent
Soit $\sc E$ un espace affine euclidien et $ABC$ un triangle. alors, 
$$
ABC \mbox{ est rectangle en }A \Longleftrightarrow BC^2=AB^2+AC^2.
$$

\Concept [] Lien du produit scalaire avec distance et angle

\noindent
Pour chaque vecteur $\vec u$ et chaque vecteur $\vec v$ d'un espace affine $\sc P$, on a 
$$
{\vec u.\vec v=\|\vec u\|.\|\vec v\|.\cos(\widehat{\mbox{$\vec u\vec v$}})}\qquad{\mbox{si }\vec u\neq\vec 0\mbox{ et }\vec v\neq\vec 0}
$$
et
$$
\vec u.\vec v=0\qquad\mbox{si }\vec u=\vec 0\mbox{ ou }\vec v=\vec 0.
$$

\Concept [] Repères orthonormés

\noindent
Un repère $(O,\vec i,\vec j,\vec k)$ est orthonormé si, et seulement si, 
$$
\|\vec i\|=1, \quad \|\vec j\|=1, \quad \|\vec  k\|=1,  \quad  \vec  i\perp\vec  j,  
\quad  \vec i\perp\vec k \quad\mbox{et}\quad \vec j\perp\vec k 
$$
\medskip \noindent Trois vecteurs $\vec u$, $\vec v$ et
$\vec  k$  forment  une  base  orthonormée  si,  et  seulement  si  $$  \|\vec  i\|=1,  \quad
\|\vec   j\|=1,   \quad   \|\vec   k\|=1,   \quad   \vec   i\perp\vec  j,  \quad
\vec i\perp\vec k \quad\mbox{et}\quad \vec j\perp\vec k $$ \medskip

\Propriete []  Soit $(O,\vec i,\vec j,\vec k)$ un repère orthonormé de l'espace euclidien $\sc E$. Alors, 
$$
\forall M\in\sc E, \qquad \vec{OM}=\underbrace{\langle \vec{OM}|\vec i\rangle}_{\mbox{abscisse }x}\vec i+\underbrace{\langle \vec{OM}|\vec j\rangle}_{\mbox{ordonnée }y}\vec j
+\underbrace{\langle \vec{OM}|\vec k\rangle}_{\mbox{cote }z}\vec k.
$$ 
Autrement dit, en faisant le produit scalaire de $\vec{OM}$ avec $\vec i$ (respectivement $\vec j$, avec $\vec k$), on trouve la composante du vecteur $\vec {OM}$ selon l'axe $(O,\vec i)$ \b(respectivement selon l'axe $(O,\vec j)$, 
l'axe $(O,\vec k)$\b). 


\Concept [] Lien entre produit scalaire et repère orthonormé

\Propriete []   Soit  $\sc  P$  un   plan   affine   euclidien,   muni   d'un  produit  scalaire  $\langle.,.\rangle$,  et  soit
$(\vec i,\vec j,\vec k)$ une base orthonormée.  Alors, $\forall  (x_1,y_1,z_1)\in\ob  R^3$  et
$\forall(x_2,y_2,z_2)\in\ob  R^3$,on  a  
$$
\langle  x_1\vec  i+y_1\vec  j+z_1\vec k|x_2\vec i+y_2\vec j+z_2\vec k\rangle=x_1x_2+y_1y_2+z_1z_2. 
\eqdef{Espacecascanonique} 
$$

\Remarque{} 1 : Le donnée d'un produit scalaire détermine quelles sont les bases orthonormés. Inversement, 
la donnée d'une base orthonormée détermine uniquement le produit scalaire. 
\bigskip
\Remarque{} 2 : Etant donnée un produit scalaire quelconque, il suffit de fixer une base orthonormée $\{\vec i,\vec j,\vec k\}$ 
pour se ramener au cas ``canonique'' ou le produit scalaire est défini par l'expression \eqref{Espacecascanonique}. 
\bigskip


\Concept [] Identification à $\ob R^3$

\noindent
La donnée d'un repère orthonormé $(O,\vec i,\vec j,\vec k)$ permet d'identifier un espace affine euclidien à $\ob R^3$ par l'isométrie affine (bijection conservant les distances/le produit scalaire)
$$
\eqalign{
	\ob R^3&\to \sc E\cr 
	(x,y,z)&\mapsto  M=O+x\vec i+y\vec j+z\vec k\quad(\mbox{i.e. }\vec{OM}=x\vec i+y\vec j+z\vec k)
}
$$

\Concept [] Dérivation du produit scalaire

\noindent
Soit $I$ un intervalle, soient $f:I\to\ob R^3$ et $g:I\to\ob R^3$ deux applications dérivables (resp. de classe $\sc C^1$) sur $I$ et soit $\langle.|.\rangle$ un produit scalaire de l'espace affine euclidien $\sc E=\ob R^3$. Alors, l'application $h:x\mapsto \langle f(x)|g(x)\rangle$ est dérivable (resp. de classe $\sc C^1$) sur $I$ et on a 
$$
\forall x\in I,\qquad h'(x)={\d\F\d x} \langle f(x)|g(x)\rangle= \langle f'(x)|g(x)\rangle+ \langle f(x)|g'(x)\rangle.
$$
Pour simplifier, on a la ``même'' formule $\ds\langle f|g\rangle'= \langle f'|g\rangle+ \langle f|g'\rangle$ que pour le produit. 

\Section CPPplan, Déterminant et orientation des espaces affines.

\Concept [] Determinant

\noindent
Soit $\sc B=\{\vec i,\vec j,\vec k\}$ une base de l'espace $\sc E$. Pour chaque triplet de vecteur $(\vec u,\vec v,\vec w)$, il existe un unique $9$-uplet de nombres réels $(a_1,b_1,c_1,a_2,b_2,c_2,a_3,b_3,c_3)$ tels que 
$$
\Q\{
\eqalign{
&\vec u=a_1\vec i+b_1\vec j+c_1\vec k,
\cr
&\vec v=a_2\vec i+b_2\vec j+c_2\vec k,
\cr
&\vec w=a_3\vec i+b_3\vec j+c_3\vec k
}
\W.
$$
et le déterminant dans la base $\sc B$ du triplet $(\vec u,\vec v,\vec k)$ est le nombre réel défini par 
$$
\eqalign{\det_{\sc B}(\vec u,\vec v,\vec w)=\det_{\sc B}\pmatrix{a_1 & a_2  &  a_3\cr  b_1  &  b_2  &  b_3\cr  c_1  &  c_2  &
c_3\cr}:=&a_1b_2c_3+c_1a_2b_3+a_3b_1c_2\cr &-a_3b_2c_1-a_1b_3c_2-c_3a_2b_1}.\eqdef{Espacedéterminant} $$

\Concept [] Coplanéarité et bases. 

Soit $\sc B=\{\vec i,\vec j,\vec k\}$ une base du plan affine $\sc P$. Alors, 
$$
\eqalign{
	\mbox{Les vecteurs $\vec u$, $\vec v$ et $\vec w$ sont coplanaires}
	\Longleftrightarrow \det_{\sc B}(\vec u,\vec v,\vec w)=0
\cr 
	\mbox{Les points $A$, $B$, $C$ et $D$ sont coplanaires}\Longleftrightarrow \det_{\sc B}(\vec AB,\vec AC,\vec AD)=0 
}
$$

\Remarque : $\{\vec u,\vec v,\vec w\}$ forment une base si, et seulement si, $\det_{\sc B}(\vec u,\vec v,\vec w)\neq0$. 
\bigskip

\Concept [] Orientation

\noindent
Un espace affine n'est pas naturellement orienté. Pour l'orienter, on fixe une base (respectivement un repère) et l'on convient que cette base particulière (resp. ce repère) est directe. 
L'orientation (directe ou non) des autres bases (resp. repères) découle alors de ce choix :  
\bigskip

\Definition []  Soit $\sc E$ un espace affine orienté par le choix d'une base $\sc B$. Alors, 
$$
\eqalign{
{\mbox{les vecteurs $\{\vec u,\vec v,\vec w\}$ forment une base directe}\Longleftrightarrow \det_{\sc B}(\vec u,\vec v,\vec w)>0},
\cr
\mbox{les  vecteurs $\{\vec u,\vec v,\vec w\}$ forment une base indirecte}\Longleftrightarrow \det_{\sc B}(\vec u,\vec v,\vec
w)<0. } $$ \bigskip

\Concept [] Propriétés du déterminant

\noindent
Le déterminant {$(\vec u,\vec v,\vec w)\mapsto \det_{\sc B}(\vec u,\vec v,\vec w)$ est une application à valeurs réelles} vé\-ri\-fi\-ant 
$$
{\forall \vec u, \vec v \mbox{ et } \vec w \mbox{ vecteurs de $\sc E$}}, 
\qquad {\eqalign{\det_{\sc B}(\vec u,\vec v,\vec  w)&=-\det_{\sc  B}(\vec u,\vec w,\vec v)\cr&=-\det_{\sc B}(\vec
v,\vec  u,\vec  w)\cr  &=-\det_{\sc  B}(\vec  w,\vec  v,\vec  u)}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\leqno{\mbox{(anti-
symétrie)}} $$

$$ 
\forall \vec u, \vec v \mbox{ vecteurs de $\sc E$}, \qquad\qquad {\Q\{\eqalign{\det_{\sc B}(\vec u,\vec u,\vec v)=0\cr
\det_{\sc B}(\vec u,\vec v,\vec u)=0\cr
\det_{\sc B}(\vec v,\vec u,\vec u)=0
}\W.}\leqno{\mbox{(alternée)}} 
$$
et vérifiant $\forall \vec u, \vec v, \vec w \mbox{ et } \vec x \mbox{ vecteurs de $\sc E$}$ et $\forall
(\lambda,\mu)\in\ob R^2$ les relations 
$$
\Q\{ \eqalign{ &{\det_{\sc B}(\lambda\vec u+\mu\vec v,\vec w,\vec
x)=\lambda\det_{\sc B}(\vec u,\vec w,\vec x) +\mu\det_{\sc B}(\vec v,\vec w,\vec x)} \cr &{\det_{\sc B}(\vec
w,\lambda\vec u+\mu\vec v,\vec x)=\lambda\det_{\sc B}(\vec w,\vec u,\vec x)+\mu\det_{\sc B}(\vec w,\vec v,\vec x)} \cr
&{\det_{\sc B}(\vec w,\vec x,\lambda\vec u+\mu\vec v)=\lambda\det_{\sc B}(\vec w,\vec x,\vec u)+\mu\det_{\sc
B}(\vec w,\vec x,\vec v)} }\W.
\leqno{\mbox{(trilinéarité)}} 
$$

\noindent
{\it Remarque : }A l'aide de la trilinéarité, on peut déduire l'anti-symétrie du caractère alterné et réciproquement. 
\bigskip

\Concept [] Déterminant dans les bases orthonormées directes

\Propriete []  Soient deux bases orthonormées directes $\sc B$ et $\sc C$ de l'espace affine euclidien orienté $\sc E$. Alors, on a 
$$
\mbox{$\forall \vec u$, $\vec v$ et $\vec w$ vecteurs de }\sc E, \qquad \det_{\sc B}(\vec u,\vec v,\vec w)=\det_{\sc C}(\vec u,\vec v,\vec w). 
$$
Autrement dit, le déterminant est le même dans toutes les bases orthonormées directes. 
\bigskip

\Remarque{ 1 : }En pratique, on utilise le déterminant dans une base orthonormée directe. 
Comme il ne dépend pas de la base orthonormée directe $\sc B$ choisie afin de le calculer, pour simplifier, on le note $\Det(\vec u,\vec v,\vec w)$ plutôt que $\det_{\sc B}(\vec u,\vec v,\vec w)$. 
\bigskip

\Remarque{ 2 : }Pour calculer le déterminant $\Det(\vec u,\vec v,\vec w)$, on décompose $\vec u$, $\vec v$ et $\vec w$ sur une base $\sc B=\{\vec i,\vec j,\vec k\}$ orthonormée directe (celle qui vous arrange le plus) 
puis on applique la formule \eqref{Espacedéterminant}. {\it Parfois, la théorie permet de faire plus simple}
\bigskip


\Concept [] Cas particulier de $\ob R^3$

L'espace affine $\ob R^3$ est naturellement muni d'un produit scalaire et d'une orientation, pour lesquels la base 
$\{(1,0,0),(0,1,0),(0,0,1)\}$ est orthonormée directe. 
Le déterminant est alors l'application qui associe aux vecteurs $(a_1,b_1,c_1)$, $(a_2,b_2,c_2)$ et $(a_3,b_3,c_3)$ de $\ob R^3$ le nombre
$$
\mbox{Det}\pmatrix{a_1 & a_2 & a_3\cr b_1 & b_2 & b_3\cr c_1 & c_2 & c_3\cr}
:=a_1b_2c_3+c_1a_2b_3+a_3b_1c_2-a_3b_2c_1-a_1b_3c_2-c_3a_2b_1.
$$

\Concept [] Volume d'un parallélépipède

Soient $\vec u$, $\vec v$ et $\vec w$ trois vecteurs d'un espace affine euclidien $\sc E$. alors, le volume d'un parallélépipède $P$ de coté $\vec u$, $\vec v$ et $\vec w$ est égal à 
$$
{
\mbox{Vol}(P)=\Q|Det(\vec u,\vec v,\vec w)\W|}. 
$$

\Concept [] Dérivation du déterminant

\noindent
Soit $I$ un intervalle, soient $f:I\to\ob R^3$, $g:I\to\ob R^3$ et $h:I\to\ob R^3$ trois applications dérivables (resp. de classe $\sc C^1$) sur $I$ et soit $\sc B$ une base de l'espace affine euclidien $\sc E=\ob R^3$. 
Alors, l'application $J:x\mapsto \det_{\sc B}\b(f(x),g(x),h(x)\b)$ est dérivable (resp. de classe $\sc C^1$)  sur  $I$.  De
plus,  $\forall  x\in  I$,  on a $$ {J'(x)= \det\b(f'(x),g(x),h(x)\b)+\det_{\sc B}\b(f(x),g'(x),h(x)\b)+\det_{\sc
B}\b(f(x),g(x),h'(x)\b)}.  $$  On  a  la  ``même''  formule $\det_{\sc  B}(f,g,h)'=\det_{\sc  B}(f',g,h)+
\det_{\sc B}(f,g',h)+\det_{\sc B}(f,g,h')$ que pour le produit de trois termes.


\Section GEEPV, Produit vectoriel. 


\Concept [] Produit vectoriel

\bigskip
\noindent
Soit {$\sc B=\{\vec i,\vec j,\vec k\}$ une base orthonormée directe} de l'espace affine euclidien orienté $\sc E$. 
Alors, pour chaque vecteurs $\vec u=a_1\vec i+b_1\vec j+c_1\vec k$ et $\vec v=a_2\vec i+b_2\vec j+c_2\vec k$, 
le produit vectoriel de $\vec u$ et de $\vec v$ est le vecteur 
$$
\vec u\wedge\vec v:=(b_1c_2-b_2c_1)\vec i+(c_1a_2-a_1c_2)\vec j+(a_1b_2-a_2b_1)\vec k
$$ 

\Concept [] Propriétés fondamentales

Le produit vectoriel est une application $(\vec u,\vec v)\mapsto \vec u\wedge\vec v$ à valeurs vectorielles vé\-ri\-fiant
$$
\eqalignno{
&{\forall \vec u \mbox{ et } \vec v \mbox{  vecteurs  de $\sc E$}}, \qquad \vec u\wedge\vec v =-\vec v\wedge\vec
u&\mbox{(anti-symétrie)} \cr &{\forall \vec u\mbox{ et } \vec v \mbox{ vecteurs de  $\sc  E$}},
\qquad\qquad     \vec     u\wedge\vec     u=\vec0 & \mbox{(alternée) }     }     $$    qui    est
bilinéaire , c'est à dire   linéaire  à  gauche   
$$
\forall \vec u, \vec v\mbox{ et } \vec w \mbox{ vecteurs de $\sc E$}, 
\forall  (\lambda,\mu)\in\ob  R^2,  
(\lambda\vec u+\mu\vec  v)\wedge\vec  w=\lambda\vec  u\wedge\vec  w+\mu\vec  v\wedge\vec  w
$$ 
et linéaire à droite 
$$
\forall  \vec  u,  \vec  v  \mbox{ et }   \vec   w  \mbox{ vecteurs  de} \sc  E,  
\forall(\lambda,\mu)\in\ob R^2, 
\vec w\wedge(\lambda\vec u+\mu\vec v)=\lambda\vec w\wedge\vec u+\mu\vec w\wedge\vec v
$$ 


\Concept [] Lien géométrique

\noindent
Dans un espace affine euclidien orienté $\sc E$ et soient deux vecteurs $\vec u$ et $\vec v$. Alors, 
\smallskip\noindent
{\bf Si $\vec u$ et $\vec v$ sont colinéaires}, le produit vectoiriel de $\vec u$ par $\vec v$ est le vecteur $\vec u\wedge\vec v=\vec 0$ 
\smallskip\noindent
{\bf Si $\vec u$ et $\vec v$ ne sont pas colinéaires}
le produit vectoiriel de $\vec u$ par $\vec v$ est le vecteur 
$$
\vec u\wedge\vec v=\|\vec u\|.\|\vec v\|.\sin(\widehat{\vec u,\vec v}.)\vec N,
$$
où $N$ est le cecteur unitaire orthogonal à $\vec u$ et $\vec v$ tel que $(\vec u,\vec v,\vec N)$ soit une base directe. 
\bigskip
\Remarque : Pour chaque couple $(\vec u, \vec v)$ de vecteurs non-colinéaires, la base $(\vec u,\vec v,\vec u\wedge\vec v)$ est directe. 
\bigskip

\Concept [] Colinéarité

\noindent
Soient $\vec u$ et $\vec v$ deux vecteurs d'un espace affine euclidien orienté. alors, on a 
$$
{
\mbox{$\vec u$ et $\vec v$ sont colinéaires }\Longleftrightarrow \vec u\wedge\vec v=\vec0}.
$$

\Concept [] Aire d'un parallélélogramme

\noindent
Soient $\vec u$ et $\vec v$ deux vecteurs d'un espace affine euclidien orienté. alors, l'aire du parallélélogramme $P$ de coté $\vec u$ et $\vec v$ est
$$
{
\mbox{\sc Aire }(P)=|\vec u\wedge\vec v|}.
$$

\Concept [] Produit mixte

\noindent
Soient $\vec u$, $\vec v$ et $\vec w$ trois vecteurs d'un espace affine euclidien orienté. alors, on a 
$$
\Det(\vec u,\vec v,\vec w)=(\vec u\wedge\vec v).\vec w.
$$

\Concept [] Double produit vectoriel 

\noindent
Soient $\vec u$, $\vec v$ et $\vec w$ trois vecteurs d'un espace affine euclidien orienté. alors, on a 
$$
\vec u\wedge(\vec v\wedge\vec w)=\langle\vec u|\vec w\rangle\vec v-\langle \vec u|\vec v\rangle\vec w.
$$

\Section DEC, Droites, plans et cercles.

\Subsection EPlans, Plans.

\Concept [] Plan défini par un point et deux vecteurs directeurs

\noindent
Dans l'espace affine $\sc E$, le plan $\sc P$ passant par un point $A$ et de vecteurs directeurs $\vec u$~et~$\vec v$ (non colinéaires) 
admet pour paramétrage 
$$
M\in\sc P\Longleftrightarrow \exists (\lambda,\mu)\in\ob R^2, \qquad \vec{AM}(\lambda,\mu)=\lambda\vec u+\mu\vec v.
$$
Soient $(O,\vec i,\vec j,\vec k)$ un repère orthonormé de $\sc E$ et $x_A,y_A,z_A,a,b,c,a',b',c'$ les nombres réels tels que 
$\vec{OA}=x_A\vec i+y_A\vec j+y_A\vec k$, $\vec u=a\vec i+b\vec j+c\vec k$ $\vec v=a'\vec i+b'\vec j+c'\vec k$
Alors, un~paramètrage du plan $\sc P$ est 
$$
\forall (\lambda,\mu)\in\ob R^2, \qquad 
\Q\{\eqalign{
x(\lambda,\mu)&=x_A+\lambda a+\mu a'
\cr
y(\lambda,\mu)&=y_A+\lambda b+\mu b'
\cr
z(\lambda,\mu)&=z_A+\lambda c+\mu c'
}\W.
$$
\medskip

\Remarque : on peut trouver une éqaution cartésienne d'un tel plan via le déterminant
$$
{
M\in\sc P\Longleftrightarrow \mbox{les vecteurs $\vec{AM}$, $\vec u$ et $\vec v$ sont coplanaires} 
\Longleftrightarrow\mbox{Det}(\vec{AM},\vec u,\vec v)=0}. 
$$

\Concept [] Plan défini par un point et un vecteur normal

\noindent
Dans l'espace affine $\sc E$, le plan $\sc P$ passant par un point $A(x_A,y_A,z_A)$ et de vecteur normal $\vec N=a\vec i+b\vec j+c\vec k$ 
admet l'équation cartésienne 
$$
{
\eqalign{
M(x,y,z)\in\sc P&\Longleftrightarrow \vec N.\vec{AM}=0\Longleftrightarrow a(x-x_A)+b(y-y_A)+c(z-z_A)=0\cr
&\Longleftrightarrow ax+by+cz=\underbrace{ax_A+by_A+cz_A}_{\mbox{constante $d$}}.
}}
$$ 
\Remarque{} 1 : un plan d'équation $ax+by+cz=d$ admet donc le vecteur $\vec N=a\vec i+b\vec j+c\vec k$ pour vecteur normal. 
\medskip

\Remarque{} 2 : en multipliant une équation cartésienne par un nombre non nul, on trouve une autre équation cartésienne du même ensemble. 
\medskip

\Remarque {} 3 : Un plan $\sc P$ passant par l'origine admet une équation cartésienne du type 
$$
M(x,y,z)\in\sc P\Longleftrightarrow ax+by+cz=0
$$
Un plan $\sc P$ ne passant pas par l'origine admet une équation cartésienne du type
$$
M(x,y,z)\in\sc P\Longleftrightarrow ax+by+cz=1,
$$
appellée équation normale du plan {(\it Il suffit de diviser par $d\neq0$).}
\bigskip

\Concept [] Plan défini par trois points non alignés. 

Soient $A$, $B$ et $C$ trois points non alignés. alors, il existe un unique plan $\sc P$ passant par $A$, $B$ et $C$. 
On peut en trouver facilement un paramétrage ou une équation cartésienne. En effet, $\sc P$ passe par le point $A$ et est dirigés par les vecteurs non colinéaires $\vec{AB}$ et $\vec{AC}$. Par ailleurs, le vecteur $\vec{AB}\wedge\vec{AC}$ est normal au plan $\sc P$. 
On peut donc se ramener facilement aux cas précédents. 

\Concept [] Distance à un plan

Dans l'espace affine euclidien, la distance $d(M,\sc P)$ d'un point $M$ à un plan $\sc P$ est la distance $MH$ ou $H$ est la projection orthogonale de $M$ sur $\sc P$. 
Si $\sc P$ passe par un point $A$ et est dirigés par deux vecteurs non-colinéaires $\vec u$ et $\vec v$, on a 
$$
d(M,\sc P)={|Det(\vec u,\vec v,\vec{AM})|\F\|\vec u\wedge\vec v\|}.
$$
Si $\sc P$ passe par un point $A$ et admet un vecteur normal $\vec N$, on a 
$$
d(M,\sc P)={|\vec N.\vec{AM}|\F\|\vec N\|}.
$$


\Subsection EDroites, Droites. 

\Concept [] Droite définie par un point et un vecteur directeur

\noindent
Dans l'espace affine $\sc E$, la droite $\sc D$ passant par un point $A$ et de vecteur directeur $\vec u$ admet pour paramétrage 
$$
{
M\in\sc D\Longleftrightarrow 
\exists t\in\ob R, \qquad \vec{AM}=t\vec u}.
$$
Soient $(O,\vec i,\vec j,\vec k)$ un repère orthonormé de l'espace $\sc E$ et $x_A,y_A,z_A,a,b,c$ les nombres réels 
tels que $\vec{OA}=x_A\vec i+y_A\vec j+z_A\vec k$ et $\vec u=a\vec i+b\vec j+c\vec k$. Alors, un paramètrage de la droite $\sc D$ est 
$$
\forall t\in\ob R, \qquad 
\Q\{\eqalign{
x(t)=x_A+at
\cr
y(t)=y_A+bt
\cr
z(t)=z_A+ct
}\W.
$$

\Concept [] Droite définie par deux points distincts $A$ et $B$

\noindent
Se ramener au cas précédent en remarquant que $\vec u:=\vec{AB}$ est un vecteur directeur de $\sc D$. 
\bigskip

\Concept [] Droite définie comme intersection de deux plans.
 
Dans l'espace affine $\sc E$, on peut définir une droite $\sc D$ comme l'intersection de deux plans non-parrallèles $\sc P_1:ax+by+cz=d$ et $\sc P_2:a'x+by'+cz'=d'$. On a alors
$$
{
M(x,y,z)\in\sc D\Longleftrightarrow M\in\sc P_1\mbox{ et }M\in\sc P_2\Longleftrightarrow\Q\{\eqalign{
ax+by+cz=d,\cr
a'x+b'y+c'z=d'
}\W.}
$$ 
Comme $\sc P_1$ et $\sc P_2$ ne sont pas parallèles, leurs vecteurs normaux $\vec{N_1}=a\vec i+b\vec j+c\vec k$ et 
$\vec{N_2}=a'\vec i+b'\vec j+c'\vec k$ ne sont pas colinéaires et alors $\vec u=\vec{N_1}\wedge\vec{N_2}$ dirige la droite $\sc D$. 


\Concept [] Perpendiculaire commune à deux droites. 

Soient $D$ et $\Delta$ deux droites non-parallèles. Alors leurs vecteurs directeurs respectifs $\vec u$ et $\vec v$ ne sont pas colinéaires et il existe une unique droite $\sc D$ perpendiculaire à $D$ et $\Delta$, qui coupe ces deux droites. Cette perpendiculaire commune $\sc D$ est dirigée par $\vec u\wedge\vec v$. 
\bigskip

\Concept [] Distance à une droite

Dans l'espace affine euclidien orienté, la distance $d(M,\sc D)$ d'un point $M$ à une droite $\sc D$ est la distance $MH$ ou $H$ est la projection orthogonale de $M$ sur $\sc D$. 
Si $\sc D$ passe par un point $A$ et admet un vecteur directeur $\vec u$, on a 
$$
d(M,\sc D)={\|\vec u\wedge\vec{AM}\|\F\|\vec u\|}.
$$

\Subsection Ecube, Sphères.

\Concept [] Definition et equation cartésienne

\Definition []  Dans un espace affine euclidien, une sphère $\sc C$ de centre $A$ et de rayon $R>0$ est l'ensemble des points $M$ 
vérifiant 
$$
AM=\|\vec AM\|=R
$$
Soit $(O,\vec i,\vec j,\vec k)$ un repère orthonormé de l'espace $\sc E$ et soit $(a,b,c)$ les coordonnées du point $A$. 
Alors, la sphère $\sc S$ admet pour équation cartésienne 
$$
M(x,y,z)\in\sc S\Longleftrightarrow \|AM\|^2=R^2\Longleftrightarrow (x-a)^2+(y-b)^2+(z-c)^2=R^2.
$$

\Concept [] Intersection d'une sphère et d'une droite

\noindent
Dans un espace affine euclidien, l'intersection d'une sphère $\sc S$ de centre $A$ et de rayon $R>0$ avec une droite $\sc D$ est :
\medskip

\noindent
\item{$\emptyset$}vide si $d(A,\sc D)>R$. 
\medskip

\noindent
\item{$\{*\}$}réduite à un point $M$ si $d(A,\sc D)=R$. La droite $\sc D$ est alors tangente à la sphère $\sc S$ en~$M$. 
\medskip

\noindent
\item{$\{**\}$}réduite à deux points si $d(A,\sc D)<R$. 
\medskip


\Concept [] Intersection d'une sphère et d'un plan

\noindent
Dans un espace affine euclidien, l'intersection d'une sphère $\sc S$ de centre $A$ et de rayon $R>0$ avec une plan $\sc P$ est :
\medskip

\noindent
\item{$\emptyset$}vide si $d(A,\sc P)>R$. 
\medskip

\noindent
\item{$\{*\}$}réduite à un point $M$ si $d(A,\sc P)=R$. Le plan $\sc P$ est alors tangent à la sphère $\sc S$ en~$M$. 
\medskip

\noindent
\item{$\{**\}$}réduite à un cercle si $d(A,\sc P)<R$, de rayon $\sqrt{R^2-d(A,\sc P)^2}$ et de centre le projeté orthogonal sur $\sc P$ du point $A$. 
\medskip

\Concept [] Intersection de deux sphères

\noindent
Dans un plan affine euclidien, l'intersection d'une sphère $\sc S$ de centre $A$ et de rayon $R>0$ avec une sphère $\sc S'\neq\sc S$ de centre $A'\neq A$ et de rayon $R'>0$ est :
\medskip
\noindent
\item{$\emptyset$}vide si $AB<|R-R'|$ ou $AB>R+R'$
\medskip
\noindent
\item{$\{*\}$}réduite à un point $M$ si $AB=|R-R'|$ ou $AB=R+R'$. Les sphères $\sc C$ et $\sc C'$ sont alors tangents en $M$. 
\medskip
\noindent
\item{$\{**\}$}réduite à un cercle si $|R-R'|<AB<R+R'$, invariant par rapport à l'axe $(AB)$. 

















                  
\hautspages{Olus Livius Bindus}{Fonctions usuelles}


\Chapter FU, Fonctions usuelles.

Dans ce chapitre nous rappelons les propriétés de certaines fonctions fondamentales (logarithme, exponentielle, puissances) 
et nous introduisons d'autres fonctions, qui interviennent notamment lors de la recherche de primitives et, par suite, dans la résolution 
de certaines d'équations différentielles simples.

\Section Fuint, Généralités sur les fonctions. 

\noindent
Dans toute cette partie, $A$ et $B$ désignent des ensembles. 
\bigskip

\Concept [] Produit cartésien et graphe

\Definition []  Le produit cartésien $A\times B$ est l'ensemble 
$$
A\times B:=\b\{(a,b):a\in A \mbox{ et }b\in B\b\}.
$$

\Definition []  Un graphe est un sous-ensemble d'un produit cartésien. 

\Concept [] Application

\Definition []  Une application $f:A\to B$ est un graphe $F$ de $A\times B$ vérifiant 
$$
\forall x\in A, \qquad \exists ! y\in B:\qquad (x,y)\in F.
$$
Cet élément $y$ est noté $y=f(x)$. On dit que $y$ est l'image de $x$ et que $x$ est un antécédent de $y$. 
\bigskip
\noindent
Plus simplement, une application $f:A\to B$ associe à chaque élément $x\in A$ une {\bf unique} image~$y\in B$, que l'on note $y=f(x)$. 
\bigskip

\Concept [] Fonction

\Definition []  Une fonction $f:A\to B$ est un sous-ensemble $F$ du produit $A\times B$ vérifiant 
$$
\forall x\in A, \mbox{ il existe {\bf au plus} un élément }y\in B\mbox{ tel que }(x,y)\in F. 
$$
S'il existe, cet élément $y$ est noté $y=f(x)$. On dit que $y$ est l'image de $x$ et que $x$ est un antécédent de $y$. 

\bigskip
\noindent
Plus simplement, une fonction $f:A\to B$ associe à chaque élément $x\in A$ au plus une~image~$y\in B$, que l'on note $y=f(x)$ si elle existe. 
\bigskip

\Definition []  L'ensemble de définition d'une fonction $f:A\to B$ est l'ensemble $\sc Df$ des éléments $x\in A$ auquels $f$ associe une image : 
$$
{
\sc Df:=\{x\in A:\exists y\in B, y=f(x)\}}.
$$


Travailler avec une application est plus pratique qu'avec une fonction. 
C'est pourquoi, étant donnée une fonction $f:A\to B$, on détermine son ensemble de~définition $\sc Df$, puis on considère l'application 
$$
\eqalign{
	\tilde f:&\sc Df\to B\cr
	x\mapsto f(x)
}
$$

\Concept [] Images directes et réciproques

\Definition []  L'image  d'un ensemble  $C\subset A$  par une application $f:A\to B$ est l'ensemble 
$$
f(C):=\{f(x):x\in C\}.
$$

\Definition []  L'image réciproque  d'un ensemble  $D\subset B$  par une application $f:A\to B$ est l'ensemble 
$$
f^{-1}(D):=\{x\in A:f(x)\in D\}.
$$


\Concept [] Restriction

\noindent
Restreindre une application $f:A\to B$ au départ à un ensemble $C\subset A$, c'est considérer l'application $\tilde f:\vtop{\mbox{$C\to B$}\mbox{$x\mapsto f(x)$}}$.
\bigskip
\noindent
Restreindre une application $f:A\to B$ à l'arrivée à un ensemble $D\subset B$ n'est possible que si $f(A)\subset D$, c'est considérer l'application $\tilde f:\vtop{\mbox{$A\to D$}\mbox{$x\mapsto f(x)$}}$.
\bigskip
\noindent
Restreindre  une  fonction  $f:A\to B$ au départ à $C\subset A$ et à l'arrivée à $D\subset B$ n'est possible que si
$f(C)\subset D$, c'est considérer l'application $\tilde f:\vtop{\mbox{$C\to D$}\mbox{$x\mapsto f(x)$}}$.


\Concept [] Loi de composition

\Definition []  Soient $A$, $B$, $C$ des ensembles et $f:A\to B$ et $g:B\to C$ des applications. Alors, la composée $g\circ f$ est l'application 
$$
g\circ f:\vtop{\mbox{$A\to C$}\mbox{$x\mapsto g\b(f(x)\b)$}}
$$

\Propriete []  La loi de composition $\circ$ des applications est associative. Soient $A$, $B$, $C$, $D$ des ensembles et $f:A\to B$, $g:B\to C$ et $h:C\to D$ des applications. Alors, on a 
$$
h\circ(g\circ f)=(h\circ g)\circ f.
$$
Ce produit est noté plus simplement $h\circ g\circ f$. 
\bigskip

\Concept [] Injectivité

\Definition []  Une application $f:A\to B$ est injective (une injection) si, et seulement, si~tout élément de $B$ a au plus un antécédent dans $A$. 
$$
\forall (x_1,x_2)\in A^2, \qquad f(x_1)=f(x_2)\Longrightarrow x_1=x_2.
$$
Autrement dit, une application est injective si, et seulement si, elle associe deux images différentes à éléments différents. 

\Concept [] Surjectivité

\Definition []  Une application $f:A\to B$ est surjective (une surjection) si, et seulement, si~tout élément de $B$ admet au moins un antécédent dans $A$. 
$$
\forall y\in B, \qquad \exists x\in A:\qquad y=f(x).
$$
Autrement dit, une application $f:A\to B$ est surjective si, et seulement si, $B=f(A)$. 

\Concept [] Bijectivité

\Definition []  Une application $f:A\to B$ est bijective (une bijection) si, et seulement, si~elle est injective et surjective .
\bigskip \noindent En particulier, une fonction $f:A\to B$ est bijective si, et seulement si, tout élément $x\in A$ a une
image dans $B$ et tout élément $y\in B$ a exactement un antécédent dans $A$.  \bigskip


\Definition []  Soit $A$ un ensemble. Alors l'identité de $A$ est l'application $\mbox{Id}_A:\vtop{\mbox{$A\to A$}\mbox{$x\mapsto x$}}$.

\Propriete []  Soient $A$, $B$ deux ensembles et $f:A\to B$ une bijection. Alors, il existe une et une seule application $g:B\to
A$ telle que $$ {f\circ g=\mbox{Id}_B}\qquad\mbox{et}\qquad {g\circ f=\mbox{Id}_A}. $$ Cette application $g$ est bijective.
On la note $f^{-1}$ et on l'appelle bijection réciproque~de~$f$.  \medskip\noindent Pour chaque $y\in B$, la
quantité $f^{-1}(y)$ est l'unique antécédent $x\in A$ de l'élément $y$ par~$f$. \bigskip

 

\Section Fulog, Logarithme, exponentielle, puissances, fonctions hyperboliques.

\Subsection Fuloga, Logarithme.

\noindent
Le logarithme népérien est l'unique primitive de la fonction $\ds x\mapsto{1\F x}$ sur l'intervalle $\Q]0,+\infty\W[$, qui s'annule en $1$. 
\bigskip

\Definition []  Le logarithme népérien est l'application $\ln:\Q]0,+\infty\W[\to\ob R$ définie par 
$$
\forall x>0 , \qquad  \ln(x):=\int_1^x{\d t\F t }.
$$
\noindent\smash{\raise-3mm\llap{ }}Utiliser le logarithme seulement pour des nombres réels strictement positifs : 
le logarithme d'un nombre complexe quelconque ne sera défini qu'au niveau Bac+3...
\bigskip

Le logarithme népérien est une application continue, strictement croissante et in\-dé\-fi\-ni\-ment dé\-ri\-va\-ble sur l'intervalle $\Q]0,+\infty\W[$. En particulier, on a 
$$
\forall x>0 , \qquad \ln'(x):={1\F x }.
$$


Le logarithme népérien est un morphisme du groupe $\Q(\Q]0,+\infty\W[,\times\W)$ dans le groupe $(\ob R,+)$. Plus simplement, 
le logarithme d'un produit est la somme des logarithme. 
$$
\forall x>0 , \qquad \forall y>0 , \qquad \ln(xy)=\ln(x)+\ln(y). \eqdef{fondlog}
$$
Comme $\ln(1):=0$, le logarithme de l'inverse est l'opposé du logarithme. 
$$
\forall x>0, \qquad {\ln\Q({1\F x}\W)=-\ln(x)}. 
$$
Plus généralement, le logarithme d'un quotient est la différence des logarithmes.
$$
\forall x>0 , \qquad \forall y>0 , \qquad{\ln\Q({x\F y}\W)=\ln(x)-\ln(y)}.
$$
et la logarithme d'une puissance est 
$$
\forall x>0 , \qquad \forall n\in\ob Z , \qquad\ln\Q(x^n\W)=n\ln(x).
$$
Le logarithme népérien n'est pas la seule application vérifiant la propriété~\eqref{fondlog}. En effet, elle est vérifiée 
par les logarithmes définis pour d'autres bases de la fa\c con suivante : 
\bigskip 

\Definition []  Le logarithme en base $a>1$ est l'application $\log_a:\Q]0,+\infty\W[\to\ob R$ définie par 
$$
\forall x>0 , \qquad  \log_a(x):={\ln(x)\F\ln(a) }.
$$
Le logarithme en base $10$ sera simplement noté $\log$ ou $\mbox{Log}$ au lieu de $\log_{10}$. 
\medskip

\Propriete []  $\ds \lim_{x\to0^+}\ln(x)=-\infty$ et $\ds\lim_{x\to+\infty}\ln(x)=+\infty$. 
\medskip

\pspicture*[](-.5,-2.4)(11,2.4)
%\psset{xunit=1cm, yunit=1cm}
\psplot[linecolor=red,plotpoints=1000]{0.05}{11}{x ln}
\psplot[linecolor=blue,plotpoints=1000]{0.001}{11}{x log}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-.5,-2.4)(11,2.4)
\rput{0}(-.2,2.2){$y$}
\rput{0}(10.8,-.15){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1.1,-.25){$1$}
\rput{0}(2.718281828,0){$+$}
\rput{0}(2.718281828,-.25){$\e$}
\rput{0}(10,0){$+$}
\rput{0}(10,-.25){$10$}
\rput{0}(-.2,-.15){$0$}
\rput{14}(5,1.85) $y=\ln(x)$ 
\psline[linewidth=.5pt,linestyle=dashed]{-}(0,1)(10,1)
\psline[linewidth=.5pt,linestyle=dashed]{-}(2.718281828,0)(2.718281828,1)
\psline[linewidth=.5pt,linestyle=dashed]{-}(10,0)(10,1)
\rput{0}(10,1){$+$}
\rput{0}(2.718281828,1){$+$}
\rput{4}(5,0.45){\blue $y=\log_{10}(x)$}
\rput{0}(0,1){$+$}
\rput{0}(-.2,1){$1$}
\endpspicture

\medskip
\centerline{%
	\tikzpicture[scale=0.5]
		\draw[very thin,color=gray] (-0.1,-3.1) grid (10.1,2.4);
		\draw[->,thick] (0,0) -- (10.2,0) node[below] {$x$};
		\draw[->,thick] (0,0) -- (0,2.5) node[left] {$y$};
		\draw[domain=0.045:10.1,samples=66,color=blue,smooth] plot (\x,{ln(\x)}) node [above left,rotate=13] {$\ln(x)$};
		\draw[domain=0.001:10.1,samples=233,color=red,smooth] plot (\x,{ln(\x)/ln(10)}) node [above left,rotate=8] {$\log(x)$};
	\endtikzpicture
}%
\Figure [Index=Courbes!Logarithme] Graphes  $y=\ln(x)$  et  $y=\log(x)$ . 
\medskip

\Propriete []  Pour $a>1$, le logarithme $\log_a:\Q]0,+\infty\W[\to\ob R$ est une application bijective et strictement~croissante. 
\bigskip


\Remarque{ \it 1} : les mathématiciens s'accordent pour dire que l'expression~$\ln(3)$ se prononce ``logarithme de trois'' et non pas ``Hélène de Troie'' (quelle faute de gout !)...de la même fa\c con que $\cos(\pi)$ se prononce ``cosinus pi'' et non pas ``cos pi''. Faites attention à ne pas éccorcher le nom des fonctions, certains esprits chagrins pourraient vous le reprocher. 
\bigskip
\Remarque{ \it2} : la signification de la notation ``$\log$'' peut varier selon les domaines, contrairement à ``$\ln$''. Par exemple, en licence de mathématiques, la notation $\log$ représente presque toujours le logarithme népérien. 
\bigskip
\Remarque{ \it 3} : le nombre de chiffres de la représentation d'un nombre $x$ en base $a$ (binaire, octal, décimal, hexadécimal...) est lié au 
logarithme en base $a$ de ce nombre. 
\bigskip


 Exercice :  A l'aide de la calculatrice, déterminer le nombre de chiffres de ${666}^{1337}$. 

\Subsection Fuexp, Exponentielle réelle.


\Definition []  L'exponentielle $\exp:\ob R\to\Q]0,+\infty\W[$ est la bijection réciproque du logarithme népérien $\ln:\Q]0,+\infty\W[\to\ob R$. Pour simplifier, on utilise les notations suivantes 
$$
\e:=\exp(1)=2.718281828\cdots\qquad\mbox{et}\qquad 
\forall x\in \ob R ,\qquad \e^x:=\exp(x) .
$$
En particulier, on a 
$$
\forall x>0, \qquad {\e^{\ln x}=x}\qquad\mbox{et}\qquad
\forall x\in\ob R,\qquad\ln(\e^x)=x
$$
L'exponentielle est strictement croissante. De plus, $\ds\lim_{x\to-\infty}\e^x=0$ et $\ds\lim_{x\to+\infty}\e^x=+\infty$.     
    
%%\readdata{\expgraph}{Graphes/GraphExp.txt}     %\psset{xunit=1cm, yunit=0.8cm}           
\pspicture*[](-7,-.4)(2,5)          \dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\expgraph}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-7,-.4)(2,5)  \rput{0}(-.2,4.8){$y$}   \rput{0}(1.9,-.25){$x$}  \rput{0}(1,0){$+$}
\rput{0}(1.1,-.25){$1$}    \rput{0}(2.718281828,0){$+$}     \rput{0}(2.718281828,-.25){$\e$}     \rput{0}(0,2.718281828){$+$}
\rput{0}(10,-.25){$10$}       \rput{0}(-.2,-.15){$0$}       \rput{0}(-.2,2.718281828){$\e$}      \rput{0}(1,2.718281828){$+$}
\psline[linewidth=.5pt,linestyle=dashed]{-}(1,0)(1,2.718281828)                    \psline[linewidth=.5pt,linestyle=dashed]{-
}(0,2.718281828,0)(1,2.718281828)    \rput{0}(10,1){$+$}    \rput{0}(2.718281828,1){$+$}    \rput{0}(-4,0.3){\red   $y=\e^x$}
\rput{0}(0,1){$+$}  \rput{0}(-.2,1){$1$}  \endpspicture  

\medskip
\centerline{%
	\tikzpicture[scale=0.8]
		\draw[very thin,color=gray] (-5.1,-0.1) grid (1.5,4.2);
		\draw[->] (0,0) -- (1.7,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,5.2) node[left] {$y$};
		\draw[domain=-5.1:1.5,samples=66,color=blue,smooth] plot (\x,{exp(\x)});
	\endtikzpicture
}%
\Figure [Index=Courbes!Exponentielle]  Graphe    $y=\e^x$ .
\medskip  

\noindent
L'exponentielle réelle est une application continue et indéfiniment dérivable sur $\ob R$. De  plus,  on  a  
$$  
\forall x\in\ob  R,  \qquad\exp'(x)=\exp(x) .  
$$  
Si  $f:I\to\ob C$ est dérivable en $a\in I$ , alors la
fonction    $g:x\mapsto    \e^{f(x)}$    est     dérivable     en     $a$     et     on    a    
$$    
{\d\F\d x}\Q(\e^{f(x)}\W)(a)=g'(a)=f'(a)\e^{f(a)}. 
$$

L'exponentielle est un morphisme du groupe $(\ob R,+)$ dans le groupe $\Q(\Q]0,+\infty\W[,\times\W)$. Plus~simplement, 
l'exponentielle d'une somme est le produit des exponentielles. 
$$
\forall(x,y)\in\ob R^2, \qquad {\e^{x+y}=\e^x\times\e^y}. \eqdef{fondexp}
$$
Comme $\e^0:=1$, l'inverse de l'exponentielle est l'exponentielle de l'opposé. 
$$
\forall x\in\ob R, \qquad {{1\F\e^x}=\e^{-x}}. 
$$
Plus généralement, l'exponentielle d'une différence est le quotient des exponentielles.
$$
\forall (x,y)\in\ob R^2, \qquad{\e^{x-y}={\thinspace \e^x\thinspace\F\e^y}}.
$$
et la puissance (entière) d'une exponentielle est 
$$
\forall (x,n)\in\ob R\times \ob Z , \qquad \Q(\e^x\W)^n=\e^{nx }.\eqdef{fondexpa}
$$

\Subsection Fupui, Puissances. 

\noindent
Le logarithme et l'exponentielle (complexe) permettent de définir la puissance d'un nombre strictement positif pour des exposants qui ne sont pas des entiers.
\bigskip

\Definition Le nombre {\bf strictement positif} $a$ à la puissance complexe $z$ est 
$$
\forall a>0 ,\qquad \forall z\in\ob C ,\qquad a^z:=\e^{z\ln(a)}.
$$

\Propriete [$a>1$] La bijection réciproque de $\log_a:\Q]0,+\infty\W[\to\ob R$ 
est l'application strictement croissante $x\mapsto a^x$. 
$$
\forall x\in\ob R, \qquad \log_a(a^x)=x \qquad\mbox{et}\qquad \forall x>0 , \qquad{a^{\log_a(x)}=x}.
$$

L'exponentielle permet de définir les fonctions puissances, les racines carrées, cubiques...
\bigskip 
\Definition []  Pour chaque entier $n\ge2$ , la racine $n^{ieme}$ de  $x\ge0$  est le nombre positif 
$$
{\root n\of x}:=x^{1/n}=\exp\Q({\ln x\F n}\W).
$$ 

L'allure du graphe d'une fonction puissance $x\mapsto x^\alpha$ d'exposant strictement positif 
varie selon que $\alpha<1$, $\alpha=1$ ou $\alpha>1$. 

%%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-.5,-.5)(7,5)
\psplot[linecolor=red,plotpoints=1000]{0}{7}{x sqrt}
\psplot[linecolor=blue,plotpoints=1000]{0}{7}{x sqrt sqrt}
\psplot[linecolor=magenta,plotpoints=1000]{0}{5}{x x mul x mul sqrt}
\psplot[linecolor=green,plotpoints=1000]{0}{5}{x x mul x mul}
\psline[linecolor=black]{-}(0,0)(5,5)
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-.5,-.4)(7,5)
\rput{0}(1,0){$+$}
\rput{0}(1.1,-.25){$1$}
\rput{0}(10,0){$+$}
\rput{0}(-.2,-.25){$0$}
\rput{12}(6,2.7){\red $y=x^{1/2}$}
\rput{45}(3.7,4){$y=x$}
\rput{65}(2.3,4.2){{\magenta$y=x^{3/2}$}}
\rput{80}(1.3,4.2){{\green$y=x^2$}}
\psline[linewidth=.5pt,linestyle=dashed]{-}(0,1)(1,1)
\psline[linewidth=.5pt,linestyle=dashed]{-}(1,0)(1,1)
\rput{5}(6,1.8){\blue $y=x^{1/4}$}
\rput{0}(0,1){$+$}
\rput{0}(-.2,1){$1$}
\rput{0}(6.8,-.2){$x$}
\rput{0}(-.2,4.8){$y$}
\endpspicture

\medskip
\centerline{%
	\tikzpicture[scale=0.8]
		\draw[very thin,color=gray] (-0.1,-0.1) grid (5.1,5.1);
		\draw[->,thick] (0,0) -- (5.2,0) node[below] {$x$};
		\draw[->,thick] (0,0) -- (0,5.1) node[left] {$y$};
		\draw[domain=0:5.1,samples=101,color=purple,smooth] plot (\x,{sqrt(sqrt(\x))}) node [above left,rotate=13] {${\root 4\of x}$};
		\draw[domain=0:5.1,samples=101,color=blue,smooth] plot (\x,{sqrt(\x)}) node [above left,rotate=13] {$\sqrt{x}$};
		\draw [color=black] (0,0) -- (5.1,5.1)  node [right] {$x$};
		\draw[domain=0:3,samples=101,color=gray,smooth] plot (\x,{\x*sqrt(\x)}) node [right,rotate=13] {$x^{3/2}$};
		\draw[domain=0:2.3,samples=66,color=red,smooth] plot (\x,{\x*\x}) node [right,rotate=8] {$x^2$};
	\endtikzpicture
}%
\Figure [Index=Courbes!Puissances] Graphes $y=\root 4\of x$, $y=\sqrt x$, $y=x$, $y=x^{3/2}$ et $y=x^2$. 
\medskip

\noindent
Les fonctions puissances satisfont également les relations \eqref{fondexp} et \eqref{fondexpa}. Ainsi, on a 
$$
\forall       a>0       ,\qquad       \forall(x,y)\in\ob        R^2       ,       \qquad       {a^{x+y}=a^x\times
a^y}\qquad\mbox{et}\qquad(a^x)^y=a^{xy}  .   $$  On   peut   également   factoriser   des   puissances.    $$
\forall     a>0 ,\qquad \forall     b>0 ,\qquad     \forall    x\in\ob    R,    \qquad
(ab)^x=a^xb^x.  $$ La croissance du logarithme est plus  lente  que celle des fonctions puissances qui est plus
lente que celle de la fonction exponentielle.  \medskip

\Propriete [Title=comparaison logarithme/exponentielle/puissances] 
Pour $\alpha>0$, on~a 
$$
\eqalign{
&{\ds\lim_{x\to+\infty}{\ln(x)\F x^\alpha}=0}, \qquad {\ds\lim_{x\to0^+}x^\alpha\ln(x)=0,}
\cr
&{\ds\lim_{x\to+\infty}{\e^x\F x^\alpha}=+\infty}\qquad\mbox{et}\qquad {\ds\lim_{x\to-\infty}x^\alpha\e^x=0.}
}
$$

\Section Fuhyp, Fonctions hyperboliques. 

\Subsection Fuch, Cosinus hyperbolique. 

\Definition []  Le cosinus hyperbolique est la fonction $\ch :\ob R\to\ob R$ définie par 
$$
\forall x\in\ob R, \qquad {\ch(x):={\e^{x}+\e^{-x}\F2}}.
$$
Le cosinus hyperbolique est strictement décroissant sur $\Q]-\infty,0\W]$ et strictement croissant sur $\Q[0,+\infty\W[$. De plus, on a 
$$
{\lim_{x\to-\infty}\ch(x)=+\infty}\qquad\mbox{et}\qquad{\lim_{x\to+\infty}\ch(x)=+\infty}.
$$
Enfin, le cosinus hyperbolique est pair et minorée par $1$. 
$$
\forall x\in\ob R, \qquad \ch(x)\ge1.
$$

\medskip
\centerline{%
	\tikzpicture
		\draw[very thin,color=gray] (-2.2,-0.1) grid (2.2,4.2);
		\draw[->] (0,0) -- (2.3,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,4.2) node[left] {$y$};
		\draw[domain=-2.1:2.1,samples=66,color=blue,smooth] plot (\x,{cosh(\x)});
	\endtikzpicture
}%
\Figure [Index=Courbes!Cosinus hyperbolique] Graphe $y=\ch(x)$. 
\medskip

\Subsection Fush, Sinus hyperbolique.
                                                             
\Definition []  Le sinus hyperbolique est la fonction $\sh :\ob R\to\ob R$ définie par 
$$
\forall x\in\ob R, \qquad {\sh(x):={\e^x-\e^{-x}\F2}}.
$$
Le sinus hyperbolique est impair et strictement croissant sur $\ob R$. De plus, il satisfait 
$$
{\lim_{x\to-\infty}\sh(x)=-\infty}\qquad\mbox{et}\qquad{\lim_{x\to+\infty}\sh(x)=+\infty}.
$$

%%\readdata{\shgraph}{Graphes/GraphSh.txt}
%%\psset{xunit=1cm, yunit=0.8cm}
\pspicture*[](-2,-4)(2,4)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\shgraph}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-2,-4)(2,4)
\rput{0}(-.2,4.8){$y$}
\rput{0}(2.9,-.25){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1.1,-.3){$1$}
\rput{0}(.2,-.3){$0$}
\rput{0}(0,1){$+$}
\rput{0}(-.15,0.75){$1$}
\endpspicture

\medskip
\centerline{%
	\tikzpicture[scale=0.5]
		\draw[very thin,color=gray] (-2.2,-4.1) grid (2.2,4.1);
		\draw[->] (0,0) -- (2.3,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,4.2) node[left] {$y$};
		\draw[domain=-2.2:2.2,samples=66,color=blue,smooth] plot (\x,{sinh(\x)}) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Sinus hyperbolique] Graphe $y=\sh(x)$. 
\medskip

Le cosinus et le sinus hyperbolique sont continus, indéfiniment dérivables sur $\ob R$ et 
$$
\forall x\in\ob R, \qquad \ch'(x)=\sh(x) \qquad \mbox{et}\qquad \sh'(x)=\ch(x) . 
$$

\Subsection Futh, Tangente hyperbolique.

\Definition []  La tangente hyperbolique est la fonction $\th :\ob C\to\ob C$ définie par 
$$
\forall x\in\ob R, \qquad {\th(x):={\sh(x)\F\ch(x)}={\e^x-\e^{-x}\F\e^{x}+\e^{-x}}}.
$$
La tangente hyperbolique est impaire et strictement croissante sur $\ob R$. De plus, on a 
$$
{\lim_{x\to-\infty}\th(x)=-1}\qquad\mbox{et}\qquad{\lim_{x\to+\infty}\th(x)=1}.
$$
La tangente hyperbolique est minorée par $-1$ et majorée par $1$
$$
\forall x\in\ob R, \qquad -1<\th(x)<1.
$$


\medskip
\centerline{%
	\tikzpicture
		\draw[very thin,color=gray] (-3.1,-1.2) grid (3.1,1.2);
		\draw[->] (0,0) -- (1.3,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,1.2) node[left] {$y$};
		\draw[domain=-3.1:3.1,samples=66,color=blue,smooth] plot (\x, {tanh(\x)}) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Tangente hyperbolique] Graphe $y=\th(x)$. 
\medskip

Enfin, la tangente hyperbolique est continue, indéfiniment dérivable et 
$$
\forall x\in\ob R, \qquad \th'(x)=1-\th^2(x)={1\F \ch^2(x)}.
$$
\Subsection FuFormulaire, Trigonométrie hyperbolique. 



\Concept [] Parité

La fonction cosinus hyperbolique est paire et les fonction sinus et tanbente hyperboliques sont impaires, autrement dit 
$$
\forall x\in\ob R, \qquad \ch(-x)=\ch(x) ,\quad \sh(-x)=-\sh(x) \quad\mbox{et}\quad \th(-x)=-\th(x).
$$


\Concept [] Lien avec l'exponentielle 

$$
\forall x\in\ob R, \qquad\ch(x)+\sh(x)=\e^x \qquad\mbox{et}\qquad \ch(x)-\sh(x)=\e^{-x }.
$$

\Concept [] Identité fondamentale

$$
\forall x\in\ob R, \qquad \ch^2(x)-\sh^2(x)=1 .
$$

\Concept [] Formules d'addition

\noindent
Soient $a$ et $b$ des nombres réels. Alors, on a 
$$
\eqalign{
&\ch(a+b)=\ch a\ch b+\sh a\sh b ,\qquad \ch(a-b)=\ch a\ch b-\sh a \sh b ,
\cr
&\sh(a+b)=\sh a\ch b+\ch a\sh b ,\qquad \sh(a-b)=\sh a\ch b-\cos a\sh b ,
\cr
&\ds\th(a+b)={\th a+\th b\F1+\th a\th b}\ \qquad\mbox{et}\qquad \th(a-b)={\th a-\th b\F1-\th a\th b}.
}
$$

\Concept [] Duplication de l'angle

\noindent
Pour chaque $x\in\ob R$, on a 
$\ds
\ch(2x)=\ch^2x+\sh^2x=2\ch^2(x)-1=1+2\sh^2(x)$,
\medskip\noindent\hfill $\sh(2x)=2\sh x\ch x$
\hfill\quad\mbox{et}\quad\hfill
$\ds\th(2x)={2\th x\F1+\th^2x}$.\hfill\null
\medskip
Réciproquement, on a également 
$$
\forall x\in\ob R, \qquad \ch^2(x)={1+\ch(2x)\F2}\qquad\mbox{et}\qquad\sh^2(x)={\ch(2x)-1\F2}.
$$

\Concept [] Linéarisation d'un produit 

\noindent
Pour chaque couple $(a,b)$ de nombres réels, on a 
$$
\eqalign{
\sh a \sh b={\ch(a+b)-\ch(a-b)\F2},&\qquad\sh a\ch b={\sh(a+b)+\sh(a-b)\F2_{\strut}}\cr
\mbox{et }\quad\ch a\ch b^{\strut}=&{\ch(a+b)+\ch(a-b)\F2}.
}
$$

\Concept [] Factorisation d'une somme 

\noindent
Soient $p$ et $q$ des nombres réels. Alors, on a 
\medskip
\noindent\hfill
$\ds\ch p+\ch q=2\ch\Q(\!{p+q\F2}\!\W)\ch\Q(\!{p-q\F2}\!\W)$, \hfill
$\ds\ch p-\ch q=2\sh\Q(\!{p+q\F2}\!\W)\sh\Q(\!{p-q\F2}\!\W)$,\hfill\null
\medskip
\noindent
\hfill$\ds\sh p+\sh q=2\sh\Q({p+q\F2}\W)\ch\Q({p-q\F2}\W)$,\hfill
$\ds\sh p-\sh q=2\ch\Q({p+q\F2}\W)\sh\Q({p-q\F2}\W)$.\hfill\null
\medskip

\Concept [] Tangente hyperbolique de l'angle moitié 

\noindent
Soit $x$ un nombre réel et soit $\ds t:=\th{x\F 2}$. Alors, on a 
$$
\ch x={1+t^2\F1-t^2}, \qquad\sh x={2t\F1-t^2}\qquad\mbox{et}\qquad\th x={2t\F1+t^2}.
$$

\Section Fucircrec, Fonctions circulaires réciproques. 

\Propriete []  Soit $f:I\to J$ une bijection de $I$ dans $J$, dérivable sur l'intervalle $I$. 
Alors, pour chaque $a\in I$ vérifiant $f'(a)\neq0$, la bijection réciproque $g:J\to I$ de l'application $f$ est dérivable en $f(a)$ et 
$$
g'\b(f(a)\b)={1\F f'(a)}.
$$

\Subsection Fuarccos, Arc cosinus.

\Definition []  L'arc cosinus $\arccos:[-1,1]\to[0,\pi]$ est la bijection réciproque de l'application 
$$
\eqalign{
	\cos:[0,\pi]&\to[-1,1]\cr 
	x&\mapsto\cos(x)
}
$$ 
\medskip

%%\readdata{\arccosgraph}{Graphes/GraphArcCos.txt}
%%\psset{xunit=2cm, yunit=1cm}
\pspicture*[](-1.4,-.4)(1.4,3.5)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\arccosgraph}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-1.4,-.4)(1.4,3.5)
\rput{0}(-.1, 3.4){$y$}
\rput{0}(1.3,-.25){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1,-.3){$1$}
\rput{0}(-.1,-.2){$0$}
\rput{0}(0,1.57){$+$}
\rput{0}(.15,1.7){${\pi\F2}$}
\psline[linewidth=.5pt,linestyle=dotted]{-}(-1,0)(-1,3.141592654)
\rput{0}(-1,0){$+$}
\rput{0}(-1.06,-.3){$-1$}
\psline[linewidth=.5pt,linestyle=dotted]{-}(0,3.141592654)(-1,3.141592654)
\rput{0}(0,3.141592654){$+$}
\rput{0}(.2,3.141592654){$\pi$}
\endpspicture

\centerline{%
	\tikzpicture
		\draw[very thin,color=gray,step={(1,1.570796327)}] (-1.1,-0.1) grid (1.1,3.2);
		\draw[->] (0,0) -- (1.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,3.3) node[left] {$y$};
		\draw[color=red,smooth,domain=0:1,samples=66] plot (\x,{rad(acos(\x))}); 
		\node [rotate=-45,color=red] at (0.1,1.77) {$\arccos x$};
		\draw[color=red,smooth,domain=-1:0,samples=66] plot (\x,{rad(acos(\x)+180)});
	\endtikzpicture
}%
\Figure [Index=Courbes!Arc cosinus]  Graphe $y=\arccos(x)$. 
\medskip

\Propriete []  L'arc cosinus est strictement croissant et continue sur $[-1,1]$. De plus, il est dérivable sur $\Q]-1,1\W[$ et 
$$
\forall x\in\Q]-1,1\W[ ,\qquad  \arccos'(x)=-{1\F\sqrt{1-x^2 }}. 
$$
En particulier, on a 
$$
\forall x\in\Q]-1,1\W[,\qquad\arccos(x)={\pi\F2}-\int_0^x{\d u\F\sqrt{1-u^2}}. 
$$


\Subsection Fuarcsin, Arc sinus.


\Definition []  L'arc sinus $\arcsin:[-1,1]\to\Q[-{\pi\F2},{\pi\F2}\W]$ est la bijection réciproque de l'application 
$$
\eqalign{
	\sin:\Q[-{\pi\F2},{\pi\F2}\W]&\to[-1,1]\cr
	x&\mapsto\sin(x)
}  
$$

%%\readdata{\arccosgraph}{Graphes/GraphArcSin.txt}
%%\psset{xunit=2cm, yunit=1cm}
\pspicture*[](-1.4,-1.8)(1.4,1.8)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\arccosgraph}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-1.4,-1.8)(1.4,1.8)
\rput{0}(-.1, 3.4){$y$}
\rput{0}(1.3,-.25){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1,-.3){$1$}
\rput{0}(.1,-.2){$0$}
\rput{0}(0,1.57){$+$}
\rput{0}(-.15,1.57){${\pi\F2}$}
\rput{0}(0,-1.57){$+$}
\rput{0}(.25,-1.575){$-{\pi\F2}$}
\psline[linewidth=.5pt,linestyle=dotted]{-}(-1,0)(-1,-1.575)
\psline[linewidth=.5pt,linestyle=dotted]{-}(-1,-1.575)(0,-1.575)
\rput{0}(-1,0){$+$}
\rput{0}(-1.06,.3){$-1$}
\psline[linewidth=.5pt,linestyle=dotted]{-}(1,0)(1,1.575)
\psline[linewidth=.5pt,linestyle=dotted]{-}(1,1.575)(0,1.575)
\rput{0}(0,3.141592654){$+$}
\rput{0}(.2,3.141592654){$\pi$}
\endpspicture

\centerline{%
	\tikzpicture
		\draw[very thin,color=gray,step={(1,1.570796327)}] (-1.1,-1.67) grid (1.1,1.67);
		\draw[->] (0,0) -- (1.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,1.77) node[left] {$y$};
		\draw[color=red,smooth,domain=-1:1,samples=66] plot (\x,{rad(asin(\x))}); 
		\node [rotate=45,color=red] at (0.1,0.4) {$\arcsin x$};
	\endtikzpicture
}%
\Figure [Index=Courbes!Arc sinus] Graphe $y=\arcsin(x)$. 
\medskip


\Propriete []  L'arc sinus est strictement croissant et continue sur $[-1,1]$. De plus, il est dérivable sur $\Q]-1,1\W[$ et 
$$
\forall x\in\Q]-1,1\W[ ,\qquad  \arcsin'(x)={1\F\sqrt{1-x^2 }}. 
$$
En particulier, on a 
$$
\forall x\in\Q]-1,1\W[,\qquad\arcsin(x)=\int_0^x{\d u\F\sqrt{1-u^2}}. 
$$


\Subsection Fuarctan, Arc tangente.


\Definition []  L'arc tangente $\arctan:\ob R\to\Q]-{\pi\F2},{\pi\F2}\W[$ est la bijection réciproque de l'application 
$$
\eqalign{
	\tan:\Q]-{\pi\F2},{\pi\F2}\W[&\to\ob R\cr
	x&\mapsto\tan(x)
}
$$
\medskip 

\centerline{%
	\tikzpicture
		\draw[very thin,color=gray,step={(1,1.570796327)}] (-5.1,-1.67) grid (5.1,1.67);
		\draw[->] (0,0) -- (5.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,1.77) node[left] {$y$};
		\draw[color=red,smooth,domain=-5.1:5.1,samples=66] plot (\x,{rad(atan(\x))}) ; 
		\node [rotate=45,color=red] at (0.1,0.4) {$\arctan x$};
	\endtikzpicture
}%
\Figure [Index=Courbes!Arc tangente] Graphe $y=\arctan(x)$. 
\medskip


\Propriete []  L'arc tangente est strictement croissante, continue, dérivable sur $\ob R$ et vérifie  
$$
\forall x\in\ob R,\qquad \arctan'(x)={1\F x^2+1}. 
$$
En particulier, on a 
$$
\forall x\in\ob R,\qquad\arctan(x)=\int_0^x{\d u\F u^2+1}. 
$$


\Exercice. Pour $x>0$, prouver que $0\le \ds{x\F x^2+1}\le \arctan(x)\le x$. 

 
\Exercice. Prouver que 
$$
\forall x>0 ,\qquad  \arctan(x)+\arctan\Q({1\F x} \W)={\pi\F2}.
$$ 
Que se passe t'il pour $x<0$ ?

\Section Fuhyprec, Fonctions hyperboliques réciproques. 

\Subsection Fuargch, Argch. 

\Definition []  L'argument cosinus hyperbolique $\argch:\Q[1,+\infty\W[\to\Q[0,+\infty\W[$ est la bijection réciproque de l'application 
$$
\eqalign{ 
	\ch:\Q[0,+\infty\W[&\to\Q[1,+\infty\W[\cr 
	x&\mapsto\ch(x)
}
$$ 


\medskip
\centerline{%
	\tikzpicture
		\draw[very thin,color=gray] (-0.1,-0.1) grid (11.2,3.2);
		\draw[->] (0,0) -- (11.2,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,3.3) node[left] {$y$};
		\draw[domain=0:3.1,samples=66,color=blue,smooth] plot ({cosh(\x)},\x);
	\endtikzpicture
}%
\Figure [Index=Courbes!Argument cosinus hyperbolique] Graphe $y=\argch(x)$. 
\medskip

\Propriete []  L'application $\argch$ est strictement croissant et continue sur $\Q[1,+\infty\W[$. De plus, elle est dérivable 
sur $\Q]1,+\infty\W[$ et 
$$
\forall x>1 ,\qquad  \argch'(x)={1\F\sqrt{x^2-1 }}. 
$$
En particulier, on a 
$$
\forall x\ge1,\qquad\argch(x)=\ln\Q(x+\sqrt{x^2-1}\W). 
$$

\Subsection Fuargsh, Argsh. 

\Definition []  L'argument sinus hyperbolique $\argsh:\ob R\to\ob R$ est la bijection réciproque de l'application 
$$
\eqalign{
	\sh:\ob R&\to\ob R\cr
	x&\mapsto\sh(x)
}
$$ 

%%\readdata{\argshgraph}{Graphes/GraphArgSh.txt}
%%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-4,-2.2)(4,2.2)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\argshgraph}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-4,-2.2)(4,2.2)
\rput{0}(-.3, 2.8){$y$}
\rput{0}(4.8,-.25){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1,-.3){$1$}
\rput{0}(.2,-.2){$0$}
\rput{0}(0,1){$+$}
\rput{0}(-.3,1){$1$}
\endpspicture

\medskip
\centerline{%
	\tikzpicture[scale=0.5]
		\draw[very thin,color=gray] (-7.1,-3.2) grid (7.1,3.2);
		\draw[->,thick] (0,0) -- (7.2,0) node[below] {$x$};
		\draw[->,thick] (0,0) -- (0,3.3) node[left] {$y$};
		\draw[domain=-2.7:2.7,samples=66,color=blue,smooth] plot ({sinh(\x)},\x) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Argument sinus hyperbolique]  Graphe  $y=\argsh(x)$. 
\medskip

\Propriete []  L'application $\argsh$ est strictement croissant et continue sur $\ob R$. De plus, elle~est dérivable sur $\ob R$ et 
$$
\forall x\in\ob R,\qquad \argsh'(x)={1\F\sqrt{x^2+1}}. 
$$
En particulier, on a 
$$
\forall x\in\ob R,\qquad\argsh(x)=\ln\Q(x+\sqrt{x^2+1}\W). 
$$

\Subsection Fuargth, Argth. 


\Definition []  L'argument tangente hyperbolique $\argth:\Q]-1,1\W[\to\ob R$ est la bijection réciproque de l'application 
$$
\eqalign{
	\th:\ob R&\to\Q]-1,1\W[\cr
	x&\mapsto\th(x)
}
$$ 



\medskip
\centerline{%
	\tikzpicture
		\draw[very thin,color=gray] (-1.2,-3.1) grid (1.2,3.1);
		\draw[->] (0,0) -- (1.3,0) node[below] {$x$};
		\draw[->] (0,0) -- (0,3.2) node[left] {$y$};
		\draw[domain=-3.1:3.1,samples=66,color=blue,smooth] plot ({tanh(\x)},\x) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Argument tangente hyperbolique] Graphe  $y=\argth(x)$. 
\medskip

\Propriete []  L'application $\argth$ est strictement croissante et continue sur $\Q]-1,1\W[$. 
De plus, elle~est dérivable sur $\Q]-1,1\W[$ et 
$$
\forall x\in\Q]-1,1\W[ ,\qquad  \argth'(x)={1\F1-x^2 }. 
$$
En particulier, on a 
$$
\forall x\in\Q]-1,1\W[,\qquad\argth(x)={1\F2}\ln\Q({1+x\F1-x}\W). 
$$























\hautspages{Olus Livius Bindus}{Courbes planes paramétrées et coniques}

\Chapter CPEC, Courbes planes paramétrées et coniques.
\bigskip

\noindent
Dans ce chapitre, le symbole $\sc P$ désigne un plan affine muni d'un produit scalaire 
et $(O,\vec i,\vec j)$ désigne un repère orthonormé direct de $\sc P$. 
\bigskip
\noindent
On rappelle que, pour chaque couple de points $(M,N)\in\sc P^2$, le~vecteur $\vec{MN}$ est uniquement défini par 
$$
N=M+\vec{MN}.
$$
et que la donnée d'un repère orthonormé $(O,\vec i,\vec j)$ permet d'identifier le plan affine $\sc P$ soit à $\ob R^2$ ou à $\ob C$ via les isométries affines (bijections conservant le produit scalaire)
$$
\eqalign{\ob C&\to\sc P\cr x+iy\mapsto  M\mbox{ tel que }\vec{OM}=x\vec i+y\vec j}
$$
\bigskip
\Section CPP, Courbes planes paramétrées.

\Subsection CPPDeath, Courbes paramétrées cartésiennes.

\Concept [] Courbes paramétrées de classe $\sc C^k$

\noindent
Une courbe paramétrée de $\sc P$ est le couple formé par un intervalle $I$ et par une application 
$$
\eqalign{ I&\to\sc P\cr t&\mapsto  M(t)}\eqdef{courbpar}
$$ 
Elle est de classe $\sc C^k$ si, et seulement si, ses fonctions coordonnées $t\mapsto x(t)$ et $t\mapsto y(t)$ définies par 
$$
\forall t\in I, \qquad \vec {OM(t)}=x(t)\vec i+y(t)\vec j. 
$$
sont $k$ fois dérivables, de dérivées continues, sur l'intervalle $I$. 
\bigskip

\Concept [] Support

\noindent
Le support de la courbe paramétrée \eqref{courbpar} est l'ensemble $\{M(t):t\in I\}$ des points $M(t)$ du plan $\sc P$ définis par  
$$
\forall t\in I, \qquad \vec {OM(t)}=x(t)\vec i+y(t)\vec j. 
$$

$$
%\readdata{\astroide}{Graphes/GraphAstroide.txt}
%\psset{xunit=2cm, yunit=2cm}
\pspicture*[](-1.2,-1.2)(1.2,1.2)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\astroide}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-1.2,-1.2)(1.2,1.2)
\rput{0}(-.15,1.1){$y$}
\rput{0}(1.1,-.15){$x$}
\rput{0}(1,0){$+$}
\rput{0}(1,.15){$1$}
\rput{0}(.1,-.1){$0$}
\rput{0}(0,1){$+$}
\rput{0}(.15,1){$1$}
\rput{0}(0,-1){$+$}
\rput{0}(.25,-1){$-1$}
\rput{0}(-1,0){$+$}
\rput{0}(-1,.15){$-1$}
\endpspicture
$$

\centerline{%
\tikzpicture[domain=0:360,smooth,variable=\x] 
	\draw[very thin,color=black!20,step=0.5] (-1.1,-1.1) grid (1.1,1.1);
	\draw[->] (-1.1,0) -- (1.1,0) node[above] {\eightpts$x$};
	\draw[->] (0,-1.1) -- (0,1.1) node[right] {\eightpts$y$};
	\draw[color=blue] plot ({cos(\x)^3},{sin(\x)^3}) ;
\endtikzpicture}%
\Figure [Index=Courbes!Support de l'astroide@Support de l'astroïde,Label=Astroide] Support de l' astroïde  $t\mapsto(\cos^3t,\sin^3t)$ pour $I=\ob R$. 
\bigskip

\Remarque :  La figure \eqrefn{Astroide} montre que la régularité du support d'une courbe paramétrée 
n'est pas bien caractérisée par la régularité de son paramétrage. 
\bigskip

\Remarque :  En pratique, on travaille dans $\ob R^2$ plutôt que dans un plan affine $\sc P$. Dans $\ob R^2$, 
une courbe paramétrée est le couple $(I,f)$ formé par un intervalle $I$ et une application 
$$
\eqalign{f: I&\to\ob R^2\cr  t&\mapsto  f(t)=\b(x(t),y(t)\b)}
$$
Son support  est alors l'ensemble image $f(I)= \{f(t):t\in I\}$. 

\Concept [] Point régulier

\noindent
On dit qu'une courbe paramétrée $(I,f)$ de classe $\sc C^1$ est régulière en un point $t_0\in I$ (ou~que~$t_0$ est un
point régulier de la courbe paramétrée) si, et~seulement~si, $$
f'(t_0):=\b(x'(t_0),y'(t_0)\b)\neq(0,0)\qquad\mbox{i.e.}\qquad {\d \vec{OM}\F \d t}(t_0):=x'(t_0)\vec
i+y'(t_0)\vec j\neq\vec 0.  $$ 

\Concept [] Point singulier

Dans le cas contraire , on dit que la courbe paramétrée $(I,f)$ est singulière en~$t_0\in I$ (ou~que~$t_0$ est un point singulier de la courbe paramétrée).  \bigskip

\Concept [] Tangente

\noindent
La tangente d'une courbe paramétrée $(I,f)$ de classe $\sc C^1$ en un point régulier $t_0\in I$ 
est l'unique droite passant par le point $f(t_0)$ et de vecteur directeur $f'(t_0)$. Autrement dit, 
c'est la droite passant par $M(t_0)$ et de vecteur directeur ${\d \vec{\mbox{\sevenrm OM}}\F \d t}(t_0):=x'(t_0)\vec i+y'(t_0)\vec j$. 
\bigskip

\Remarque. Soit $(I,f)$ une courbe paramétrée de classe $\sc C^N$ et $t_0\in I$. On suppose qu'il existe un plus petit entier $n\in\{1,\cdots,N\}$ tel que 
$$
f^{(n)}(t_0):=\B(x^{(n)}(t_0),y^{(n)}(t_0)\B)\neq(0,0).
$$ 
Alors, la tangente à la courbe paramétrée $(I,f)$ au point $t_0\in I$ 
est l'unique {droite passant par le point $f(t_0)$ et de vecteur directeur $f^{(n)}(t_0)$}. 
\bigskip

\Concept [] Branches infinies 

\noindent
Soit $(I,f)$ une courbe paramétrée et soit $t_0$ appartienant à $I$ ou égal à l'une des extrémités de l'intervalle $I$.
Alors, on dit que la courbe paramétrée $(I,f)$ admet une branche infinie en $t_0$ si, et seulement si
$$
\lim_{t\to t_0}x(t)=\pm\infty\qquad \mbox{ou }\lim_{t\to t_0}y(t)=\pm\infty.
$$
Une branche infinie peut être de plusieurs types : 
\medskip

\Bullet Si $\ds\lim_{t\to t_0}x(t)=\pm\infty$ et $\ds\lim_{t\to t_0}y(t)=a\in\ob R$, la courbe admet l'asymptote horizontale~$y=a$. 
\medskip

$$
%\readdata{\thgraph}{Graphes/GraphTh.txt}
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-3,-1.3)(3,1.3)
\dataplot[plotstyle=curve,linewidth=.8pt,linecolor=red]{\thgraph}
\psaxes*[labels=none,ticks=none]{<->}(-0.5,.2)(-3,-1.3)(3,1.3)
\rput{0}(-.7,1.2){$y$}
\rput{0}(2.9,0){$x$}
\rput{0}(-.65,-0){$0$}
\psline[linewidth=.5pt,linecolor=green]{-}(-3,1)(3,1)
\psline[linecolor=blue,linewidth=.5pt]{-}(-3,-1)(3,-1)
\rput{0}(-0.5,1){$+$}
\rput{0}(-.25,1.2){\green $a$}
\rput{0}(-0.5,-1){$+$}
\rput{0}(-.25,-0.75){\blue $a'$}
\endpspicture
$$

\centerline{%
	\tikzpicture
		\draw[very thin,color=gray] (-3.1,-1.1) grid (4.1,2.1);
		\draw[->,thick] (-3.1,0) -- (4.3,0) node[below] {$x$};
		\draw[->,thick] (0,-1.2) -- (0,2.2) node[left] {$y$};
		\draw [color=blue,thick] (1.1,2) --  (4.1,2) node [below,pos=0.5] {$y=a$} node[above,pos=0.5] {$x\to+\infty$} ;
		\draw [color=blue,thick] (-3.1,-1) -- (-0.5,-1) node[below,pos=0.5]  {$y=a'$} node [above,pos=0.5] {$-\infty\gets x$} ;
		\draw[domain=-0:4.1,samples=66,color=red,smooth,variable=\x] plot (\x,{2*tanh(\x)}) ;
		\draw[domain=-3.1:0,samples=66,color=red,smooth,variable=\x] plot (\x,{tanh(2*\x)}) ;
	\endtikzpicture
}%
\Figure [Index=Courbes!Asymptotes horizontales] Asymptotes horizontales. 
\bigskip

\Bullet Si $\ds\lim_{t\to t_0}x(t)=a\in\ob R$ et $\ds\lim_{t\to t_0}y(t)=\pm\infty$, la courbe admet l'asymptote verticale~$x=a$. 
\medskip
$$
%% Mathematica : Table[{x,Tan[x]},{x,-Pi/2+0.32,Pi/2-0.32,0.02}]
%\readdata{\tangraph}{Graphes/GraphTan.txt}
%\psset{xunit=1cm,yunit=1cm}
\pspicture*[](-.5,-1.8)(4.14,3)
\psaxes*[labels=none,ticks=none]{->}(0,0)(-.5,-1.8)(4.14,3)
\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}
\rput{0}(3.8415,1.2){\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=red]{\tangraph}}
\psline[linewidth=.3pt,linecolor=green]{-}(1.2707,-3)(1.2707,3)
\psline[linewidth=.3pt,linecolor=blue]{-}(2.5707,-3)(2.5707,3)
\rput{0}(1,-.3){\green $a$}
\rput{0}(2.3,.3){\blue $a'$}
\rput{0}(-0.3,.3){$0$}
\rput{0}(-.3,2.8){$y$}
\rput{0}(4,-.3){$x$}
\endpspicture
$$

\centerline{%
	\tikzpicture[scale=0.6]
	\draw[very thin,color=gray] (-0.5,-2.8) grid (4.1,4.1);
		\draw[->,thick] (0,0) -- (4.3,0) node[below] {$x$};
		\draw[->,thick] (0,0) -- (0,4.2) node[left] {$y$};
		\draw [color=red,thick] (1.570796327,-2.8) --  (1.570796327,4.3) node [right,pos=0.45] {$x=a$} node [below,rotate=90,pos=0.9] {$y\to+\infty$} node [above,rotate=90,pos=0.1] {$-\infty\gets y$};
		\draw[domain=-0.5:1.35,samples=66,color=blue,smooth] plot (\x,{tan(\x r)});
		\draw[domain=1.9:3.1,samples=66,color=blue,smooth] plot (\x,{tan(\x r)});
	\endtikzpicture
}%
\Figure [Index=Courbes!Asymptotes verticales] Asymptotes verticales. 
\medskip

\Bullet Si $\ds\lim_{t\to t_0}x(t)=\pm\infty$ et $\ds\lim_{t\to t_0}y(t)=\pm\infty$, il y a plusieurs cas selon la limite $\ds \lim_{t\to t_0}{y(t)\F x(t)}$ : 
\medskip

\noindent \quad $\star$ Si $\ds \lim_{t\to t_0}{y(t)\F x(t)}=0$, la courbe admet une branche parabolique de direction $(Ox)$. 
\medskip

%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-4,-1.5)(7,3.5)
\scalebox{-1 -1}{\psplot[linecolor=red,plotpoints=1000]{0}{4}{x sqrt sqrt}}
\psplot[linecolor=red,plotpoints=1000]{0}{7}{x x mul x mul x mul x mul sqrt sqrt sqrt}
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-4,-1.5)(7,3.5)
\rput{0}(6.8,-.3){$x$}
\rput{0}(-.3,3.3){$y$}
\rput{0}(-.3,-.3){$0$}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(-0.3,1){$1$}
\rput{0}(1,-0.4){$1$}
\endpspicture

\centerline{%
	\tikzpicture[scale=0.8]
	\draw[very thin,color=gray] (-5.2,-1.1) grid (7.1,3.4);
		\draw[->,thick] (-5.2,0) -- (7.3,0) node[below] {$x$};
		\draw[->,thick] (0,-1.1) -- (0,3.5) node[left] {$y$};
		\draw[domain=0:6.2,samples=66,color=blue,smooth] plot (\x+1,{sqrt(1+2*\x)});
		\draw[domain=0:6.2,samples=66,color=blue,smooth] plot (1-\x,{3/2-sqrt(1+4*\x)/2});
	\endtikzpicture
}%
\Figure [Index=Courbes!Branches paraboliques horizontales] Branches paraboliques de direction $(Ox)$. 
\bigskip

\noindent\quad$\star$ Si $\ds \lim_{t\to t_0}{y(t)\F x(t)}=a\neq0$ et $\ds\lim_{t\to t_0}\b(y(t)-ax(t)\b)=b\in\ob R$, la courbe admet une asymptote d'équation $y=ax+b$. 
\medskip

%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-.3,-3)(8,1.66)
\psplot[linecolor=red,plotpoints=1000]{.2}{8}{x 3 div 1 sub x 300 mul x sin 200 mul add cos x div add}
\psplot[linecolor=blue,plotpoints=1000]{-.3}{8}{x 3 div 1 sub}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-.3,-3)(8,1.66)
\rput{0}(-.2,3.3){$y$}
\rput{0}(-.2,-.2){$0$}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(-0.2,1){$1$}
\rput{0}(0.9,0.2){$1$}
\endpspicture

\centerline{%
	\tikzpicture
	\draw[very thin,color=gray] (-2.1,-1.4) grid (3.1,2.8);
		\draw[->,thick] (-2.2,0) -- (3.3,0) node[below] {$x$};
		\draw[->,thick] (0,-1.5) -- (0,3) node[left] {$y$};
		\draw[-,color=red] (-2,1) -- (2.5,-1.25) node[pos=0.8,below,rotate=-25]{$y=ax+b$};
		\draw[domain=0.3:4.5,samples=66,color=blue,smooth] plot (\x-2,{1-\x/2+3/(4*\x+1)});
	\endtikzpicture
}%
\Figure [Index=Courbes!Asymptotes obliques] Asymptote d'équation  $y=ax+b$. 
\bigskip


\noindent\quad$\star$ Si $\ds \lim_{t\to t_0}{y(t)\F x(t)}=a\neq0$ et $\ds\lim_{t\to t_0}\b(y(t)-ax(t)\b)=\pm\infty$, la courbe admet une branche  parabolique de direction $y=ax$. 
\medskip
$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-.5,-1.2)(6,4.5)
\psplot[linecolor=red,plotpoints=1000]{.2}{6}{x 3 div x sqrt 1 mul add}
\psplot[linecolor=blue,plotpoints=1000]{-.5}{6}{x 3 div}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-.5,-.5)(6,4.5)
\rput{0}(5.8,-.2){$x$}
\rput{0}(-.2,4.3){$y$}
\rput{0}(-.2,-.3){$0$}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(-0.2,1){$1$}
\rput{0}(1,-0.3){$1$}
\endpspicture
$$
\Figure Branchespara2, Branche parabolique de direction  $y=ax$ . 
\bigskip\goodbreak
\noindent\quad$\star$ Si $\ds \lim_{t\to t_0}{y(t)\F x(t)}=\pm\infty$, la courbe admet une branche parabolique de direction $(Oy)$. 
\medskip
$$
%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-1,-1.5)(3.5,3.5)
\psplot[linecolor=red,plotpoints=1000]{-4}{0}{x x mul 5 mul }
\psplot[linecolor=red,plotpoints=1000]{0}{7}{x x mul x mul .1 mul }
\psaxes*[labels=none,ticks=none]{->}(0.5,0.5)(-1,-1.5)(3.5,3.5)
\rput{0}(3.4,.2){$x$}
\rput{0}(.2,3.3){$y$}
\rput{0}(.2,.2){$0$}
\rput{0}(0.5,1.5){$+$}
\rput{0}(1.5,0.5){$+$}
\rput{0}(0.2,1.5){$1$}
\rput{0}(1.5,0.9){$1$}
\endpspicture
$$
\Figure Branchespara3, Branches paraboliques de direction $(Oy)$. 
\bigskip


\Remarque : lorsque l'on trouve une direction asymptotique, on essaye en général de déterminer la position de la courbe par rapport à l'asymptote. Cela peut ête utile. 
\bigskip

 Contrairement aux apparences, les branches paraboliques sont rarement des paraboles. 
\bigskip


\Concept Algorithme d'étude d'une courbe paramétrée

\medskip
 1) Recherche de l'ensemble de définition, de dérivabilité. 
\medskip
\noindent 2) Etude de la périodicité et de la parité (symétries de la courbe). 
\medskip
\noindent 3) Etudier le signe des dérivées pour dresser un tableau de variation. 
\medskip
\noindent 4) Compléter le tableau de variation en faisant apparaitre points singuliers, 

\noindent\quad tangentes, asymptotes, branches paraboliques, points doubles...
\medskip
\qquad 5) Tracer la courbe.

\bigskip
\Concept [] Interprétation cinématique

\noindent
Géomètriquement, on peut assimiler le support d'une courbe paramétrée
$$
\vec{OM(t)}=x(t)\vec i+y\vec j\qquad(t\in I)
$$
à la trajectoire d'un point mobile, dont la position instantanée est donnée au temps $t\in I$ par la donnée du point $M(t)$. 

\bigskip
\noindent
On remarque que le ``vecteur vitesse moyen'' du point mobile entre les temps $t_0$ et $t$ est donnée par le taux d'accroissement 
$$
{\vec{M(t)M(t_0)}\F t-t_0}={x(t)-x(t_0)\F t-t_0}\vec i+{y(t)-y(t_0)\F t-t_0}\vec j, 
$$
qui converge vers le vecteur vitesse instantané en $t_0$ 
$$
\vec v(t_0):=\lim_{t\to t_0}{\vec{M(t)M(t_0)}\F t-t_0}=x'(t_0)\vec i+y'(t_0)\vec j 
$$
lorsque l'on fait tendre $t$ vers $t_0$. \medskip
\noindent
On remarque que le vecteur instantané $\vec v(t_0)$ est tangent à la trajectoire du point mobile au point $M(t_0)$. 
De plus, la vitesse instantannée en $t_0$ est la quantité 
$$
v(t_0)=\|\vec v(t_0)\|=\sqrt{x'(t_0)^2+y'(t_0)^2}.
$$
De même, le ``vecteur acceleration moyen'' du point mobile entre les temps $t_0$ et $t$ est donnée par le taux d'accroissement 
$$
{\vec{v(t)}-\vec {v(t_0)}\F t-t_0}={x'(t)-x'(t_0)\F t-t_0}\vec i+{y'(t)-y'(t_0)\F t-t_0}\vec j, 
$$
qui converge vers le vecteur acceleration instantané en $t_0$ 
$$
\vec a(t_0):=\lim_{t\to t_0}{\vec{v(t)}-\vec {v(t_0)}\F t-t_0}=x''(t_0)\vec i+y''(t_0)\vec j 
$$
lorsque l'on fait tendre $t$ vers $t_0$. En particulier, l'acceleration instantanée en $t_0$ est 
$$
a(t_0)=\|\vec a(t_0)\|=\sqrt{x''(t_0)^2+y''(t_0)^2}.
$$
\bigskip

\Subsection CPPP, Courbes paramétrées polaires.

\Concept [] Repère polaire

\noindent
Pour chaque nombre réel $\theta$, le repère polaire est formé des deux vecteurs $\vec u(\theta)$ et $\vec v(\theta)$ définis par 
$$
\vec u(\theta):=\cos(\theta)\vec i+\sin(\theta)\vec j\qquad\mbox{et}\qquad\vec v(\theta):=\vec u\Q(\theta+{\pi\F2}\W)=-\sin(\theta)\vec i+\cos(\theta)\vec j. 
$$

 Les vecteurs $\vec u(\theta)$ et $\vec v(\theta)$ du repère polaire, sont parfois noté plus simplement $\vec u$ et $\vec v$, bien que ce ne sont pas 
des vecteurs fixes du plan $\sc P$ (ils dépendent de l'angle $\theta$). 
\bigskip

\Remarque. Les applications $\theta\mapsto\vec u(\theta)$ et $\theta\mapsto\vec v(\theta)$ sont indéfiniment dérivables sur $\ob R$ et on a 
$$
\forall \theta\in\ob R, \qquad \vec u'(\theta):=\vec v(\theta)\qquad\mbox{et}\qquad\vec v'(\theta)=-\vec u(\theta).
$$
\bigskip
 
\Concept [] Courbes paramétrées polaires de classe $\sc C^k$

\noindent
La courbe paramétrée polaire associée à un intervalle $I$ et à deux fonctions $\rho:I\to\ob R$ et~$\theta:I\to\ob R$ est la courbe paramétrée $(I,f)$ définie par 
$$
\forall t\in I, \qquad f(t)=\B(\rho(t)\cos\b(\theta(t)\b),\rho(t)\sin\b(\theta(t)\b)\B). 
$$
Autrement dit, c'est la courbe paramétrée cartésienne donnée par 
$$
\forall t\in I, \qquad \Q\{\eqalign{x(t)&=\rho(t)\cos\b(\theta(t)\b)\cr
y(t)&=\rho(t)\sin\b(\theta(t)\b)}
\W.
$$
Une courbe polaire est dite de classe $\sc C^k$ lorsque les applications $\rho$ et $\theta$ sont de classe $\sc C^k$ sur~l'intervalle~$I$. 
\bigskip

\Remarque{ \it 1}. Une courbe paramétrée polaire est une courbe paramétrée du type 
$$
\vec{OM(t)}=\rho(t)\vec u\b(\theta(t)\b).
$$

\Remarque{ \it 2}. Dans le repère polaire, on peut exprimer la dérivée (i.e. la vitesse instantanée) et la dérivée seconde (i.e. l'acceleration instantanée) d'une courbe paramétrée polaire. 
Ainsi, pour une courbe polaire de classe $\sc C^1$, on a 
$$
\forall t\in I, \qquad {\d\vec{OM}\F \d t}=\vec{vitesse}=\rho'(t)\vec u\b(\theta(t)\b)+\rho(t)\theta'(t)\vec v\b(\theta(t)\b)
$$
et pour une courbe polaire de classe $\sc C^2$, on a 
$$
\eqalign{\forall t\in I, \qquad {\d^2\vec{OM}\F \d t^2}&=\vec{acceleration}\cr&=\B(\rho''(t)-\rho(t)\theta'(t)^2\B)\vec u\b(\theta(t)\b)+\B(2\rho'(t)\theta'(t)+\rho(t)\theta''(t)\B)\vec v\b(\theta(t)\b)}
$$

\Concept [] Courbes définies par une équation polaire

La courbe polaire associée à une application $\theta\mapsto \rho(\theta)$ à valeurs réelles et de classe $\sc C^k$ sur un intervalle $I$ est la courbe paramétrée $(I,f)$ définie par 
$$
\forall \theta\in I, \qquad f(\theta)=\B(\rho(\theta)\cos\b(\theta\b),\rho(\theta)\sin\b(\theta\b)\B). 
$$
Autrement dit, c'est la courbe paramétrée cartésienne donnée par 
$$
\forall \theta\in I, \qquad \Q\{\eqalign{x(t)&=\rho(\theta)\cos(\theta)\cr
y(t)&=\rho(\theta)\sin(\theta)}
\W.
$$
et plus simplement encore par 
$$
\vec{OM(\theta)}=\rho(\theta)\vec u\b(\theta\b).
$$


\Remarque. Etant une courbe polaire associée à une fonction $\rho:I\to\ob R$ de classe $\sc C^1$, on~a   
$$
\forall\theta\in I, \qquad{\d\vec{OM}\F \d\theta}=\rho'(\theta)\vec u(\theta)+\rho(\theta)\vec v(\theta).
$$
En particulier, le point $\theta\in I$ est régulier si, et seulement si, $\rho(\theta)\neq 0$ ou $\rho'(\theta)\neq0$. Dans~ce~cas, la tangente au point $M(\theta)$ 
à la courbe est la droite passant par $M(\theta)$ de~vecteur directeur $\rho'(\theta)\vec u(\theta)+\rho(\theta)\vec v(\theta)$, facile à déterminer dans le repère polaire. 
\medskip
\noindent Un vecteur normal (orthogonal) à la tangente est 
$$
\rho'(\theta)\vec u\Q(\theta+{\pi\F 2}\W)+\rho(\theta)\vec v\Q(\theta+{\pi\F 2}\W):=-\rho(\theta)\vec v(\theta)+\rho'(\theta)\vec v(\theta)
$$


\Concept Algorithme d'étude d'une courbe polaire

1) Recherche de l'ensemble de définition, de dérivabilité de $\rho$. 
\medskip
\noindent 2) Etude de la périodicité et de la parité de $\rho$ (symétries de la courbe). 
\medskip
\noindent 3) Résoudre l'équation $\rho(\theta)=0$ (passage au pôle).
\medskip 
\noindent 4) Etudier le signe des dérivées pour dresser un tableau de variation. 
\medskip
\noindent 5) Compléter le tableau de variation en faisant apparaitre points singuliers, 

\noindent\quad tangentes, asymptotes, branches paraboliques (utiliser le repère polaire), points doubles.
\medskip
\qquad 6) Tracer la courbe polaire.



\Concept [] Branches infinies 

\noindent
On dit qu'une courbe paramétrée associée à une application $\theta\mapsto \rho(\theta)$ admet une branche infinie en $\theta_0$ dans chacune des situations suivantes : 
\medskip

\Bullet Si $\theta_0=\pm\infty$ et si $\ds\lim_{\theta\to\theta_0}\rho(\theta)=\pm\infty$, la courbe polaire admet une branche spirale. 

$$
%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-2.3,-2.6)(3.1,1.9)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{300}{1080}{t t mul 360 div 1080 div t cos mul t t mul 360 div 1080 div t sin mul}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-3,-3)(3,3)
\rput{0}(2.8,.2){$x$}
\rput{0}(.2,2.8){$y$}
\rput{0}(-.2,-.2){$0$}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(0.3,1){$1$}
\rput{0}(1,0.4){$1$}
\endpspicture
$$
\Figure Branchespirale, Branche spirale.
\bigskip

\Bullet Si $\theta_0=\pm\infty$ et si $\ds\lim_{\theta\to\theta_0}\rho(\theta)=a\in\ob R$, la courbe polaire admet un cercle asymptote de centre $0$ et de rayon $|a|$. 

$$
%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-2.5,-2.2)(3,3)
\pscircle[linecolor=blue](0,0){1.6}
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{1.1}{7}{2 1 t t mul div add t 90 mul cos mul 2 1 t t mul div add t 90 mul sin mul}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-3,-3)(3,3)
\rput{0}(2.8,.2){$x$}
\rput{0}(.2,2.8){$y$}
\rput{0}(-.2,-.2){$0$}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(0.3,1){$1$}
\rput{0}(1,0.4){$1$}
\psline[linecolor=blue]{<->}(0,0)(1.41421,-1.41421) 
\rput{-45}(0.9,-0.7){\scalebox{.8 .8}{\blue rayon}}
\rput{-45}(0.6,-1){\scalebox{.8 .8}{\blue $|a|$}}
\endpspicture
$$
\Figure Cercleasymp, Cercle asymptote de rayon $|a|$. 
\bigskip

\Bullet Si $\theta_0\in\ob R$ et si $\ds\lim_{\theta\to\theta_0}\rho(\theta)=\pm\infty$, deux cas se présentent : 
\bigskip

\noindent\quad$\star$ Si $\ds\lim_{\theta\to\theta_0}\rho(\theta)\sin(\theta-\theta_0)=a\in\ob R$, 
la courbe polaire admet une droite asymptote de vecteur directeur $\vec u(\theta_0)$ et passant par le point $P$ défini par $\vec{OM}=a\vec v(\theta_0)$. 
$$
%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-1.5,-1)(10,5)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0.01}{.8}{.5 t div t 360 mul 10 add cos mul .5 t div t 360 mul 10 add sin mul}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-1.5,-1)(10,5)
\rput{0}(9.8,.2){$x$}
\rput{0}(.2,4.8){$y$}
\rput{0}(-.2,-.2){$0$}
\rput{11}(0,0){\psline[linecolor=blue](-5,3)(20,3)}
\rput{11}(0,0){\psline[linecolor=blue]{->}(0,0)(0,3)}
\rput{11}(0,0){\psline[]{->}(0,0)(2,0)}
\rput{11}(0,0){\psline[]{->}(0,0)(0,2)}
\rput{11}(1.5,.55){\scalebox{.8 .8}{$\vec u(\theta_0)$}}
\rput{101}(-.4,1){\scalebox{.8 .8}{$\vec v(\theta_0)$}}
\rput{0}(-.7,3.2){\scalebox{.8 .8}{$P$}}
\rput{11}(0,0){\psline[linestyle=dotted](2,0)(10,0)}
\rput{11}(0,0){\psline[linecolor=black]{->}(8,0)(8,2.9)}
\rput{11}(0,0){\psline[linecolor=black]{<->}(0,0)(8,2.9)}
\rput{11}(7.25,4.4){$+$}
\rput{0}(7.25,4.7){\scalebox{.8 .8}{$M$}}
\rput{30}(3,2.2){\scalebox{.8 .8}{$\rho(\theta)\vec u(\theta_0)$}}
\rput{281}(7.8,3){\scalebox{.8 .8}{$\rho(\theta)\sin(\theta-\theta_0)$}}
\rput{101}(-.65,2.3){\scalebox{.8 .8}{$\blue a$}}
\endpspicture
$$
\Figure Droiteasym, Droite asymptote de vecteur directeur $\vec u(\theta_0)$ passant par $P$. 
\bigskip

\noindent\quad$\star$ Si $\ds\lim_{\theta\to\theta_0}\rho(\theta)\sin(\theta-\theta_0)=\pm\infty$, 
la courbe polaire admet une branche parabolique de direction la droite de vecteur directeur $\vec u(\theta_0)$. 

$$
%\psset{xunit=.8cm, yunit=.8cm}
\pspicture*[](-1.5,-1)(10,6)
\rput{15}{\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{10}{t t sqrt}}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-1.5,-1)(10,6)
\rput{0}(9.8,.2){$x$}
\rput{0}(.2,5.8){$y$}
\rput{0}(-.2,-.2){$0$}
\rput{15}(0,0){\psline[]{->}(0,0)(2,0)}
\rput{15}(0,0){\psline[]{->}(0,0)(0,2)}
\rput{15}(1.5,.55){\scalebox{.8 .8}{$\vec u(\theta_0)$}}
\rput{105}(-.5,1){\scalebox{.8 .8}{$\vec v(\theta_0)$}}
\rput{15}(0,0){\psline[linestyle=dotted](2,0)(10,0)}
\rput{15}(0,0){\psline[linecolor=black]{->}(9,0)(9,3)\rput{0}(9,3){$+$}\rput{-15}(9,3.3){\scalebox{.8 .8}{$M$}}}
\rput{15}(0,0){\psline[linecolor=black]{<->}(0,0)(9,3)}
\rput{30}(3,2.3){$\rho(\theta)\vec u(\theta_0)$}
\rput{285}(8.6,3.8){\scalebox{.8 .8}{$\rho(\theta)\sin(\theta-\theta_0)$}}
\endpspicture
$$
\Figure Branchepara4, Branche parabolique de direction orientée par $\vec u(\theta_0)$. 
\bigskip

\Subsection Courbcla, équations polaires classiques. 

\Concept [] Droite ne passant pas par l'origine

\noindent
Pour chaque droite $\sc D$ ne passant pas par $O$, il existe un unique couple $(a,b)\neq (0,0)$ tel que 
$\sc D$ soit d'équation cartésienne $ax+by=1$. Et alors, 
$\sc D$ est d'équation polaire 
$$
\rho(\theta)={1\F a\cos(\theta)+b\sin\theta}.\leqno{(*)}
$$
Inversement, $(*)$ définit une droite $\sc D$ d'équation $ax+by=1$ ne passant pas pas $O$. 
\bigskip
$$
%\psset{xunit=2cm, yunit=2cm}
\pspicture*[](-.5,-.2)(2.7,2)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{3 t cos 2 mul t sin add div t cos mul 3 t cos 2 mul t sin add div t sin mul }
\psaxes*[labels=none,ticks=none]{->}(0,0)(-.5,-.2)(2.7,2)
\rput{0}(2.5,-.1){$x$}
\rput{0}(-.1,1.8){$y$}
\rput{0}(-.1,-.1){$O$}
\psline[]{->}(0,0)(1,1)
\rput{0}(1,1){$+$}
\rput{0}(1.3, 1.1){$M(\theta)$}
\psline[linecolor=magenta]{->}(0,0)(0.707,0.707)
\rput{30}(0.8,1.4){\green$\psframe[linecolor=blue](0,0)(0.1,.1)$}
\rput{30}(0.8,1.4){\psline[linecolor=blue]{->}(0,0)(.5,0)\rput{0}(.6,.1){\scalebox{.6 .6}{$\blue \vec V=a\vec i+b\vec j$}}}
\psarc[linecolor=magenta]{->}(0,0){.6}{0}{45}
\rput{0}(0.35, .15){$\magenta\ss\theta$}
\rput{45}(0.30, .4){$\magenta\ss\vec u(\theta)$}
\rput{45}(0.9, 0.7){$\rho(\theta)$}
\rput{0}(2.1, 0.2){\red {$\sc D:ax+by=1$}}
\endpspicture
$$


\Remarque. Le vecteur $\vec V:=a\vec i+b\vec j$ est non-nul et normal à la droite $\sc D$, qui est le lieu des points $M$ pour lesquels $\vec V.\vec{OM}=1$. 

\Concept [] Cercle passant par l'origine

\noindent
$\sc C$ est un cercle de centre $\ds\Q({a\F2},{b\F2}\W)$ passant par $O$$\Longleftrightarrow$ 
$\sc C$ est d'équation polaire
$$
\rho(\theta)=a\cos\theta+b\sin\theta. \leqno{(**)}
$$

$$
%\psset{xunit=2cm, yunit=2cm}
\pspicture*[](-.5,-.7)(2.5,2)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{t cos 2 mul t sin add t cos mul t cos 2 mul t sin add t sin mul}
\psaxes*[labels=none,ticks=none]{->}(0,0)(-.5,-.7)(2.5,2)
\rput{0}(2.3,-.1){$x$}
\rput{0}(-.1,1.8){$y$}
\rput{0}(-.1,-.1){$0$}
\rput{0}(1,.5){$+$}
\rput{0}(1.25,.3){$\Q({a\F2},{b\F2}\W)$}
\psline[]{->}(0,0)(2,1)
\rput{0}(2,1){$+$}
\rput{0}(2.2,1.1){$\ss(a,b)$}
\rput{0}(1,1.618){$+$}
\rput{0}(1.1, 1.8){$M(\theta)$}
\psline[linecolor=magenta]{->}(0,0)(0.5257,0.8506)
\psline[linestyle=dashed, linecolor=green]{-}(1,1.618)(2,1)
\rput{240}(1,1.618){\green$\psframe[linecolor=green](0,0)(0.1,.1)$}
\psline[]{->}(0,0)(1,1.618)
\psarc[linecolor=magenta]{->}(0,0){.6}{0}{60}
\rput{0}(0.3, .25){$\magenta\ss\theta$}
\rput{60}(0.25, .6){$\magenta\ss\vec u(\theta)$}
\rput{60}(0.85, 1.1){$\rho(\theta)$}
\rput{0}(2.2, 0.2){\red {$\sc C$}}
\endpspicture
$$



\Remarque : comme l'inversion $\mbox{Inv}:z\mapsto1/\overline{z}$ transforme un point vérifiant $(*)$ en un point vérifiant $(**)$ et réciproquement, on montre que l'inversion $\mbox{Inv}$ transforme une droite ne passant pas par l'origine 
en cercle privé d'un point (l'origine) et vice-versa. 


\Section CPPC, Coniques.

\Subsection ConDeath, Définitions. 
\bigskip


\Definition []  Une courbe $\sc C$ est une conique du plan $\sc P$ $\Longleftrightarrow$ la courbe 
$\sc C$ admet dans un repère orthonormé $(O,\vec i,\vec j)$ de $\sc P$ une équation cartésienne du type 
$$
ax^2+bxy+cy^2+dx+ey+f=0.\eqdef{eqnr}
$$
avec $(a,b,c,d,e,f)\in\ob R^6$ et $(a,b,c)\neq(0,0,0)$. 
\bigskip


\Definition []  Soit $F$ un point du plan, $\sc D$ une droite ne passant pas par $F$ et $e>0$. Alors, la 
conique de foyer $F$, de directrice $\sc D$ et d'excentricité $e$ est l'ensemble 
$$
\sc C=\b\{M\in\sc P:MF=e MH\b\}.
$$
où $H$ désigne le projeté orthogonal de $M$ sur $\sc D$ ($MH$ est donc la distance de $M$ à $\sc D$). 
\bigskip
$$
MF=eMH
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-2.1,-1.5)(3,1.5)
\psline[linecolor=black]{-}(2.3094,-1.5)(2.3094,1.5)
\rput{0}(1.73205,0){$+$}
\rput{0}(1.53205,-.2){$F$}
\rput{0}(-1,0.86602){$+$}
\rput{0}(-1,1.2){$M$}
\rput{0}(2.3094,0.86602){$+$}
\rput{0}(2.6,0.86602){$H$}
\rput{0}(2,-1.2){$\sc D$}
\psline[linecolor=black,linestyle=dashed]{-}(-1,0.86602)(2.3094,0.86602)
\psline[linecolor=black,linestyle=dashed]{-}(-1,0.86602)(1.73205,0)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{t cos 2 mul t sin}
\endpspicture
$$
\Figure Exemplecon, Conique de foyer $F$, de directrice $\sc D$ et d'exentricité $\ds{\sqrt3\F2}$. 
\bigskip


\Remarque. Soit $\sc D$ une droite ne passant pas par un point $F$. Pour chaque nombre $e>0$, l'ensemble des points $M$ du plan vérifiant 
$$
{MF\F MH}=e,
$$
où $H$ est la projection orthogonale de $M$ sur $\sc D$, est appelé une ligne de niveau de~$\ds{MF\F MH}$. Ces lignes de niveau sont des coniques. 
\bigskip


\noindent{\bf Conique.} Une conique $\sc C$ de foyer $O$ (l'origine), d'excentricité $e$ et de 
directrice $\sc D$ est d'équation polaire 
$$
{
\rho(\theta)={e\|\vec {OK}\|\F1+e\cos(\theta-\theta_0)}}, 
$$
où $K$ est le projeté orthogonal de $O$ sur $\sc D$ et où $\theta_0:=\widehat{(\vec i,\vec{OK})}$. \pn
Le nombre $p=e\|\vec{OK}\|$ est appelé paramètre de la conique $\sc C$. 
\bigskip
$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-4,-2)(2,1.7)
\psaxes*[labels=none,ticks=none]{->}(-1.5,-.85)(-4,-1.8)(2,1.7)
\psline[]{->}(-1.5,-.85)(-.5,-.85)
\rput{0}(1.8,1.5){\red$\sc C$}
\rput{0}(1.8,-1.1){$x$}
\rput{30}(-2.2,.7){\magenta $\theta-\theta_0$}
\rput{0}(-1.7,1.5){$y$}
\rput{0}(-.7,-.55){$\vec i$}
\rput{0}(0,0){$\theta$}
\rput{0}(-.9,0){$r$}
\rput{0}(-3.8,-.3){\green $\theta_0$}
\rput{210}(0,0){
\psline[linecolor=blue]{-}(2.3094,-2.5)(2.3094,1.5)
\rput{-210}(1.73205,0){$+$}
\rput{-210}(1.53205,.3){$O$}
\rput{-210}(-1,-0.86602){$+$}
\rput{-210}(-1,-1.1){$M$}
\rput{-210}(2.3094,0){$+$}
\psline[linecolor=green, linestyle=dashed]{-}(1.73205,0)(4,0)
\rput{-210}(2.6,0){$K$}
\rput{-210}(2,-2.4){\blue $\sc D$}
\psline[linecolor=black,linestyle=dashed]{<-}(-1,-0.86602)(1.73205,0)
\psarc[linecolor=green,linestyle=dotted]{->}(1.73205,0){2}{-210}{0}
\psarc[]{->}(1.73205,0){1.5}{-210}{-165}
\psarc[linecolor=magenta,linestyle=dashed]{<-}(1.73205,0){1.5}{-165}{0}
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{t cos 2 mul t sin}}
\endpspicture
$$

\noindent{\bf Réciproque.} Soit $(a,b,c)\in\ob R^3$ avec $a\neq 0$ et soit $\sc C$ la courbe d'équation polaire 
$$
{
\rho(\theta)={1\F a+b\cos\theta+c\sin\theta}}.
$$ 
Si $(b,c)=(0,0)$, $\sc C$ est un cercle de centre $O$ et de rayon $1/|a|$ . \pn
Si $(b,c)\neq(0,0)$, $\sc C$ est une conique de foyer $O$, d'exentricité $\ds e$ 
et de directrice $\sc D$ avec 
$$
{e={\sqrt{b^2+c^2}\F|a|}}\qquad\mbox{et}\qquad \sc D:\quad \ds\rho(\theta)={1\F b\cos\theta+c\sin\theta}
$$


\Subsection ConEllip, Ellipse.
\bigskip

Soit $a\ge b>0$ et soit $(O,\vec i,\vec j)$ un repère orthonormé. Alors, la courbe $\sc E$ d'équation 
$$
{{x^2\F a^2}+{y^2\F b^2}=1} \eqdef{eqrel}
$$
est appelée ellipse d'axe focal $(O,\vec i)$, de demi-grand axe $a$, de demi-petit axe $b$ et d'exentricité 
$$
{
e=\sqrt{1-{b^2\F a^2}}}. 
$$
C'est une conique admettant deux couples foyer-directrice $(F,\sc D)$ et $(F',\sc D')$, avec $F$ et $F'$ de coordonnées $(ae,0)$ et $(-ae,0)$ et 
$\sc D$ et $\sc D'$ d'équation cartésiennes $x=a/e$ et $x=-a/e$. 
Les points $A(a,0)$, $B(0,b)$, $C(-a,0)$ et $D(0,-b)$ sont appelés les sommets de l'ellipse $\sc E$. 
$$
%\psset{xunit=2cm, yunit=2cm}
\pspicture*[](-2.2,-1.4)(2.3,1.4)
\psaxes*[labels=none,ticks=none]{->}(0,0)(-2.2,-1.4)(2.3,1.4)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{t cos 2 sqrt mul t sin}
\rput{0}(1,0){$+$}
\rput{0}(-1,0){$+$}
\rput{0}(1,.2){$F$}
\rput{0}(-1,.2){$F'$}
\rput{0}(-.1,-.1){$O$}
\rput{0}(.25,.1){$\vec i$}
\psline[]{->}(0,0)(.5,0)
\rput{0}(-.1,.25){$\vec j$}
\psline[]{->}(0,0)(0,0.5)
\psline[linestyle=dashed]{-|}(0,-1.1)(1.41,-1.1)
\rput{0}(0.7,-1){$a$}
\psline[linecolor=magenta, linestyle=dashed]{-|}(0,-0.5)(1,-0.5)
\rput{0}(0.5,-.38){\magenta$ae$}
\psline[linestyle=dotted]{-}(1,0)(1,-0.5)
\psline[linecolor=blue, linestyle=dashed]{-}(0,-1.35)(2,-1.35)
\rput{0}(1,-1.28){\blue$a/e$}
\psline[linestyle=dashed]{-|}(-1.61,0)(-1.61,1)
\rput{0}(-1.71,.5){$b$}
\psline[linestyle=dotted]{-}(1.41,0)(1.41,-1.2)
\psline[linestyle=dotted]{-}(0,-1)(0,-1.2)
\psline[linestyle=dotted]{-}(0,1)(-1.61,1)
\psline[linestyle=dotted]{-}(-1.41,0)(-1.61,0)
\psline[linecolor=blue]{-}(-2,-1.4)(-2,1.4)
\rput{0}(-1.85,-1.2){\blue$\sc D'$}
\psline[linecolor=blue]{-}(2,-1.4)(2,1.4)
\rput{0}(1.85,1.2){\blue$\sc D$}
\rput{0}(0.9,1){\red$\sc E$}
\rput{0}(1.41,0){$+$}
\rput{0}(-1.41,0){$+$}
\rput{0}(0,1){$+$}
\rput{0}(0,-1){$+$}
\rput{0}(1.51,0.1){$A$}
\rput{0}(-1.51,-0.1){$C$}
\rput{0}(0.1,1.1){$B$}
\rput{0}(-0.1,-1.1){$D$}
\rput{0}(-.1,1.3){$y$}
\rput{0}(2.25,.1){$x$}
\endpspicture
$$
\Figure Ellipse, L'ellipse $\sc E$ et ses éléments caractéristiques.
\bigskip

\Remarque. Un paramètrage usuel de l'ellipse $\sc E$ est 
$$
{
\Q\{\eqalign{x(t)=a\cos t\cr y(t)=b\sin t\cr}\W.}\qquad(t\in\ob R). 
$$
\medskip
\noindent
Inversement, si $\sc E$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$0\!<\!e\!<\!1$, alors $\sc E$ satisfait l'équation \eqref{eqrel} pour 
$$
a={e FK\F1-e^2}\qquad\mbox{et}\qquad b={e FK\F\sqrt{1-e^2}}
$$
dans le repère orthonormé direct $(O,\vec i,\vec j)$ uniquement déterminé par 
$$
\vec i={\vec {FK}\F FK}\qquad \vec{KO}={\vec {KF}\F1-e^2}, 
$$
où $K$ désigne la projection orthogonale de $F$ sur $\sc D$. 
\bigskip

\Propriete []  Soient $F$ et $F$' deux points distincts du plan et $\ell>F'F$ un nombre réel. Alors, le lieu des points $M$ vérifiant
$$
F'M+MF=\ell
$$
est l'ellipse de foyers $F$ et $F'$, de grand axe $\ell$, de petit axe $\ds\sqrt{\ell^2-F'F^2}$ et d'exentricité 
$$
e={F'F\F\ell}.
$$ 

\Subsection ConHyper, Hyperbole.
\bigskip

Soient $a>0$, $b>0$ et $(O,\vec i,\vec j)$ un repère orthonormé. Alors, la courbe $\sc H$ d'équation
$$
{{x^2\F a^2}-{y^2\F b^2}=1}. \eqdef{eqrel2}
$$
est appelée hyperbole de centre $O$, d'axe transverse (ou focal) 
$(O,\vec i)$ et d'exentricité 
$$
e=\sqrt{1+{b^2\F a^2}}. 
$$
Elle est constituée de deux composantes connexes $\sc H_1$ et $\sc H_2$ et admet deux couples foyer-directrice $(F,\sc D)$ et $(F',\sc D')$, 
avec $F$ et $F'$ de coordonnées $(ae,0)$ et $(-ae,0)$ et 
$\sc D$ et $\sc D'$ d'équation cartésiennes $x=a/e$ et $x=-a/e$.
\pn
Les points $S$ et $S'$ de coordonnées $(a,0)$ et $(-a,0)$ sont appelés sommets de l'hyperbole~$\sc H$, qui admet également deux asymptotes $\Delta$ et $\Delta'$ 
d'équation $\ds{y\F b}+{x\F a}=0$ et ${y\F b}-{x\F a}=0$. 
$$
%\psset{xunit=1.5cm, yunit=1.5cm}
\pspicture*[](-3,-2.5)(3,2.5)
\psaxes*[labels=none,ticks=none]{->}(0,0)(-3,-2.5)(3,2.5)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{-2.5}{2.5}{t t mul 1 add sqrt t }
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{-2.5}{2.5}{0 t t mul 1 add sqrt sub t }
\psline[linecolor=magenta]{-}(-3,-3)(3,3)
\psline[linecolor=magenta]{-}(-3,3)(3,-3)
\rput{0}(1.41,0){$+$}
\rput{0}(-1.41,0){$+$}
\rput{0}(1.41,.2){$F$}
\rput{0}(-1.41,.2){$F'$}
\rput{0}(-.1,-.3){$O$}
\rput{0}(.5,.2){$\vec i$}
\psline[]{->}(0,0)(.5,0)
\rput{0}(-.2,.5){$\vec j$}
\psline[]{->}(0,0)(0,0.5)
\psline[linecolor=blue]{-}(0.707,-2.5)(0.707,25)
\rput{0}(0.9,2.3){\blue$\sc D$}
\psline[linecolor=blue]{-}(-0.707,-2.5)(-0.707,2.5)
\rput{0}(-1,-2.3){\blue$\sc D'$}
\psline[linestyle=dotted]{-}(1.41,0)(1.41,-2)
\psline[linestyle=dashed]{-|}(0,-2)(1.41,-2)
\rput{0}(0.9,-1.9){$ae$}
\psline[linestyle=dotted]{-}(1,0)(1,-1.6)
\psline[linestyle=dashed, linecolor=red]{-|}(0,-1.6)(1,-1.6)
\rput{0}(0.5,-1.5){\red $a$}
\psline[linestyle=dashed,linecolor=blue]{-}(0,-1.2)(0.707,-1.2)
\rput{0}(0.35,-1.1){\blue$a/e$}
\rput{0}(-2.5,2){\red$\sc H_2$}
\rput{0}(2.5,2){\red$\sc H_1$}
\rput{0}(1,0){$+$}
\rput{0}(-1,0){$+$}
\rput{0}(0.85,0.15){$S$}
\rput{0}(-.85,-0.15){$S'$}
\rput{0}(-.1,2.4){$y$}
\rput{0}(2.8,.1){$x$}
\rput{0}(2,2.2){$\magenta \Delta'$}
\rput{0}(2,-2.2){$\magenta \Delta$}
\endpspicture
$$
\Figure Hyperboreen, L'hyperbole $\sc H$ 
et ses éléments caractéristiques. 
\bigskip\goodbreak

\Remarque. Un paramètrage usuel des branches de l'hyperbole $\sc H$ est 
$$
(\sc H_1)\quad \Q\{\eqalign{
x(t)&=a\ch t
\cr 
y(t)&=b\sh t
\cr}\W.
\qquad\mbox{et}\qquad(\sc H_2)\quad 
\Q\{\eqalign{
x(t)&=-a\ch t\cr 
y(t)&=b\sh t\cr
}\W.
\qquad(t\in\ob R). 
$$
\medskip
\noindent
Inversement, si $\sc H$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$e>1$, alors~$\sc H$ satisfait l'équation \eqref{eqrel2} pour 
$$
a={e FK\F e^2-1}\qquad\mbox{et}\qquad b={e FK\F\sqrt{e^2-1}}
$$
dans le repère orthonormé direct $(O,\vec i,\vec j)$ uniquement déterminé par 
$$
\vec i={\vec {FK}\F FK}\qquad \vec{KO}={\vec {KF}\F1-e^2}, 
$$
où $K$ est la projection orthogonale de $F$ sur $\sc D$. 
\bigskip

\Propriete []  Soient $F$ et $F$' deux points distincts du plan et $\ell\in\Q]0,F'F\W[$ un nombre réel. Alors, le lieu des points $M$ vérifiant
$$
|F'M-MF|=\ell
$$
est l'hyperbole de foyers $F$ et $F'$ d'exentricité 
$$
e={F'F\F\ell}.
$$ 

\Subsection ConPara, Parabole.
\bigskip
\noindent
Soit $p>0$ et soit $(O,\vec i,\vec j)$ un repère orthonormé du plan. Alors, la courbe $\sc P$ d'équation 
$$
y^2=2px\eqdef{eqrel3}
$$
est appelée parabole de sommet $O$, d'axe $(O,\vec i)$ 
et de paramètre $p$. Elle~admet un foyer~$F$ de coordonnées~$\ds({p\F2},0)$ 
et une directrice $\sc D$ d'équation $\ds x=-{p\F2}$. Son excentricité est $e=1$. 

$$
%\psset{xunit=2cm, yunit=1.8cm}
\pspicture*[](-2,-2)(2,2)
\psaxes*[labels=none,ticks=none]{->}(0,0)(-1,-2)(2,2)
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{-2}{2}{t t mul 2 div t }
\rput{0}(.5,0){$+$}
\rput{0}(.5,.2){$F$}
\rput{0}(-.1,-.1){$O$}
\rput{0}(1,.2){$\vec i$}
\psline[]{->}(0,0)(1,0)
\rput{0}(-.2,1){$\vec j$}
\psline[]{->}(0,0)(0,1)
\psline[linecolor=blue]{-}(-0.5,-3)(-0.5,3)
\rput{0}(-0.65,-1.8){\blue$\sc D$}
\psline[linestyle=dotted]{-}(.5,0)(.5,-1.6)
\psline[linestyle=dashed]{-|}(0,-1.6)(.5,-1.6)
\rput{0}(0.25,-1.45){$p/2$}
\psline[linestyle=dashed,linecolor=blue]{-}(0,-1)(-.5,-1)
\rput{0}(-0.3,-.85){\blue$p/2$}
\rput{0}(1.8,1.7){\red$\sc P$}
\rput{0}(-.1,1.9){$y$}
\rput{0}(1.9,.1){$x$}
\endpspicture
$$
\Figure Parab, Parabole $\sc P$ et ses éléments caractéristiques. 
\bigskip\goodbreak

\Remarque. Un paramètrage usuel de la parabole $\sc P$ est 
$$
{
\Q\{\eqalign{x(t)&={t^2\F 2p}\cr y(t)&=t\cr}\W.}\qquad(t\in\ob R). 
$$
\medskip

\noindent
Inversement, si $\sc P$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$e=1$, alors~$\sc P$ satisfait l'équation \eqref{eqrel3} pour 
$p=FK$ 
dans le repère orthonormé direct $(O,\vec i,\vec j)$ 
uniquement déterminé par 
$$
\vec i=-{\vec {FK}\F FK}\qquad \vec{FO}={\vec {FK}\F2}, 
$$
où $K$ est la projection orthogonale de $F$ sur $\sc D$.
\bigskip

\Subsection ConRap, Reduction.
\bigskip


\noindent{\bf But. }Etant donnée une conique $\sc C$ d'équation 
$$
ax^2+bxy+cy^2+dx+ey+f=0,\eqdef{eqnr}
$$
on applique une rotation puis une translation (ou l'inverse) au repère $(O,\vec i,\vec j)$ pour obtenir un repère orthonormé 
$(C,\vec {u},\vec{v})$ dans lequel la conique $\sc C$ à une équation ``réduite'', plus simple que \eqref{eqnr}. 
\bigskip 


$$
%\psset{xunit=2cm, yunit=2cm}
\pspicture*[](-2.3,-1.5)(2.3,1.4)
\psaxes*[labels=none,ticks=none]{->}(-2,-1.3)(-2.2,-1.5)(2.3,1.4)
\rput{30}(0,0){
\parametricplot[linewidth=1pt,linecolor=red,plotpoints=200]{0}{360}{t cos 2 sqrt mul t sin}
\rput{0}(1,0){$+$}
\rput{0}(-1,0){$+$}
\rput{-30}(1,.2){$F$}
\rput{-30}(-1,.2){$F'$}
\rput{-30}(-.1,-.1){$O'$}
\rput{0}(.25,.1){$\vec u$}
\psline[]{->}(0,0)(.5,0)
\rput{0}(-.1,.25){$\vec v$}
\psline[]{->}(0,0)(0,0.5)
\rput{-30}(0.9,1){\red$\sc E$}
\rput{0}(1.41,0){$+$}
\rput{0}(-1.41,0){$+$}
\rput{0}(0,1){$+$}
\rput{0}(0,-1){$+$}
\rput{-30}(1.51,0.1){$A$}
\rput{-30}(-1.51,-0.1){$C$}
\rput{-30}(0.1,1.1){$B$}
\rput{-30}(-0.1,-1.1){$D$}}
\rput{0}(-2.1,1.3){$y$}
\rput{0}(2.25,-1.2){$x$}
\rput{0}(-2.1,-1.4){$O$}
\rput{0}(-1.7,-1.15){$\vec i$}
\psline[]{->}(-2,-1.3)(-1.5,-1.3)
\rput{0}(-2.15,-.9){$\vec j$}
\psline[]{->}(-2,-1.3)(-2,-.8)
\endpspicture
$$
\Figure Ellipss, Une ellipse et un repère orthonormée $(C,\vec u,\vec v)$ qui la réduit. 
\bigskip

\noindent{\bf Méthode. }1) Calculer le discriminant 
$$
\Delta:=b^2-4ac
$$ 
pour déterminer le type de la conique : Si $\sc C$ n'est pas dégénérée (droite(s), point ou $\emptyset$), c'est une ellipse si $\Delta<0$, une parabole si $\Delta=0$ 
et une hyperbole si $\Delta>0$. 
\medskip\noindent
2) Si l'équation \eqref{eqnr} comporte un terme croisé (du type $bxy$ avec $b\neq0$), faire une rotation d'angle $\theta$ bien choisi pour s'en débarasser. 
$$
\Q\{
\eqalign{\vec u=\cos\theta\vec i+\sin\theta j,\cr
\vec v=-\sin\theta\vec i+\cos\theta j}\W.\qquad \vec{OM}=x\vec i+y\vec j=X\vec u+Y\vec v
$$
En rempla\c cant $x$ et $y$ dans \eqref{eqnr} par $x=X\cos\theta-Y\sin\theta$ et $y=X\sin\theta+Y\cos\theta$, où $(X,Y)$ désigne les coordonnées de $M$ dans le repère $(O,\vec u,\vec v)$, 
on obtient alors une équation du type suivant (avec $\Delta=-4\alpha\beta$) 
$$
\alpha X^2+\beta Y^2+\gamma X+\delta Y+\Gamma=0\qquad(\Delta=b^2-4ac=\alpha\beta).\eqdef{eqnr2}
$$
3) Essayer de se débarasser des termes linéaires ($\gamma X$ et $\delta Y$) en faisant une translation 
$$
\underbrace{X\vec u+Y\vec v}_{\vec{OM}}=\underbrace{x_c\vec u+y_c\vec v}_{\vec{OC}}+\underbrace{\sc X\vec u+\sc Y\vec v}_{\vec{CM}},
$$
qui fait intérvenir le centre $C$ du nouveau repère. On remplacera $X$ et $Y$ dans \eqref{eqnr2} par $X=x_c+\sc X$ et $Y=y_c+\sc Y$, 
où $(x_c,y_c)$ désigne les coordonnées de $C$ dans $(O,\vec u,\vec v)$ et où~$(\sc X,\sc Y)$ désigne les coordonnées du point $M$ dans le repère $(C,\vec u,\vec v)$.
\medskip
\noindent
4) Reconnaitre la conique. 






                                             




\hautspages{Olus Livius Bindus}{Equations différentielles linéaires}


\Chapter ADL, Equations différentielles linéaires.
\bigskip

Dans tout ce chapitre, $I$ est un intervalle non vide  et on pose  $\ob K:=\ob R$ ou $\ob K:=\ob C$ . 
\bigskip

\Section ADL0, Rappels. 

\Definition []  Une primitive d'une application $f$ sur un intervalle $I$ est une application~$F$, dérivable sur $I$, telle que 
$$
\forall x\in I, \qquad F'(x)=f(x).
$$

\Definition []  Soient $f\!:\![a,b]\to\ob C$ une fonction continue (par morceaux) et $f_1,f_2\!:\![a,b]\to\ob R$, ses composantes réelles et imaginaires. Autrement dit 
$$
\forall x\in[a,b],\qquad f(x)=f_1(x)+if_2(x).
$$
Alors, l'intégrale sur le segment $[a,b]$ de la fonction complexe $f$ est 
$$
\int_a^bf(t)\d t:=\int_a^bf_1(t)\d t+i\int_a^bf_2(t)\d t.
$$

\Theoreme [$f:{[a,b]}\to\ob C$ fonction continue et $F:{[a,b]}\to\ob C$ primitive de $f$ sur ${[a,b]}$]
$$
\int_a^bf(t)\d t=F(b)-F(a)=:{[F]}_a^b.
$$

\Section ADL1, Equations différentielles linéaires du $1^{\mbox{\sevenrm er}}$ ordre.

\Subsection ADL10, Equation différentielle linéaire $y'=0$. 

\Theoreme [$f:I\to\ob K$ fonction dérivable] 
La fonction $f$ est une solution de l'équation différentielle 
$$
\forall x\in I , \qquad  f'(x)=0 
$$
si, et seulement si, la fonction $f$ est constante sur l'intervalle $I$  c'est  à  dire  si,  et  seulement  s'il  existe
$c\in\ob K$  tel que $$ 
\forall x\in I , \qquad f(x)=c.
$$

\Subsection ADL11, Equation différentielle $y'=a(x)$. 

\Theoreme [$x_0\in I$ et $a:I\to\ob K$ fonction continue] 
Une fonction dérivable  $f:I\to\ob K$  est une solution de l'équation différentielle 
$$
\forall x\in I , \qquad  f'(x)=a(x) 
$$
si, et seulement si, la fonction $f$ est une primitive sur l'intervalle $I$ de la fonction $a$, c'est à dire
si,  et seulement s'il existe $c\in\ob K$ tel que 
$$ 
\forall x\in I, \qquad f(x)=\int_{x_0}^xa(t)\d t+c. 
$$


\Subsection ADL12, Equation différentielle linéaire $y'=ay$. 

\Theoreme [$a\in\ob K$]
Une fonction dérivable $f:I\to\ob K$ est une solution de l'équation différentielle linéaire 
$$
\forall x\in I, \qquad f'(x)=af(x)
$$
si, et seulement s'il existe $\lambda\in\ob K$ tel que 
$$
\forall x\in I,\qquad f(x)=\lambda\e^{ax}.
$$
En particulier, la fonction $x\mapsto\e^{ax}$ est l'unique solution du problème de cauchy
$$
\Q\{\eqalign{
f'(x)&=af(x)\qquad(x\in\ob R),\cr
f(0)&=1.
}\W.
$$

\noindent{\bf Application}. Les fonctions dérivables $f:\ob R\to\ob K$ vérifiant l'équation fonctionnelle 
$$
\forall (x,y)\in\ob R^2,\qquad f(x+y)=f(x)f(y)
$$
sont les fonctions $x\mapsto\e^{ax}$ pour $a\in\ob K$ et la fonction nulle $x\mapsto0$. 
\bigskip

\Subsection ADL13, Equation différentielle linéaire $y'+a(x)y=0$. 

\noindent
Dans cette section, $I\neq\emptyset$ est un intervalle  et  $a:I\to\ob C$ est une fonction continue . 
\bigskip

\Definition []  Une solution de l'équation différentielle $y'+a(x)y=0$ sur un intervalle~$I$
est une fonction dérivable $g:I\to\ob K$ telle que 
$$
g'(x)+a(x)g(x)=0\qquad(x\in I).\leqno{(H)}
$$

\Propriete []  Soient $g_1$ et $g_2$ des solutions  de l'équation $(H)$. Alors, pour  $(\lambda,\mu)\in\ob K^2$ , 
l'application $\lambda g_1+\mu g_2$ est une solution de l'équation différentielle $(H)$. 
\bigskip

\Theoreme [$x_0\in I$]
Une fonction $g:I\to\ob K$ est une solution de l'équation $(H)$
si, et~seulement s'il existe $\lambda\in\ob K$ tel que 
$$
\forall x\in I,\qquad g(x)=\lambda\underbrace{\exp\Q(\int_{x_0}^xa(t)\d t\W)}_{g_0(x)}.
$$
En d'autres mots, l'ensemble des solutions de l'équation différentielle linéaire $(H)$ est une droite vectorielle 
engendrée par une solution ne s'annulant pas, la fonction $g_0$. 

\centerline{\bf Résolution élégante de l'équation différentielle $(H)$ sur $I$}
\medskip
\noindent\ 1) {\bf Au brouillon :} trouver une solution particulière $g_0$ de $(H)$ ne s'annulant pas. 

\noindent Ecrire que ${g'\F g}=-a(x)$ puis intégrer en utilisant que $\ln g$ est une primitive de ${g'\F g}$. 
Enfin, utiliser l'exponentielle pour en déduire $g$.  
\smallskip
\noindent2) {\bf Sur votre copie}, vérifier que la fonction $g_0$ trouvée au 1) satisfait $(H)$ et ne s'annule pas sur $I$. 
\smallskip
\noindent\quad 3) En utilisant que l'ensemble solution de $(H)$ est une droite vectorielle, 

\centerline{conclure que les solutions $g$ de $(H)$ sont les fonctions $g=\lambda g_0$ pour $\lambda\in\ob K$. } 
\bigskip

\noindent
Pour chaque $x_0\in I$ et chaque nombre $c\in\ob K$, le problème de Cauchy
$$
\Q\{\eqalign{
&g'(x)+a(x)g(x)=0\qquad(x\in I),\cr
&g(x_0)=c.
}\W.
$$
admet une unique solution $g:I\to\ob K$, la fonction 
$$
g:x\mapsto c\exp\Q(\int_{x_0}^xa(t)\d t\W).
$$

\Subsection ADL13, Equation différentielle linéaire avec second membre $y'+a(x)y=b(x)$. 

\noindent
Dans cette section, $I\neq\emptyset$ est un intervalle  et les fonctions  $a,b:I\to\ob K$ sont continues . 
\bigskip


\Definition []  Une solution sur un intervalle~$I$ de l'équation différentielle $y'+a(x)y=b(x)$ est une fonction dérivable $f:I\to\ob K$ telle que 
$$
\forall x\in I, \qquad f'(x)+a(x)f(x)=b(x). \leqno{(E)}
$$

\Propriete []  Etant données deux solutions $f$ et $f_0$ de l'équation différentielle $(E)$, 
la~différence $g:=f-f_0$ est une solution de l'équation différentielle homogène associée 
$$
g'(x)+a(x)g(x)=0\qquad(x\in I). \leqno{(H)}
$$ 
\medskip

\Theoreme La solution générale $f$ de l'équation $(E)$ est la somme d'une solution particulière $f_0$ de l'équation $(E)$ et de la solution générale $g$ de l'équation homogène~$(H)$. 
$$
\forall x\in I, \qquad\underbrace{f(x)}_{\mbox{solution générale de }(E)}=
\underbrace{f_0(x)}_{\mbox{solution particulière de }(E)}+\underbrace{g(x)}_{\mbox{solution générale de }(H)}
$$


\Theoreme [$x_0\in I$]
Une fonction $f:I\to\ob K$ est une solution de l'équation $(E)$
si, et~seulement s'il existe $\lambda\in\ob K$ tel que 
$$
\forall x\in I,\qquad f(x)=\underbrace{\int_{x_0}^xb(t)\exp\Q(\int_t^xa(u)\d u\W)\d t}_{f_0(x)}
+\lambda\underbrace{\exp\Q(\int_{x_0}^xa(t)\d t\W)}_{g_0(x)}.
$$


\centerline{\bf Résolution de $(E)$ par la variation de la constante}
\medskip
\noindent\qquad\ \ {\bf Au brouillon : } 1) Par la méthode précédente, trouver une solution $g_0$, 

\quad ne s'annulant pas sur~$I$, de l'équation homogène $(H)$. 
\medskip\noindent
\centerline{2) Chercher une solution particulière $f_0$ de l'équation $(E)$. Pour cela, on pose} 
$$
f_0(x)=g_0(x)h(x)\qquad (x\in I).\eqdef{yaa}
$$
En reportant cette relation dans $(E)$ et en utilisant que $g_0$ est une solution de $(H)$, on en déduit que 
la fonction inconnue $h$ satisfait $h'(x)=b(x)/g_0(x)$ pour $x\in I$. 

\noindent\ En~intégrant, on trouve $h$ puis on déduit de \eqref{yaa} une solution $f_0$ de $(E)$. \medskip
\noindent\quad\ {\bf Sur votre copie : } 3) Vérifier que la fonction $g_0$ trouvée en 1) satisfait~$(H)$. 
\medskip
\centerline{4) En déduire que les solutions de $(H)$ sont de la forme $g=\lambda g_0$ pour $\lambda\in\ob C$. }
\medskip

\centerline{5) Vérifier que la fonction $f_0$ trouvée en 2) est solution de $(E)$. \qquad}
\medskip

\centerline{6) Conclure que les solutions $f$ de l'équation différentielles $(E)$}\centerline{sont les fonctions $f=f_0+\lambda g_0$ pour $\lambda\in \ob C$. }
\bigskip
\noindent
Pour chaque élément $x_0\in I$ et chaque nombre $c\in\ob K$, le problème de Cauchy
$$
\Q\{\eqalign{
&f'(x)+a(x)f(x)=b(x)\qquad(x\in I),\cr
&f(x_0)=c.
}\W.
$$
admet une unique solution $f:I\to\ob K$.
\bigskip

\Remarque{ \it 1.\/} Si $\alpha,\beta,\gamma:I\to\ob K$ sont des fonctions continues et si l'application $\alpha$ ne s'annule pas sur $I$, résoudre l'équation différentielle 
$$
\alpha(x)f'(x)+\beta(x)f(x)=\gamma(x)\qquad (x\in I)
$$
revient à résoudre l'équation différentielle $(E)$ pour les fonctions $a$ et $b$ définies par 
$$ 
\forall x\in I, \qquad a(x):={\beta(x)\F\alpha(x)}\qquad\mbox{et}\qquad b(x):={\gamma(x)\F\alpha(x)}.
$$

\Remarque{ \it 2 \bf (Principe de superposition).} Si $b(x)=b_1(x)+b_2(x)$ pour $x\in I$ et si les~fonctions $f_1$ et $f_2$ vérifient sur l'intervalle $I$ les équations 
$$
\eqalign{
f_1'(x)+a(x)f_1(x)&=b_1(x)\qquad(x\in I),\cr
f_2'(x)+a(x)f_2(x)&=b_2(x)\qquad(x\in I),
}
$$ 
alors la fonction $f=f_1+f_2$ est solution de l'équation différentielle~$(E)$. 

\Section ADLA, Méthode d'Euler.


Il existe beaucoup d'équations différentielles que l'on ne sait pas résoudre, c'est-à-dire dont on ne connait pas la forme explicite des solutions. 
A défaut de pouvoir déterminer exactement les solutions, on peut cependant en chercher de bonnes approximations, à~l'aide de procédés de construction de solutions approchées. 
La méthode d'Euler est l'un de ces procédés. \bigskip


\noindent
Soient $[a,b]$ un segment et $c$ un nombre réel. Pour construire par la méthode d'Euler une~solution approchée $\tilde f$ du problème de Cauchy
$$
\Q\{\eqalign{
&f'(x)=\alpha(x)f(x)+\beta(x)\qquad(a\le x\le b),
\cr
&f(a)=c, 
}\W.\eqdef{cauchy}
$$
on commence par choisir un entier $N\ge1$ pour le nombre de pas de la méthode d'Euler, 
puis on découpe l'intervalle $[a,b]$ en $N$ petits segments $[x_0,x_1],\cdots,[x_{N-1},x_N]$ en posant 
$$
x_k:=a+k{b-a\F N}\qquad (0\le k\le N)
$$
de sorte que chaque segment $[x_k,x_{k+1}]$ est de longueur $\ds{b-a\F N}$ et que 
$$
a=x_0<x_1<x_2<x_3<\cdots<x_{N-1}<x_N=b
$$
La fonction continue $\tilde f$ est alors définie en N étapes de la fa\c con suivante : \medskip\noindent
\noindent{\bf Etape $0$}. La fonction $\tilde f$ est définie sur $[x_0,x_1]$ comme l'unique fonction affine $\tilde f$ vérifiant 
$$
\tilde f(x_0)=f(x_0)\qquad\mbox{et}\qquad \tilde f'(x_0)=f'(x_0). 
$$
Comme $x_0=a$, il résulte alors du problème de Cauchy \eqref{cauchy} que 
$$
\tilde f(x_0)=c\qquad\mbox{et}\qquad\tilde f'(x_0)=\alpha(x_0)c+\beta(x_0).
$$ 
La fonction $\tilde f$ étant affine sur le segment $[x_0,x_1]$, elle satisfait alors 
$$
\tilde f(x)=\tilde f(x_0)+\tilde f'(x_0)(x-x_0)\qquad (x_0\le x\le x_1).
$$

\medskip
\noindent
{\bf Etape $k$ (pour $1\le k<N$)}. La fonction $\tilde f$ ayant été construite sur l'intervalle $[x_0,x_k]$ au cours des précédentes étapes, 
on pose $c_k:=\tilde f(x_k)$ et on définit $\tilde f$ comme l'unique fonction affine sur $[x_k,x_{k+1}]$ vérifiant 
$$
\tilde f(x_k)=c_k\qquad\mbox{et}\qquad \tilde f'(x_k)=\alpha(x_k)c_k+\beta(x_k). 
$$
La fonction $\tilde f$ étant affine sur $[x_k,x_{k+1}]$, elle satisfait alors 
$$
\tilde f(x)=\tilde f(x_k)+\tilde f'(x_k)(x-x_k)\qquad (x_k\le x\le x_{k+1}).
$$


%%\readdata{\tangraph}{Graphes/GraphTan.txt}
%\psset{xunit=2cm,yunit=.7cm}
\pspicture*[](-.5,-.5)(3.5,7.6)
\psaxes*[labels=none,ticks=none]{<->}(0,0)(-.2,-.5)(2,7.6)
\psline[linewidth=.5pt,linecolor=\LSB](0,1)(2,1)
\psline[linewidth=.5pt,linecolor=blue](0,1)(1,1)(2,2)
\psline[linewidth=.5pt,linecolor=\DSG](0,1)(0.666667,1)(1.33333,1.44444)(2.,2.7284)
\psline[linewidth=.5pt,linecolor=green](0,1)(0.4,1)(0.8,1.16)(1.2,1.5312)(1.6,2.26618)(2.,3.71653)
\psline[linewidth=.5pt,linecolor=\LS](0,1)(0.2,1)(0.4,1.04)(0.6,1.1232)(0.8,1.25798)(1.,1.45926)(1.2,1.75111)(1.4,2.17138)(1.6,2.77937)(1.8,3.66877)(2.,4.98952)
\psline[linewidth=.5pt,linecolor=red](0.,1)(0.0666667,1.)(0.133333,1.00444)(0.2,1.01337)(0.266667,1.02688)(0.333333,1.04514)(0.4,1.06837)(0.466667,1.09686)
(0.533333,1.13098)(0.6,1.17119)(0.666667,1.21804)(0.733333,1.27218)(0.8,1.33437)(0.866667,1.40554)(0.933333,1.48675)(1.,1.57925)(1.06667,1.68454)
(1.13333,1.80433)(1.2,1.94065)(1.26667,2.09591)(1.33333,2.27289)(1.4,2.47493)(1.46667,2.70592)(1.53333,2.9705)(1.6,3.27415)(1.66667,3.6234)(1.73333,4.026)
(1.8,4.49122)(1.86667,5.03017)(1.93333,5.65614)(2.,6.38516)
\dataplot[plotstyle=curve,linewidth=.5pt,linecolor=black]{0 1 0.1 1.00501 0.2 1.0202 0.3 1.04603 0.4 1.08329 0.5 1.13315 0.6 1.19722 0.7 1.27762 0.8 1.37713 
0.9 1.4993 1 1.64872 1.1 1.83125 1.2 2.05443 1.3 2.32798 1.4 2.66446 1.5 3.08022 1.6 3.59664 1.7 4.24185 1.8 5.05309 1.9 6.07997 2 7.38906}
\rput{0}(2.5,1){{\LightSkyBlue un pas}}
\rput{0}(2.5,2){{\blue deux pas}}
\rput{0}(2.5,3.71653){ cinq pas }
\rput{0}(2.5,2.7284){{\DarkSeaGreen trois pas}}
\rput{0}(2.5,4.98952){{\LightSalmon dix pas}}
\rput{0}(2.5,6.38516){{\red trente pas}}
\rput{0}(2.7,7.38906){solution exacte}
\rput{0}(0,1){$+$}
\rput{0}(1,0){$+$}
\rput{0}(-0.15,1){$1$}
\rput{0}(1,-0.4){$1$}
\rput{0}(-0.15,7.4){$y$}
\rput{0}(2,-0.4){$x$}
\endpspicture

\Figure Euler, Méthode d'Euler pour le problème de Cauchy $y'=xy$ et $y(0)=1$. 
\medskip\goodbreak

\Section ADL2, Equations du second ordre à coefficients constants.


\Subsection ADL13, Equation différentielle linéaire homogène $ay''+by'+cy=0$. 

\noindent
Dans cette section, $I\neq\emptyset$ est un intervalle  et  $a\neq0$, $b$ et $c$ sont des nombres de $\ob K$ . 
\bigskip
\noindent


\Propriete [linéarité] 
Soient $g_1$ et $g_2$ des solutions de l'équation différentielle linéaire 
$$
ag''(x)+bg'(x)+cg(x)=0\qquad(x\in I).\leqno{(H)}
$$ 
Alors, pour $(\lambda,\mu)\in\ob K^2$, la fonction $\lambda g_1+\mu g_2$ est une solution de l'équation $(H)$. 
\bigskip
 
\Definition []  Le polynôme caractéristique associé à l'équation différentielle $(H)$ est 
$$
P:=az^2+bz+c
$$
et l'équation caractéristique  associée à l'équation $(H)$ est l'équation  $P(z)=0$ . 
\bigskip

\Propriete { \bf ($\ob K=\ob C$)}. Soit $P$ le polynôme caractéristique associé à l'équation dif\-fé\-ren\-tiel\-le $H$ et soient $z_1$ et $z_2$ ses deux racines. 
\medskip
\noindent
Si $\Delta=0$, alors $z_1=z_2=:z$ et 
$$
\eqalign{
	g_1:I&\to\ob C\cr
	x&\mapsto\e^{zx}
}
\qquad\mbox{et}\qquad 
\eqalign{
	g_2:I&\to\ob C\cr
	x&\mapsto x\e^{zx}
	}
$$ 
Si $\Delta\neq0$, alors $z_1\neq z_2$ et 
$$
\eqalign{
	g_1:I&\to\ob C\cr
	x&\mapsto\e^{z_1x}
}
\qquad\mbox{et}\qquad 
\eqalign{
	g_2:I&\to\ob C\cr
	x&\mapsto\e^{z_2x}
	}
$$ 
 sont solutions de $(H)$.
\medskip\noindent
De plus, $g_1$ et $g_2$ sont linéairement indépendantes (elles ne sont pas proportionnelles). 
\bigskip


\Propriete [] { \bf (cas $\ob K=\ob R$)}. 
Soit $P$ le polynôme caractéristique rassocié à l'équation  dif\-fé\-ren\-tiel\-le   $H$   et  
soient  $z_1$  et  $z_2$  ses  deux  racines .  \medskip  \noindent Si  $\Delta<0$,  alors
$z_1=\overline{z_2}=:r+i\theta$            
                                                                                et
$$
\eqalign{
	g_1:I&\to\ob C\cr
	x&\mapsto\e^{rx}\cos(\theta x)
}
\qquad\mbox{et}\qquad 
\eqalign{
	g_2:I&\to\ob C\cr
	x&\mapsto\e^{rx}\sin(\theta x)
	}
$$ 
sont des solutions réelles de l'équation différentielle $(H)$.  \medskip

\noindent Si $\Delta=0$. Alors $z_1=z_2=:z$, les fonctions
$$
\eqalign{
	g_1:I&\to\ob C\cr
	x&\mapsto\e^{zx}
}\qquad\mbox{et}\qquad 
\eqalign{
	g_2:I&\to\ob C\cr
	x&\mapsto x\e^{zx}
	}
$$   sont des solutions réelles de l'équation différentielle $(H)$. 
\medskip

\noindent
Si $\Delta>0$, alors $z_1 \neq z_1$ et $(z_1,z_2)\in\ob R^2$. Les fonctions 
$$
\eqalign{
	g_1:I&\to\ob C\cr
	x&\mapsto\e^{z_1x}
}
\qquad\mbox{et}\qquad 
\eqalign{
	g_2:I&\to\ob C\cr
	x&\mapsto\e^{z_2x}
	}
$$ 
 sont des solutions réelles de l'équation différentielle $(H)$. 
\medskip\noindent
De plus, $g_1$ et $g_2$ sont linéairement indépendantes (elles ne sont pas proportionnelles). 
\bigskip


\Theoreme  Une fonction $f:I\to\ob C$ dérivable est une solution de l'équation $(H)$ si, et~seulement s'il existe $(\lambda_1,\lambda_2)\in\ob K^2$ tel que 
$$
\forall x\in I ,\qquad  g(x)=\lambda_1g_1(x)+\lambda_2g_2(x) .
$$
En d'autres mots, l'ensemble des solutions de l'équation différentielle linéaire $(H)$ est un plan vectoriel 
engendré par deux solutions $g_1$ et $g_2$ linéairement indépendantes. 
\bigskip


\centerline{\bf Résolution pratique de l'équation différentielle $(H)$ sur $I$}\medskip
\noindent\ 1) On résoud l'équation caractéristique $P(z)=0$ et on en déduit deux solutions linéairement indépendantes $g_1$ et $g_2$ de l'équation différentielle $(H)$ sur $I$. 
  
\smallskip
\noindent2) En utilisant que l'ensemble solution de $(H)$ est un plan vectoriel, conclure que 

\centerline{les solutions $g$ de $(H)$ sont les fonctions $g=\lambda_1 g_1+\lambda_2g_2$ pour $(\lambda_1,\lambda_2)\in\ob K^2$. } 

\bigskip
\noindent
Pour chaque $x_0\in I$ et chaque couple de nombres $(\alpha,\beta)\in\ob K^2$, le problème de Cauchy
$$
\Q\{\eqalign{
&ag''(x)+bg'(x)+cg(x)=0\qquad(x\in I),\cr
&g(x_0)=\alpha,\cr
&g'(x_0)=\beta
}\W.
$$
admet une unique solution $g:I\to\ob K$. 

\Subsection ADL13, Equation différentielle linéaire avec second membre $ay''+by'+cy=d(x)$. 

\noindent
Dans cette section, $I\neq\emptyset$ est un intervalle, $a\neq0$, $b$ et $c$ sont des nombres de $\ob K$ et
$d:I\to\ob K$ est une fonction continue.  \bigskip \noindent

\Propriete []  Etant données deux solutions $f$ et $f_0$ de l'équation différentielle 
$$
af''(x)+bf'(x)+cf(x)=d(x)\qquad(x\in I),\leqno{(E)}
$$
la~différence $g:=f-f_0$ est une solution de l'équation différentielle homogène associée 
$$
ag''(x)+bg'(x)+cg(x)=0\qquad(x\in I). \leqno{(H)}
$$ 
\medskip

\Theoreme 
La solution générale $f$ de l'équation $(E)$ est la somme d'une solution particulière $f_0$ de l'équation $(E)$ 
et de la solution générale $g$ de l'équation homogène $(H)$. 
$$
\forall x\in I, \qquad\underbrace{f(x)}_{\mbox{solution générale de }(E)}=
\underbrace{f_0(x)}_{\mbox{solution particulière de }(E)}+
\underbrace{\lambda_1g_1(x)+\lambda_2g_2(x)}_{\mbox{solution générale $g(x)$ de }(H)}.
$$


\noindent
\Propriete []  Pour chaque $x_0\in I$ et chaque couple $(\alpha,\beta)\in\ob K^2$, le problème de Cauchy
$$
\Q\{\eqalign{
	&af''(x)+bf'(x)+cf(x)=d(x)\qquad(x\in I),\cr
	&f(x_0)=\alpha,\cr
	&f'(x_0)=\beta
}\W.
$$
admet une unique solution $f:I\to\ob K$.


\Propriete [Title=Principe de superposition]
Si $d(x)=d_1(x)+d_2(x)$ pour $x\in I$ et si les~fonctions $f_1$ et $f_2$ vérifient sur l'intervalle $I$ les équations 
$$
\eqalign{
af_1''(x)+bf_1'(x)+cf_1(x)&=d_1(x)\qquad(x\in I),\cr
af_2''(x)+bf_2'(x)+cf_2(x)&=d_2(x)\qquad(x\in I),
}\eqdef{superposition}
$$ 
alors la fonction $f=f_1+f_2$ est solution de l'équation différentielle~$(E)$. 
\bigskip

\Propriete  
Si $a,b,c$ sont réels, alors 
$f:I\to\ob C$ vérifie l'équation~$(E)$~si, et~seulement~si, les fonctions réelles $f_1:x\mapsto\re\b(f(x)\b)$ et $f_2:x\mapsto\im\b(f(x)\b)$ sont solutions des équations différentielles~\eqref{superposition} 
pour les fonctions définies par 
$$
\forall x\in I,\qquad d_1(x):=\re\b(d_1(x)\b)\qquad\mbox{ et }\qquad d_2(x):=\im\b(d(x)\b).
$$ 

\Exemple.  Pour $\omega>0$, les fonctions $x\mapsto \e^{i\omega x}$ et $x\mapsto\e^{-i\omega x}$ sont des solutions de 
$$
g''(x)+\omega^2g(x)=0\qquad (x\in\ob R). 
$$ 
A fortiori, leur partie réelle $x\mapsto\cos(\omega x)$ et leur partie imaginaire $x\mapsto\pm\sin(\omega x)$ sont des solutions à valeurs réelles ce cette même équation. 

\noindent
Les solutions de l'équation $y""+y=0$ sur $\ob R$ à valeur complexes sont alors les fonctions 
$$
\eqalign{
&\mbox{(à valeurs complexes)}\qquad \forall x\in\ob R, \qquad f(x)=\lambda_1\e^{i\omega x}+\lambda_2\e^{-i\omega x}, \qquad(\lambda_1,\lambda_2)\in\ob C^2
\cr
&\mbox{(à valeurs réelles)}\qquad \forall x\in\ob R, \qquad f(x)=\mu_1\cos(\omega x)+\mu_2\sin(\omega x), \qquad(\mu_1,\mu_2)\in\ob R^2
}
$$\goodbreak



\Remarque{ \it 3 {\bf (Cas particulier au programme)}}. S'il existe un nombre $\alpha\in\ob K$ et un polynôme $Q$ de degré $n$ et à coefficients dans $\ob K$ tels que 
$$
d(x)=Q(x)\e^{\alpha x}\qquad(x\in I), \eqdef{polyexp}
$$
alors on peut trouver une solution particulière $f_0:I\to\ob K$ de l'équation $(E)$ de la forme 
$$
f_0(x)=R(x)\e^{\alpha x}\qquad(x\in I),
$$ 
où $R$ est un polynôme à coefficient dans $\ob K$ :
\smallskip
\Bullet de degré $n$ si $\alpha$ n'est pas racine du polynôme caractéristique $P$ de $(E)$, i.e. si $P(\alpha)\neq0$.
\smallskip
\Bullet de degré $n+1$ si $\alpha$ est une racine simple du polynôme $P$, i.e. si $P(a)=0$ et $\Delta\neq0$  
\smallskip
\Bullet de degré $n+2$ si $\alpha$ est une racine double du polynôme $P$, i.e si $P(a)=0$ et $\Delta=0$. 
\bigskip


\Remarque{ \it 4 {\bf (Cas général)}.} Si la fonction continue $d:I\to\ob K$ n'est pas une somme de fonctions du type \eqref{polyexp}, on peut tout de même résoudre l'équation différentielle $(E)$ 
en la divisant par $a\neq 0$ pour se ramener à l'étude de l'équation différentielle 
$$
f''(x)+\tilde a(x)f'(x)+\tilde b(x)f(x)=\tilde c(x)\qquad (x\in I) \leqno{(F)}
$$
pour les fonctions $\tilde a$, $\tilde b$ et $\tilde c$ définies par 
$$
\forall x\in I, \qquad \tilde a(x):={b\F a}, \qquad \tilde b(x):={c\F a}\quad\mbox{et}\quad \tilde c(x):={d(x)\F a}.
$$
et en utilisant la méthode de variation de la constante, en remarquant que $g_0:x\mapsto\e^{zx}$ est une solution, ne s'annulant pas sur $I$, 
de l'équation homogène 
$$
g''(x)+\tilde a(x)g'(x)+\tilde b(x)g(x)=0\qquad(x\in I),\leqno{(G)}
$$
si $z$ est une racine du polynôme caractéristique $P$ associé à $(H)$.  
\bigskip

\centerline{\bf Résolution de $(G)$ par la variation de la constante}
\medskip
\noindent\ Soient $\alpha$, $\beta$, $\gamma$ des fonctions continues de $I$ dans $\ob K$. 

\noindent
Si l'on dispose d'une solution $g_0$ de l'équation $(G)$, ne s'annulant pas sur $I$, 
la 

\noindent variation de la constante permet de résoudre l'équation différentielle $(F)$. 

\noindent
Pour cela, on cherche les solutions $f$ de l'équation $(F)$ sous la forme 
$$
f(x)=g_0(x)h(x)\qquad (x\in I)\eqdef{varconst}
$$
et on est alors amené à résoudre l'équation différentielle 
$$
g_0(x)h''(x)+2g_0'(x)h'(x)+\tilde a(x)h'(x)=\tilde c(x)\qquad (x\in I),
$$
qui se réduit à l'équation différentielle du premier ordre
$$
k'(x)+\alpha(x)k(x)=\beta(x)\qquad (x\in I)\eqdef{eqlinun}
$$
si l'on pose 
$$
\forall x\in I, \qquad k(x):=h'(x), \qquad\alpha(x):={2g_0'(x)+\tilde a(x)\F g_0(x)}\quad \mbox{et}\quad\beta(x):={\tilde c(x)\F g_0(x)}.
$$
\quad En résolvant successivement \eqref{eqlinun} puis $h'=k$ et en reportant dans \eqref{varconst}, 

\qquad on trouve toutes les solutions de l'équation $(F)$. 







                     

\hautspages{Olus Livius Bindus}{Nombres entiers et dénombrement}


\Chapter fonc, Nombres entiers et dénombrement. 
\bigskip

\Section group, Arithmétique dans $\ob Z$. 

\Subsection gah, Généralités.

\Concept [] Multiples et diviseurs

\Definition []  Un multiple $m$ d'un entier $n\in\ob Z$ est le produit de $n$ par un entier $k\in\ob Z$.
$$
m\mbox{ est un multiple de }n\in\ob Z\quad \Longleftrightarrow\quad \exists k\in\ob Z:\quad m=kn.
$$ 

\Definition []  Un entier $n$ est un diviseur d'un entier $m\in\ob Z$ si et seulement si $m$ est un multiple de l'entier $m$. 
$$
n\mbox{ est un diviseur de }m\in\ob Z\quad \Longleftrightarrow\quad \exists k\in\ob Z:\quad m=kn.
$$ 
Auquel cas, on note $n|m$. 
\bigskip

\Remarque : Pour $n\in\ob Z^*$ et $m\in\ob Z$, on a 
$$
m\mbox{ est un multiple de }n\quad \Longleftrightarrow\quad n\mbox{ est un diviseur de }m \Longleftrightarrow {m\F n}\in\ob Z
$$
\bigskip

\Remarque : L'ensemble des diviseurs de $0$ est $\ob Z$, l'entier $0$ est le seul multiple de $0$ et 
$$
m \mbox{ est un multiple (resp. diviseur) de }n\ \Longleftrightarrow\ -m \mbox{ est un multiple (resp. diviseur) de }n.
$$ 
Si $d$ est un diviseur d'un nombre $n\neq0$, on a forcément $1\le |d|\le |n|$. 
\bigskip

\Concept [] PGCD et PPCM

\Definition []  Le PGCD (plus grand diviseur commun) de deux entiers strictement positifs $n$ et $k$ est le plus grand nombre $d$ divisant $n$ et $k$. On le note 
$\mbox{PGCD}(n,k)$. 
$$
\mbox{PGCD}(n,k):=\max\{d\in\ob N:d|n\mbox{ et }d|k\}
$$ 

\Definition []  Le PPCM (plus petit commun multiple) de deux entiers strictements positifs $n$ et $k$ est le plus petit multiple positif de $n$ et de $k$. 
On le note $\mbox{PPCM}(m,n)$. 
$$
\mbox{PPCM}(n,k):=\min\{m\in\ob N:n|m\mbox{ et }k|m\}
$$ 

\Concept [] Nombres premiers entre eux. 

\Definition []  On dit que deux nombres $n$ et $k$ sont premiers entre eux, si et~seulement~si leur PGCD est $1$. 
\bigskip

\Theoreme [Title=Théorème de Gauss;$n$ et $k$ deux entiers strictements positifs]
Si $d|nk$ et si l'entier $d$ est premier avec $n$, alors on a $d|k$. 
\bigskip
  
\Propriete []  Si l'entier $k$ est premier avec $m$ et avec $n$, alors, l'entier $k$ est premier avec $mn$. 
\bigskip


\Concept [] Nombres premiers

\Definition []  On appelle nombre premier tout entier positif admettant exactement deux diviseurs positifs ($1$ et lui même).
\bigskip

\Propriete []  A l'exception de $2$, tous les nombres premiers sont impairs. 
\bigskip
 
\Propriete []  Il existe un nombre infini de nombres premiers. 
\bigskip

\Remarque : la liste des nombres premiers inférieurs à $100$ : 
$$
2,\ 3,\ 5,\ 7,\ 11,\ 13,\ 17,\ \!19,\ 23,\ 29,\ 31, \ 37, \ \!41,\ 43, \ 47,\ \!53, \ 59,\ 61,\ 67,\ 71,\ 73,\ 79,\ 83,\ 89,\ \!97
$$

\Subsection gah, Propriétés.

\Theoreme [$n\in\ob Z$ et $d\in\ob Z^*$;Title=Division euclidienne] 
Il existe un unique couple d'entier $q$ (le quotient) et $r$ (le reste) tels que 
$$
n=qd+r\qquad \mbox{et}\qquad 0\le r<|d|.
$$

\Remarque : on a alors 
$$
{n\F d}=q+{r\F d}, 
$$
où $q$ est la partie entière du nombre $n/d$ et $r/d$ est la partie fractionnaire de $n/d$. 
\bigskip

Exercice :  division euclidienne de $1224$ par $13$, de $-127$ par $11$ et de $-2359$ par $-3$ ? 
\bigskip

\Theoreme [Title=Décomposition en facteurs premiers] Pour chaque entier relatif $m\ge2$, il existe un entier $K\ge1$, 
des nombres premiers $p_1<p_2< \cdots< p_K$ et des nombres entiers strictement positifs $n_1,\cdots, n_K$ tels que 
$$
m=\prod_{k=1}^K p_k^{n_k}. 
$$
De plus, un tel vecteur $(K,p_1,\cdots, p_K, n_1,\cdots,n_K)$, réalisant la décomposition de $m$ en produit de facteurs premiers, est unique. 
\bigskip

\Propriete []  Soient $n$ et $k$ deux entiers strictements positifs. Alors, on a
$$
nk=\mbox{Pgcd}(n,k)\times \mbox{PPCM}(n,k)
$$ 

\Section En, Dénombrement. 

\Subsection Ef, Ensembles finis. 

\Definition []  Un ensemble $E$ est fini si, et seulement s'il existe un entier $n\ge1$ et une bijection $f:\{1,\cdots, n\}\to E$. 
Dans ce cas, cet entier $n$ est unique. On l'appelle cardinal (ou nombre d'éléments) de l'ensemble $E$ et on le note $\mbox{Card}(E)$. 
\bigskip

\Remarque : on convient que l'ensemble vide est fini et que $\mbox{Card}(\emptyset)=0$. 
\bigskip

\Propriete []  Soient $E$ et $F$ deux ensembles finis tels que $\Card(f)=\Card(E)$ et soit $f:E\to F$ une application. Alors, on a 
$$
f\mbox{ est injective}\quad\Longleftrightarrow\quad f\mbox{ est surjective}\quad\Longleftrightarrow\quad f\mbox{ est bijective}.
$$

\Propriete []  Soient $E$ et $F$ deux eensembles. Si $E$ est fini et s'il existe une bijection $f:E\to F$, alors l'ensemble $F$ est fini et on a 
$$
\Card(F)=\Card(E).
$$

\Propriete []  Soit $E$ un ensemble fini. Alors, tout sous-ensemble $F$ de $E$ est fini et on a 
$$
\Card(F)\le\Card(E)
$$
avec égalité si, et seulement si $F=E$.
\bigskip

\Propriete []  Une partie non vide $P$ de $\ob N$ est finie si, et suelement si ettle est majorée. 
\bigskip

 
\Propriete []  Si $P$ est une partie finie de $\ob N$, il existe une bijection strictement croissante $f:\{1,\cdots,\Card(P)\}\to P$ et une seule. 
\bigskip

\Subsection Ef, Opérations sur les ensembles finis, dénombrement. 

\Propriete []  Soient $E$ et $F$ des ensembles finis. Alors, 

\noindent
L'ensemble $\sc P(E)$ des parties de $E$ est un ensemble fini, de cardinal 
$$
\Card\b(\sc P(E)\b)=2^{\mbox{\sevenrm Card}(E)},
$$ 
L'ensemble $\sc Bij(E)$ des bijections $f:E\to E$ (des permutations de $E$) est un ensemble fini, de cardinal 
$$
\Card\b(\sc Bij(E)\b)=\Card(E)!
$$ 
Le produit $E\times F$ est un ensemble fini, de cardinal 
$$
\Card(E\times F)=\card(E)\times\Card(F)
$$ 
L'ensemble $F^E=\sc F(E,F)$ des applications $fE\to F$ est un ensemble fini, de cardinal 
$$
\Card\Q(F^E\W)=\Card(F)^{\mbox{\sevenrm Card}(E)}.
$$ 
Les ensembles $E\cup F$ et $E\cap F$ sont finis et vérifient 
$$
\Card(E\cup F)=\Card(E)+\card(F)-\Card(E\cap F). 
$$
En particulier, si $E\cup F$ est une réunion disjointe (c'est à dire si $E\cap F=\emptyset$), que l'on note (rarement ) $E\amalg F$, on a 
$$
\Card(E\amalg F)=\Card(E)+\Card(F). 
$$

\Remarque : Si $E_1, E_2,\cdots, E_n$ sont des sous-ensembles d'un ensemble $E$, disjoints deux à deux, on a 
$$
\bigsqcup_{1\le k\le n}E_k=\Card(E_1\cup\cdots\cup E_n)=\Card(E_1)+\cdots+\Card(E_n)=\sum_{1\le k\le n}\Card(E_k).
$$ 

\Propriete []  Soit $E$ un ensemble de cardinal $n$ et $F$ un ensemble de cardinal $p\le n$. Alors, il existe exactement 
$$
A_n^p:={n!\F (n-p)!}
$$
injections $f:E\to F$ (resp. arrangements de $p$ éléments choisis parmi $n$). 
\bigskip

\Propriete []  Soit $E$ un ensemble cardinal $n\in\ob N$. Alors, pour $0\le p\le n$, 
il existe exactement 
$$
\ds{n\choose p}=c_n^p:={n!\F p!(n-p)!}
$$ 
parties de $E$ ayant $p$ éléments (resp. combinaisons de $p$ éléments choisis parmi $n$). 
\bigskip

\Propriete []  Soient $(n,p)\in\ob N^2$ tels que $0\le p\le n$. Alors, on a 
$$
{n\choose p}={n\choose n-p}.
$$
Pour chaque entier $n\in\ob N$, on a 
$$
{n\choose 0}={n\choose n}:=1
$$
et, pour $0\le p<n$, on a 
$$
{n\choose p}+{n\choose p+1}={n+1\choose p+1}.\leqno{\mbox{Triangle de Pascal}}
$$

\Remarque : Si $E$ est un ensemble de cardinale $n$, on a 
$$
\underbrace{2^n}_{\mbox{\sevenrm Card}(\sc P(E))}=\sum_{0\le k\le n}\underbrace{{n\choose k}}_{\rlap{\mbox{\sevenrm nb de sous ensembles à $\ss p$ éléments de $\ss E$}}}.
$$













\hautspages{Olus Livius Bindus}{Structures algébriques}


\Chapter fonc, Structures algèbriques. 
\bigskip

\Section group, Groupes. 

\Subsection group1, Généralités. 


\Definition Un ensemble $G$ muni d'une opération {\bf interne} $\star$ forme un groupe si, et~seulement si, 
les cinq propriétés suivantes sont satisfaites.
\pn
i) L'ensemble $G$ n'est pas vide : 
$$
\exists a\in G.
$$ 
\noindent ii) L'opération $\star$ est interne à $G$ : 
$$
\forall (x,y)\in G^2, \qquad x\star y\mbox{ est défini}\quad\mbox{et}\quad x\star y\in G
$$
\noindent iii) L'opération $\star$ admet un élément neutre, noté $e$, dans $G$ : 
$$
\exists e\in G:\qquad \forall x\in G,\qquad e\star x=x\star e=e
$$
\noindent iv) Chaque élément de l'ensemble $G$ admet un inverse pour la loi $\star$ : 
$$
\forall x\in G, \qquad \exists y\in G:\qquad x\star y=y\star x=e
$$
\noindent v) L'opération $\star$ est associative dans $G$ :
$$
\forall (x,y,z)\in G^3, \qquad(x\star y)\star z=x\star(y\star z)
$$

\bigskip

\Definition Un groupe $(G,\star)$ est dit commutatif (ou abélien) si, et~seulement si, 
L'opération $\star$ est commutative dans $G$ : 
$$
\forall (x,y)\in G^2, \qquad x\star y=y\star x
$$
\bigskip

\Remarque : En ce qui concerne les groupes, on utilise essentiellement deux notations. 
La~loi~$\star$, l'élément $x\star y$, l'élément neutre, l'inverse de $x$ et $\underbrace{x\star x\star \cdots\star x}_{\mbox{\sevenrm $\ss n$ fois}}$ sont notés : 
\medskip
\noindent
Pour la notation additive : respectivement $+$, $x+y$, $0$, $-x$ et $nx$.
\medskip
\noindent
Pour la notation multiplicative (utilisée en particulier pour les groupes non-commutatifs) : respectivement $\times$, $xy$, $1$, $x^{-1}$ et $x^n$.
\bigskip

\Exemples.  $(\ob C,+)$, $(\ob C^*, \times)$, $\Q(\{f:I\to I: f\mbox{ bijection}\}, \circ\W)$, $(\ob R^n,+)$, $\Q(Gl_n(\ob R), \times\W)$... 
\bigskip


\Remarque : Dans un groupe $(G,\star)$, l'élément neutre $e$ est unique et l'inverse pour $\star$ de chaque vecteur $x$ est unique. 
\bigskip

\Subsection group2, Sous-groupes.

\Definition [] Un sous-groupe d'un groupe $(G,\star)$ est un groupe $(H, \otimes)$ inclus dans $G$ dont la loi
$\otimes$ est la restriction à $H$ de la loi $\otimes$ de~$G$.  \bigskip

\Remarque : pour simplifier, un sous-groupe $H$ de $G$ est un groupe plus petit que $H$ (au sens de l'inclusion), muni de la même opération (de la même structure). 
\bigskip

\Remarque : Prouver que $H$ est un groupe est beaucoup trop bourrin via les propriétés i) à v) alors que 
c'est sans douleur via les sous-groupes. 
\bigskip

\Propriete []  Soit $(G,\star)$ un groupe. Alors $(\{e\},\star)$ et $(G,\star)$ sont deux sous-groupes de $(G,\star)$. 
\bigskip

\Propriete []  Si $H$ est un sous-groupe de $(G,\star)$, alors $H$ contient l'élément neutre $e$ de $G$ pour la loi $\star$. 
\bigskip

\Propriete []  Un ensemble $H$ forme un sous-groupe d'un groupe $(G,\star)$ si, et~seulement~si
$H\neq\emptyset$, $H\subset G$ et si 
$$
{\forall (x,y)\in H^2,\qquad x\star y^{-1}\in H}, 
$$
c'est-à-dire si $H$ est stable par la loi $\star$ et par passage à l'inverse
$$
\Q\{
\eqalign{
&\forall (x,y)\in H^2, \qquad x\star y\in H
\cr
&\forall x\in H, \qquad x^{-1}\in H}\W.
$$

Exercice :  Prouver que $(\ob Z, +)$ et que $\Q(\{z\in\ob C:z^n=1\},\times\W)$ sont des groupes ($n\in\ob N^*$). 
\bigskip

\Subsection group3, Morphismes de groupes. 

Un morphisme des groupes $(G,\star)$ et $(H,\otimes)$ est une application $f:G\to H$ vérifiant 
$$
\forall (x,y)\in\ob G^2 , \qquad  f(x\star y)=f(x)\otimes f(y) .
$$ 

\Definition []  un morphisme de groupe $f:(G,\star)\to(H,\otimes)$ est un isomorphisme de groupes si, 
et seulement si $f:G\to H$ est bijective. 
\bigskip

\Remarque : Dans le cas précédent, on dit alors que les groupes $(G,\star)$ et $(H,\otimes)$ sont isomorphes. 
\bigskip

\Propriete []  Si $f:(G,\star)\to(H,\otimes)$ est un isomorphisme, alors, sa bijection réciproque $f^{-1}:(H,\otimes)\to(G,\star)$ 
en est également un. 
\bigskip

\Propriete []  Soit $f:(G,\star)\to (H,\otimes)$ un morphisme de groupes. Alors, l'ensemble 
$$
\Ker f:=\{x\in G:f(x)=e_H\}
$$
est un sous groupe de $(G,\star)$, appelé noyau du morphisme de groupes $f$. 
\bigskip

\Propriete []  Soit $f:(G,\star)\to (H,\otimes)$ un morphisme de groupes. Alors, l'ensemble 
$$
\IM f:=\{f(x):x\in G\}
$$
est un sous groupe de $(H,\otimes)$, appelé image du morphisme de groupes $f$. 
\bigskip

                             
\Section Ann, Anneaux (unitaires).

\Subsection Anneau1, Généralités. 

\Definition Un ensemble $A$ muni de deux opérations {\bf internes} $+$ et $\times$ forme un anneau (unitaire) si, et~seulement si, 
les dix propriétés suivantes sont satisfaites.
\pn
\noindent{i)} L'ensemble $A$ n'est pas vide : 
$$
\exists a\in A.
$$ 
\noindent{ii)} L'opération $+$ est interne à $A$ : 
$$
\forall (x,y)\in A^2, \qquad x+y\mbox{ est défini}\quad\mbox{et}\quad x+y\in A
$$
\noindent{iii)} L'opération $+$ admet un élément neutre, noté $0$, dans $A$ : 
$$
\exists 0\in A:\qquad \forall x\in E,\qquad 0+x=x+0=x
$$
\noindent{iv)} Chaque élément de l'ensemble $A$ admet un inverse (opposé) pour la loi $+$ : 
$$
\forall x\in A, \qquad \exists y\in A:\qquad x+y=0=y+x
$$
\noindent{v)} L'opération $+$ est associative dans $A$ :
$$
\forall (x,y,z)\in A^3, \qquad(x+y)+z=x+(y+z)
$$ 
\noindent{vi)} L'opération $+$ est commutative dans $A$ : 
$$
\forall (x,y)\in A^2, \qquad x+y=y+x
$$
\noindent{vii)} L'opération $\times$ est distributive sur l'opération $+$ dans $A$ : 
$$
\forall(x,y,z)\in A^3, \qquad x\times(y+z)=x\times y+x\times z\quad\mbox{et}\quad (y+z)\times x=y\times x+z\times x.
$$ 
\noindent{viii)} L'opération $\times$ est interne à $A$ : 
$$
\forall (x,y)\in A^2,\qquad x\times y\mbox{ est défini}\quad \mbox{et}\quad x\times y\in A
$$
\noindent{ix)} L'opération $\times$ admet un élément neutre non nul dans $A$, noté $1$ (Anneau unitaire):
$$
\exists 1\in A, \qquad\mbox{ différent de }0: \qquad \forall x\in A, \qquad x\times1=1\times x=x.
$$ 
\noindent{x)} L'opération $\times$ est associative dans $A$
$$
\forall (x,y,z)\in A^3, \qquad(x\times y)\times z=x\times(y\times z)
$$ 

\bigskip

\noindent
\Definition []  Un anneau $(A,+,\times)$ est commutatif si, et seulement si, 
\bigskip
\noindent xi) L'opération $\times$ est commutative dans $A$ : 
$$
\forall (x,y)\in A^2, \qquad x\times y=y\times x
$$
\bigskip

                             
\Exemples.  $(\ob C,+,\times)$, $(\ob R,+,\times)$, $(\ob Q, +,\times)$, $\Q(\sc F(I,\ob R),+,\times\W)$, $\Q(\sc M_n(\ob R),+,\times\W)$, $\sc L(E,+,\circ )$... 
\bigskip

\Remarque : Si $(A,+,\times)$ est un anneau, alors $(A,+)$ est un groupe commutatif. A fortiori, l'élément neutre $0$ est unique, de même que 
l'inverse pour $\times$ (l'opposé) de chaque vecteur~$x$, qui est noté $-x$. L'élément neutre $1$ pour la loi $\times$ est également unique. 
\bigskip

\Subsection Anneau2, Sous-anneau. 

\Definition [] Un sous-anneau d'un anneau $(A,+,\times)$ est un anneau $(B, \star, \otimes)$ inclus dans $A$ dont les lois
$\star$ et $\otimes$ sont les restrictions à $B$ des lois $+$ et $\times$ de~$A$.  \bigskip

\Remarque : pour simplifier, un sous-anneau $B$ de $A$ est un anneau plus petit que $A$ (au sens de l'inclusion), muni des mêmes opérations (de la même structure). 
\bigskip

\Remarque : Prouver que $A$ est un anneau est beaucoup trop bourrin via les propriétés i) à x) alors que 
c'est sans douleur via les sous-anneaux. 
\bigskip

\Propriete []  Si $B$ est un sous-anneau de $(A,+,\times)$, alors $B$ contient les éléments neutres $0$ et $1$ de $A$ pour les lois $+$ et $\times$. 
\bigskip

\Propriete []  Un ensemble $B$ forme un sous-anneau d'un anneau $(A,+,\times)$ si, et~seulement~si
$B\neq\emptyset$, $B\subset A$ et si 
$$
\forall (x,y)\in B^2,\qquad x-y\in B\quad\mbox{et}\quad x\times y\in B, 
$$
c'est-à-dire si $B$ est stable pour les lois $\times$ et $+$ ainsi que par passage à l'opposé
$$
\Q\{
\eqalign{
&\forall (x,y)\in B^2, \qquad x\times y\in B
\cr
&\forall (x,y)\in B^2, \qquad x+y\in B
\cr
&\forall x\in B, \qquad -x\in B
}\W.
$$

Exercice :  Prouver que $(\ob Z, +,\times)$ est un anneau.     
\bigskip

\Subsection Anneau3, Propriétés. 

\Concept [] Identité algèbrique

\Theoreme [Title=Egalité fondamentale de l'algèbre;$a$ et $b$ éléments d'un anneau $(A,+,\times)$  vérifiant {$ab=ba$}] 
$$
\forall n\in\ob N, \qquad a^n-b^n=(a-b)\sum_{k=0}^{n-1}a^kb^{n-1-k}.
$$

\Remarque : cette égalité est vraie pour $n=0$ avec la convention selon laquelle $a^0=b^0=1$ et selon laquelle une somme vide est nulle. 
\bigskip


\Concept [] Binôme de Newton

\Theoreme [Title=Binôme de Newton;$a$ et $b$ éléments d'un anneau $(A,+,\times)$  vérifiant {$ab=ba$}]
$$
\forall n\in\ob N,\qquad (a+b)^n=\sum_{0\le k\le n}{n\choose k}a^kb^{n-k}.
$$

\Remarque : cette égalité est vraie pour $n=0$ avec la convention selon laquelle $a^0=b^0=1$ et selon laquelle une somme vide est nulle. 
\bigskip

\Concept [] Distributivité et sommes

Soient $A$ un anneau, $n\in\ob N^*$, $a\in A$ et $(b_1,\cdots, b_n)\in A^n$. alors, on a 
$$
\eqalign{
&a\sum_{1\le k\le n}b_k=a(b_1+b_2+\cdots+b_n)=(ab_1+ab_2+\cdots+ab_n)=\sum_{1\le k\le n}ab_k
\cr
&\Q(\sum_{1\le k\le n}b_k\W)a=(b_1+b_2+\cdots+b_n)a=(b_1a+b_2a+\cdots+b_na)=\sum_{1\le k\le n}b_ka
}
$$

\Concept [] Double distributivité

Soient $A$ un anneau, $(m,n)$ un couple d'entiers strictements positifs, $(a_1,\cdots, a_m)\in A^m$ et $(b_1,\cdots, b_n)\in A^n$. alors, on a 
$$
\Q(\sum_{1\le k\le m}a_k\W)\times\Q(\underbrace{\sum_{1\le \ell\le n}b_\ell}_c\W)=\sum_{1\le k\le
m}\Q(a_k\underbrace{\sum_{1\le \ell\le n}b_\ell}_c\W) =\sum_{1\le k\le m}\Q(\sum_{1\le \ell\le n}a_kb_\ell\W) $$ On a
également $$ \Q(\underbrace{\sum_{1\le k\le m}a_k}_d\W)\times\Q(\sum_{1\le \ell\le n}b_\ell\W)=\sum_{1\le \ell\le
n}\Q(\Q(\underbrace{\sum_{1\le k\le m}a_k}_d\W)b_\ell\W) =\sum_{1\le \ell\le n}\Q(\sum_{1\le k\le m}a_kb_\ell\W) $$


\Propriete []  Soient $A$ un anneau, $(m,n)$ un couple d'entiers strictements positifs, $(a_1,\cdots, a_m)\in A^m$ et $(b_1,\cdots, b_n)\in A^n$. alors, on a 
$$
\sum_{1\le k\le m}\Q(\sum_{1\le \ell\le n}a_kb_\ell\W)=\sum_{1\le \ell\le n}\Q(\sum_{1\le k\le m}a_kb_\ell\W)
$$

\Remarque : cette propriété est également une conséquence de la propriété, plus g'enérale, de Fubini. 
\bigskip

\Propriete [Index=Theoreme@Théorème!de Fubini;Title=Théorème de Fubini (triangle)] soit $(G,+)$ un groupe commutatif. Soient $(n,p)$ un couple d'entiers positifs et soient $(a_{k,\ell})$ une famille d'éléments de $G$ avec $1\le k\le m$ et $1\le \ell\le n$. 
Alors, 
$$
\sum_{\ss1\le k\le m\atop\ss1\le\ell\le n}a_{k,\ell}:=\sum_{k=1}^m\ \sum_{\ell=1}^na_{k,\ell}=\sum_{\ell=1}^n\ \sum_{k=1}^ma_{k,\ell}
$$

                                             
\Propriete [Title=Théorème de Fubini]
soit $(G,+)$ un groupe commutatif, 
soient $(n,p)$ un couple d'entiers positifs et soient $(a_{k,\ell})$ une famille d'éléments de $G$. 
Alors, 
$$
	\sum_{k=1}^n\sum_{\ell=1}^ka_{k,\ell}=\sum_{\ell=1}^n\sum_{k=\ell}^na_{k,\ell}.
$$

\Section K, Corps.

\Subsection L, Définition. 

\Definition Un ensemble $\ob K$ muni de deux opérations $+$ et $\times$ 
forme un corps si, et~seulement si, les onze propriétés suivantes sont satisfaites.
\bigskip
\noindent\qquad\llap{i)}\ L'ensemble $\ob K$ n'est pas vide : 
$$
\exists a\in \ob K.
$$ 
\noindent\qquad\llap{ii)}\ L'opération $+$ est interne à $\ob K$ : 
$$
\forall (a,b)\in \ob K^2, \qquad a+b\mbox{ est défini}\quad\mbox{et}\quad a+b\in \ob K
$$
\noindent\qquad\llap{iii)}\ L'opération $+$ admet un élément neutre, noté $0$, dans $\ob K$ : 
$$
\exists 0\in \ob K:\qquad \forall a\in \ob K,\qquad 0+a=a+0=a
$$
\noindent\qquad\llap{iv)}\ Chaque élément de l'ensemble $\ob K$ admet un inverse pour la loi $+$ : 
$$
\forall a\in \ob K, \qquad \exists b\in \ob K:\qquad a+b=0=b+a
$$
\noindent\qquad\llap{v)}\ L'opération $+$ est associative dans $\ob K$ :
$$
\forall (a,b,c)\in \ob K^3, \qquad(a+b)+c=a+(b+c)
$$ 
\noindent\qquad\llap{vi)}\ L'opération $+$ est commutative dans $\ob K$ : 
$$
\forall (a,b)\in \ob K^2, \qquad a+b=b+a
$$
\noindent\qquad\llap{vii)}\ L'opération $\times$ est distributive sur l'opération $+$ dans $\ob K$ : 
$$
\forall(a,b,c)\in \ob K^3, \qquad a\times(b+c)=a\times b+a\times c\quad\mbox{et}\quad (b+c)\times a=b\times a+c\times a.
$$ 
\noindent\qquad\llap{viii)}\ L'opération $\times$ est interne à $\ob K$ : 
$$
\forall (a,b)\in \ob K^2,\qquad a\times b\mbox{ est défini}\quad \mbox{et}\quad a\times b\in \ob K
$$
\noindent\qquad\llap{ix)}\ L'opération $\times$ admet un élément neutre non nul dans $\ob K$, noté $1$ :
$$
\exists 1\in \ob K, \qquad\mbox{ différent de }0: \qquad \forall a\in \ob K, \qquad a\times1=1\times a=a.
$$ 
\noindent\qquad\llap{x)}\ Chaque élément non nul de $\ob K$ est inversible pour $\times$ : 
$$
\forall a\in \ob K, \qquad\mbox{différent de }0, \qquad \exists b\in \ob K:\qquad a\times b=b\times a=1. 
$$
\noindent\qquad\llap{xi)}\ L'opération $\times$ est associative dans $\ob K$
$$
\forall (a,b,c)\in \ob K^3, \qquad(a\times b)\times c=a\times(b\times c)
$$


\Definition []  Un corps $(\ob K,+,\times)$ est commutatif si, et seulement si, 
\noindent\qquad\llap{xii)}\ L'opération $\times$ est commutative dans $\ob K$ : 
$$
\forall (a,b)\in \ob K^2, \qquad a\times b=b\times a
$$

\Exemples. $(\ob C,+,\times)$, $(\ob R,+,\times)$ et $(\ob Q, +,\times)$ sont des corps commutatifs. 
\bigskip


\Remarque : Un corps est un anneau dans lequel tout élément non nul admet un inverse. 
\bigskip

                                                                                                  
\Subsection M, Sous-corps. 


\Definition []  Un sous-corps d'un corps $(\ob K,+,\times)$ est un corps $(L, \star, \otimes)$ inclus dans $\ob K$ 
dont les lois $\star$ et $\otimes$ sont les restrictions à $L$ des lois $+$ et $\times$ de $\ob K$. 
\bigskip

\Remarque : pour simplifier, un sous-corps $L$ de $\ob K$ est un corps plus petit que $\ob K$ (au sens de l'inclusion), 
muni des mêmes opérations (de la même structure). 
\bigskip

\Remarque : Prouver que $L$ est un corps est beaucoup trop bourrin via les propriétés i) à xi) alors que 
c'est sans douleur via les sous-corps. 
\bigskip

\Propriete []  Si $L$ est un sous-corps de $(\ob K,+,\times)$, alors $L$ contient les éléments neutres $0$ et $1$ de $\ob K$ 
pour les lois $+$ et $\times$. 
\bigskip

\Propriete []  Un ensemble $L$ forme un sous-corps d'un corps $(\ob K,+,\times)$ si, et~seulement~si
$L\neq\emptyset$, $L\subset \ob K$ et si $L$ est stable pour les lois $+$ et $\times$. 
$$
\eqalign{
	&\forall (x,y)\in B^2, \qquad x+y\in B
	\cr
	&\forall (x,y)\in B^2, \qquad x\times y\in B
}
$$

 
Exercice :  prouver que $Q[i]:=\{a+ib:(a,b)\in\ob Q^2\}$ est un sous corps de $(\ob C,+,\times)$. 
\bigskip

\Remarque : Un sous-anneau d'un corps $(\ob K, +,\times)$ est un corps. 
\bigskip

\Section Alg, Algèbres.
\bigskip

       
\Definition [] Un ensemble $A$ muni de deux opérations internes $+$ et $\times$ et d'une opération externe $\cdot$ 
sur un corps $\ob K$ forme une algèbre $(A,+,\cdot,\times)$ si, et~seulement~si, 
\medskip
\noindent\qquad\llap{i)}\ L'ensemble $A$ muni des lois $+$ et $\times$ forme un anneau $(A,+,\times)$.
\medskip
\noindent\qquad\llap{ii)}\ L'ensemble $A$ muni des lois $+$ et $\cdot$ forme un $\ob K$-espace vectoriel $(A,+,\cdot)$. 
\medskip
\noindent\qquad\llap{iii)}\ Mobilité du scalaire : 
$$
\forall \lambda\in\ob K, \quad\forall (x,y)\in A^2,\qquad \lambda.(x\times y)=(\lambda.x)\times y=x\times(\lambda.y).
$$


\Exemple. $\sc L(E,+,\cdot,\times)$ et $\ob K[X]$, l'ensemble des polynômes � coefficients dans $\ob K$, 
sont des algèbres sur le corps $\ob K$. 
\bigskip

                
         




\hautspages{Olus Livius Bindus}{Polynômes}


\Chapter fonc, Polynômes. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip


\Section poly, Polynômes à une indeterminée. 

\Subsection gah, Généralités.

\Concept [] Espace vectoriel des polynômes $\ob K[X]$

\Definition []  On appelle polynôme à coefficients dans $\ob K$ toute suite $P=(a_n)_{n\in\ob N}$ d'éléments du corps $\ob K$ telle que l'ensemble 
$\{n\in\ob N:u_n\neq0\}$ soit fini.
\bigskip

\Propriete []  L'ensemble $\ob K[X]$ des polynômes (d'indeterminée $X$) à coefficients dans $\ob K$, muni de l'addition $+$ et de la multiplication externe $.$ des suites, est un sous espace vectoriel de $(\ob K^{\ob N},+,.)$. 
\bigskip

\Definition []  Pour chaque entier $n$, on note $X^n$ le polynôme 
$$
X^n:=\{0,\cdots,0,\vtop{\mbox{$1$}\mbox{$n$}},0,\cdots,0,\cdots\}
$$
et l'on convient que $X^0=1$. 
\bigskip

\Propriete []  La famille $\{X^n\}_{n\in\ob N}$ est une base de l'espace vectoriel $\ob K[X]$ et pour tout polynôme $P=(a_k)_{k\in\ob N}$ de $\ob K[X]$, on a 
$$
P=\sum_{k=0}^\infty a_kX^k
$$

\Remarque : on rappelle que la suite $(a_k)_{k\in\ob N}$ et la somme précédentes ne contiennent qu'un nombre fini de termes non nuls. 
Ainsi, pour chaque polynôme $P$, il existe un entier $N\ge0$ et des nombres $(a_0,\cdots,a_N)\in\ob K^{N+1}$ (uniques) tels que 
$$
P=\sum_{k=0}^Na_kX^k.
$$

\Concept [] Algèbre commutative des polynômes $\ob K[X]$ 


\Definition []  Pour chaque polynôme $P=\sum_{k=0}^\infty a_kX^k$ de $\ob K[X]$ et chaque scalaire $\lambda\in\ob K$, le polynôme $\lambda.P$ est défini par 
$$
\lambda.P=\sum_{k=0}^\infty(\lambda a_k)X^k.
$$
Etant donnés $P=\sum_{k=0}^\infty a_kX^k$ et $Q=\sum_{k=0}^\infty b_kX_k$ dans $\ob K[X]$, 
le polynôme $P+Q$ est défini par 
$$
P+Q:=\sum_{k=0}^\infty(a_k+b_k)X^k
$$
et le polynôme $P\times Q$ est défini par
$$
P\times Q:=\sum_{k=0}^\infty c_kX^k\qquad \mbox{avec}\qquad \forall k\in\ob N, \quad c_k=\sum_{m+n=k}a_mb_n=\sum_{\ell=0}^ka_\ell b_{k-\ell}.
$$

\Theoreme []  L'ensemble $(\ob K[X],+,\cdot,\times)$ forme une $\ob K$-algèbre commutative. 

\Concept [] Degré d'un polynôme


\Definition []  Le degré d'un polynôme $P=(a_n)_{n\in\ob N}$ non nul de $\ob K[X]$ est le nombre
$$
\deg(P):=\sup\{n\in\ob N:a_n\neq0\}.
$$
Par convention, le polynôme nul est noté $0$ et son degré est $\deg(0):=-\infty$. 
\bigskip

\Definition []  Soit $P=\sum_{k=0}^\infty$ un polynôme non nul de degré $D=\deg(P)$. Alors, on a 
$$
P=\sum_{k=0}^Da_kX_k.
$$
Le nombre $a_D$ est appelé coefficient du monôme dominant $a_DX^D$ du polynôme $P$. 
\bigskip

\Definition []  On dit qu'un polynôme $P$ non nul est unitaire (ou normalisé) si, et~seulement~si le coefficient de son monôme dominant est égal à $1$. 
\bigskip

\Exemple. $X^3+2X+7$ et $(X+1)^{1337}$ sont des polynômes unitaires. Le polynôme $7X^3+1$ n'en est pas un, car son monôme dominant est $7X^3$. 
\bigskip

 
\Propriete []  Soient $P$ et $Q$ deux polynômes non nuls de $\ob K[X]$. Alors, on a 
$$
\deg(PQ)=\deg(P)+\deg(Q).
$$
Pour chaque $\lambda\in\ob K^*$, on a 
$$
\deg(\lambda P)=\deg(P).
$$
Si $P+Q\neq0$, on a 
$$
\deg(P+Q)\le\max\b\{\deg(P),\deg(Q)\b\}.
$$
Si de plus, $\deg(P)\neq\deg(Q)$, on a 
$$
\deg(P+Q)=\max\{\deg(P),\deg(Q)\}.
$$

\Concept [] Espace vectoriel des polynômes $\ob K_n[X]$ 


\Definition []  Pour chaque entier $n\ge0$, on note $\ob K_n[X]$ le sous-espace vectoriel de $\ob K[X]$ constitué par les polynômes de degré inférieur à $n$. 
$$
\ob K_n[X]:=\Vect_{\ob K}(1,X,X^2,\cdots,X^n).
$$

\Propriete []  Pour chaque entier $n\ge0$, on a $\dim\ob K_n[X]=n+1$. 
\bigskip

\Propriete []  Si la famille $(P_k)_{0\le k\le n}$ de polynômes de $\ob K[X]$ est échelonnée en degré (i.e. vérifiant $\deg(P_k)=k$ pour $0\le k\le n$), alors 
c'est une base de $\ob K_n[X]$. 

\Concept [] Dérivation


\Definition []  Pour chaque polynôme $P=\sum_{k=0}^\infty a_kX^k$ de $\ob K[X]$, on définit le polynôme dérivé $P'$ de $\ob K[X]$ par 
$$
P':=\sum_{k=1}^\infty ka_kX_{k-1}=\sum_{n=0}^\infty(n+1)a_{n+1}X^n.
$$

\Propriete []  Si $P$ est un polynôme de degré strictement positif, alors
$$
\deg(P')=\deg(P)-1.
$$
Si $P$ est de degré inférieur ou égal à $0$, alors, on a 
$$
\deg(P')=0.
$$

\Propriete []  L'application $P\mapsto P'$ est un endomorphisme surjectif de $\ob K[X]$, non-injectif. En particulier, on a 
$$
\eqalign{
\forall(\lambda,\mu)\in\ob K^2,\qquad \forall(P,Q)\in\ob K[X]^2,& \qquad (\lambda P+\mu Q)'=\lambda P'+\mu Q'
\cr
\forall(P,Q)\in\ob K[X]^2,& \qquad(PQ)'=P'Q+PQ'.
}
$$

\Theoreme [Title=Formule de Leibniz pour les polynômes;$P$ et $Q$ deux polynômes à coefficients dans $\ob K$]
$$
\forall n\ge0, \qquad (PQ)^{(n)}=\sum_{0\le k\le n}{n\choose k}P^{(k)}Q^{(n-k)}.
$$

\Section Fpol, Fonctions polynômiales. 

\Concept [] Fonctions polynômiales

\Definition []  Soit $A$ un anneau contenant $\ob K$. 
La fonction polynômiale associée sur $A$ au polynôme $P=\sum_{k=0}^Na_kX^k$ à coefficients dans $\ob K$ est l'application
$$
\eqalign{\tilde P: A&\to A\cr x&\mapsto \ds \sum_{0\le k\le N}a_kx^k}.
$$ 

\Remarque : En ce qui nous concerne, les fonctions polynômiales étudiées le seront quasiment toujours pour $A=\ob K$. 
\bigskip


\Remarque : bien que les expressions de $P$ et de $\tilde P$ soient semblables, ce sont des objets fondamentalements distincts : le polynôme $P$ est une suite de coefficients avec lesquels on fait du calcul formel alors que $\tilde P$ est une application, dont les propriétés sont très fortement liées à l'anneau $A$ : par exemple, on pourra considérer des polynômes de matrices pour $A=\sc M_n(\ob K)$, des polynômes d'endomorphismes pour $A=\sc L(E)$, des polynômes d'applications pour $A=\sc F(I,\ob K)$ ou des fonctions polynômes pour $A=\ob K$. 
\bigskip

\Propriete []  Soit $\sc Fp$ l'ensemble des fonctions polynômes sur $\ob K$. Alors, l'application 
$$
\eqalign{ (\ob K[X],+,\cdot,\times)&\to{(\sc Fp,+,\cdot,\times)}\cr  P&\mapsto \tilde P}
$$
est un isomorphisme d'espace vectoriels et d'anneaux (i.e. d'algèbres). 
\bigskip

\Remarque : la fonction polynômiale $\tilde P$ sera, par abus de notation, très souvent notée $P$. A vous de faire la différence 
\bigskip
 
\Concept [] Substitution

\Propriete []  Soit $A$ un anneau contenant $\ob K$ et soit $\alpha\in A$. Alors, l'application 
$$
\eqalign{ (\ob K[X],+,\cdot,\times)&\to(A,+,\cdot,\times)\cr  P&\mapsto\tilde P(\alpha)}
$$
est un morphisme d'espace vectoriels et d'anneaux (i.e d'algèbres). 
\bigskip

\Remarque : En particulier, quand on applique ce morphise d'anneau pour $\alpha\in A$ à un polynôme $P$, on dit qu'on substitue $\alpha$ à l'indeterminée $X$. Comme c'est un morphisme d'espace vectoriel et d'anneau (d'algèbre), cette opération conserve les égalités polynômiales. Ainsi, on a 
$$
P=Q\quad \Longrightarrow\quad\forall\alpha\in A, \qquad \tilde P(\alpha)=\tilde Q(\alpha).
$$

\Propriete []  Pour $\alpha\in A$, anneau contenant $\ob K$, l'application 
$$
\eqalign{ (\ob K[X],+,\cdot,\times)&\to(A,+,\cdot,\times)\cr P&\mapsto \tilde P(\alpha)}
$$
est un morphisme d'espace vectoriels (et d'anneaux). 
\bigskip

\Concept [] Zéros d'un polynôme. 

\Definition : l'équation algèbrique (polynômiale) associée à un polynôme $P=\sum_{k=0}^Na_kX^k$ est l'équation $\tilde P(x)=0$, c'est à dire 
$$
\sum_{0\le k\le N}a_kx^k=0,\eqdef{gah}
$$
d'inconnue $x$ variant dans un anneau $A$ contenant $\ob K$. 
\bigskip

\Definition []  Un zéro (ou une racine) d'un polynôme $P=\sum_{k=0}^Na_kX^k$ à coefficients dans~$\ob K$ est un nombre $\alpha\in\ob K$ tel que 
$P(\alpha)=0$, i. e. une solution de l'équation algèbrique \eqref{gah}. 
\bigskip

\Definition []  Soit $P\in\ob K[X]$ un polynôme et $\alpha\in\ob K$ l'une de ses racines. Alors, la~multiplicité de $\alpha$ en temps que racine de $P$ est le plus grand entier $n\ge1$ pour lequel il~existe $R\in\ob K[X]$ tel que 
$$
P=(X-\alpha)^n R.
$$
C'est aussi le plus grand entier $n\ge1$ pour lequel on a 
$$
\Q\{
\eqalign{
P(\alpha)=0, 
\cr
P'(\alpha)=0, 
\cr
\quad\vdots\quad
\cr
P^{(n-1)}(\alpha)=0
}
\W.
$$

\Section Pol, Arithmétique et polynômes.
 
\Concept [] Multiples et diviseurs

\Definition []  Un multiple $P$ d'un polynôme $Q\in\ob K[X]$ est le produit de $Q$ par un poylnôme $R\in\ob K[X]$.
$$
P\mbox{ est un multiple de }Q\quad \Longleftrightarrow\quad \exists R\in\ob K[X]:\quad P=QR.
$$ 

\Definition []  Un polynôme $Q$ est un diviseur d'un polynôme $P\in\ob K[X]$ si et seulement si $P$ est un multiple du polynôme $Q$. 
$$
Q\mbox{ est un diviseur de }P\quad \Longleftrightarrow\quad \exists R\in\ob K[X]:\quad P=QR.
$$ 
Auquel cas, on note $Q|P$. 
\bigskip

\Remarque : L'ensemble des diviseurs du polynôme $0$ est $\ob K[X]$, le polynôme $0$ est le seul multiple de $0$ et 
$P$ est un multiple (resp. diviseur) de $Q$ si, et~seulement~si
$$
\forall\lambda\in\ob K^*, \quad \lambda P \mbox{ est un multiple (resp. diviseur) de }Q.
$$ 
Si $Q$ est un diviseur d'un polynôme $P\neq0$, on a forcément $0\le\deg(Q)\le\deg(P)$. 
\bigskip

\Concept [] Division euclidienne

\Theoreme [Title=Division euclidienne;$P$ et $D\neq0$ polynômes à coefficients dans $\ob K$]
Il existe un unique couple de polynômes $Q$ (le quotient) et $R$ (le reste) tels que 
$$
P=QD+R\qquad \mbox{et}\qquad\deg(R)<\deg(D).
$$

\Propriete []  Pour $\alpha\in\ob K$, le reste de la division euclidienne d'un polynôme $P\in\ob K[X]$ par le polynôme $X-\alpha$ est la constante $\tilde P(\alpha)$. 
\bigskip

\Propriete []  Un polynôme $P\in\ob K[X]$ est divisible par $X-\alpha$ si, et~seulement~si le nombre $\alpha$ est une racine de $P$. 
\bigskip

\Concept [] PGCD et PPCM

\Definition []  Le PGCD (plus grand diviseur commun) de deux polynômes $P$ et~$Q$ non nuls est le polynôme unitaire, 
divisant $P$ et $Q$, de plus haut degré. On le note $\mbox{PGCD}(P,Q)$. 
\bigskip

\Definition []  Le PPCM (plus petit commun multiple) de deux polynômes non nuls $P$ et~$Q$ est le polynôme unitaire, multiple 
de $P$ et $Q$, de plus petit degré. On le note $\mbox{PPCM}(P,Q)$. 
\bigskip

\Remarque { \it(Algorythme d'Euclie)} : pour calculer le PGCD de deux polynômes, on peut utiliser l'algorythme suivant : 
Si $Q$ et $R$ sont le quotient et le reste de la division euclidienne du polynome $P$ par le polynôme $D$, on a 
$$
P=QD+R\qquad\mbox{avec}\qquad\deg(R)< \deg(D)
$$
et en particulier, on en déduit que 
$$
\mbox{PGCD}(P,D)=\mbox{PGCD}(D,R),
$$
le PGCD de droite étant plus facile à calculer puisque le polynôme $R$ est de plus petit degré que le polynôme $P$. 
\bigskip

\Concept [] Polynômes premiers entre eux. 

\Definition []  On dit que deux polynômes $P$ et $Q$ sont premiers entre eux, si et~seulement~si leur PGCD est $1$. 
\bigskip

\Theoreme [Title=Théorème de Gauss;$P$ et $Q$ deux polynômes non nul] 
Si $D$ divise $PQ$ et si le polynôme $D$ est premier avec $P$, alors $D$ divise $Q$. 
\bigskip
  
\Propriete []  Si le polynôme $P$ est premier avec $Q$ et avec $R$, alors, le polynôme $P$ est premier avec $QR$. 
\bigskip

\Concept [] Polynômes irreductibles

\Definition []  Un polynôme non nul $P\in\ob K[X]$ est dit irreductible dans le corps $\ob L$ si ses seuls diviseurs $Q\in L[X]$ sont de degré $0$ 
ou de degré $\deg P$. Dans le cas contraire, on dit que le polynôme $P$ est réductible. 
\bigskip

\Remarque : Un polynôme $P$ de $\ob K[X]$ est réductible dans $\ob L$ si, et~seulement~si 
$$
\exists(Q,R)\in\ob K[X]^2, \mbox{ avec }\deg(Q)\ge1\mbox{ et } \deg R\ge1\mbox{ tel que }\quad P=QR.
$$
\medskip

\Subsection gah, Polynômes scindés.

\Concept [] Théorème fondamental

\Theoreme [Index=Theoreme@Théorème!de d'Alembert;Title=Théorème de d'Alembert-Gauss]  
Tout polynôme non constant admet au moins une racine dans $\ob C$. 

\Propriete []  Les polynômes irréductibles dans $\ob C$ (resp. dans $\ob R$) sont les polynômes de degré $0$ et $1$ (et les polynômes de degré $2$ de discriminant $\Delta<0$). 
\bigskip

\Theoreme [Title=Décomposition en facteurs irréductibles;$P$ polynôme non constant à coefficients dans $\ob K$ ] 
Il existe un nombre $\alpha\in\ob K^*$, un entier $K\ge1$, des polynômes unitaires $P_1,P_2,\cdots, P_K$ irréductibles sur $\ob K$ et des nombres entiers strictement positifs $n_1,\cdots, n_K$ tels que 
$$
P=\alpha\prod_{k=1}^K P_k^{n_k}. 
$$
De plus, un tel vecteur $(\alpha, K,P_1,\cdots, P_K, n_1,\cdots,n_K)$, réalisant la décomposition de $P$ en produit de facteurs irreductibles, est unique. 

\Remarque : Décomposer les polynômes non nuls $P$ et $Q$ en produits de facteurs irréductibles permet de calculer leur PGCD (comme pour les nombres entiers). 
On en déduit leur $PPCM$ car il existe une constante $\lambda\in\ob K^*$ telle que 
$$
PQ=\mbox{PGCD}(P,Q)\mbox{PPCM}(P,Q).
$$

\Definition []  On dit qu'un polynôme $P\in\ob K[X]$ est scindé sur le corps $\ob K$ si, et~seulement~si on peut l'écrire comme un produit de polynômes de $\ob K[X]$ de degré $1$, c'est à dire si toutes ses racines appartiennent au corps $\ob K$. 
\bigskip

\Propriete []  Tous les polynômes de $\ob C[X]$ sont scindés sur $\ob C$. Soit $P\in\ob C[X]$, un polynôme de degré $n$ et de coefficient dominant $\alpha$, le polynôme $P$ admet des racines $z_1,\cdots, z_p$ de multiplicité respective $n_1,\cdots, n_p$. Alors, on a 
$$
P=\alpha\prod_{1\le k\le p}(X-z_k)^{n_k}.
$$

\Propriete []  Un polynôme non nul de $\ob C_n[X]$ admet au plus $n$ racines distinctes dans $\ob C$. 
\bigskip

\Propriete []  Si $P\in\ob C[X]$ admet au moins $n+1$ racines distinctes alors $P=0$. 
\bigskip

\Concept [] Relations coefficients-racines 

\Propriete []  Etant donné un polynôme scindé $P=\sum_{0\le k\le n}a_kX^k$, de degré $n\ge1$ et et de racines $z_1,\cdots, z_p$, on a 
$$
a_n\prod_{1\le k\le n}(X-z_k)=P=\sum_{0\le k\le n}a_jX^k=P.
$$
En développant le produit de droite, on obtient alors les relations coefficients-racines : 
$$
\eqalign{
\sum_{1\le k\le n}z_k&=-{a_{n-1}\F a_n}
\cr
\sum_{1\le i< j\le n}z_iz_j&={a_{n-2}\F a_n}
\cr
&\vdots
\cr
\sum_{1\le i_1< i_2< \cdots<i_p\le n }z_{i_1}z_{i_2}\cdots z_{i_p}&=(-1)^p{a_{n-p}\F a_n}\qquad (1\le p\le n)
\cr
&\vdots
\cr
z_1z_2\cdots z_n&=(-1)^n{a_0\F a_n}
}
$$

\Remarque : à l'aide de ces relations, on peut aussi calculer des sommes du type
$$
\sum_{1\le k\le n}(z_k)^\ell\qquad (\ell\ge1). 
$$

Exercice :  Notant $a$, $b$, $c$ et $d$ les racines du polynôme $X^4+X^3+X^2+X+1$, calculer 
$$
a^2+b^2+c^2+d^2\qquad \mbox{ et } \qquad a^3+b^3+c^3+d^3.
$$

Exercice :  Notant $a$, $b$ et $c$ les racines du polynôme $X^3+2X^2+3X+4$, calculer la somme 
$$
{1\F a}+{1\F b}+{1\F c}
$$
de deux manieres diferentes. 
\bigskip


\Propriete [Title=Racines $n^\ieme$ de l'unité] 
Pour chaque entier $n\ge1$, on a 
$$
X^n-1=\prod_{k=0}^{n-1}\Q(X-\e^{2\pi ik/n}\W).
$$ 


\hautspages{Olus Livius Bindus}{Espaces vectoriels}

\pagetitretrue


\Chapter EV, Espaces vectoriels.
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip

\Section GEV, Espaces vectoriels. 

\Subsection DeathEV, Définition.

\noindent
Soit $E$ un ensemble muni d'une loi externe $\ob K\times E\to\ob E$, notée $.$ et d'une loi interne,~notée~$+$. 
Alors, $(E,+,.)$ est un espace vectoriel sur le corps $(\ob K,+,\times)$ si, et seulement si 
\medskip
\item{i)}L'ensemble $E$ n'est pas vide : 
$$
E\neq\emptyset.
$$ 
\item{ii)}L'opération $+$ est une loi interne de $E$ : 
$$
\forall (x,y)\in E^2, \qquad x+y\mbox{ est défini}\quad\mbox{et}\quad x+y\in E
$$
\item{iii)}L'opération $+$ admet un élément neutre, noté $0$, dans $E$ : 
$$
\exists 0\in E:\qquad \forall x\in E,\qquad 0+x=x+0=x
$$
\item{iv)}Chaque élément de l'ensemble $E$ admet un inverse pour la loi $+$ : 
$$
\forall x\in E, \qquad \exists y\in E:\qquad x+y=0=y+x
$$
\item{v)}L'opération $+$ est associative dans $E$ :
$$
\forall (x,y,z)\in E^3, \qquad(x+y)+z=x+(y+z)
$$ 
\item{vi)}L'opération $+$ est commutative dans $E$ : 
$$
\forall (x,y)\in E^2, \qquad x+y=y+x
$$
\item{vii)}L'opération $.$ est une loi externe de $E$ : 
$$
\forall \lambda\in\ob K, \qquad \forall x\in E,\qquad \lambda.x\mbox{ est défini}\quad \mbox{et}\quad \lambda.x \in E
$$
\item{viii)}L`élément neutre $1$ pour la multiplication de $\ob K$ est neutre pour la loi externe$.$ : 
$$
\forall x\in E,\qquad 1.x=x.
$$
\item{ix)}L'opération $.$ est associative dans $E$
$$
\forall (\lambda,\mu)\in \ob K^2, \qquad\forall x\in E, \qquad(\lambda\times\mu).x=\lambda.(\mu.x).
$$ 
\item{x)}L'opération $.$ est distributive sur l'opération $+$ dans $E$ : 
$$
\eqalign{
\forall\lambda\in\ob K,\qquad\forall(x,y)\in E^2, \qquad \lambda.(x+y)=\lambda.x+\lambda.y
\cr
\forall(\lambda,\mu)\in\ob K^2,\qquad\forall x\in E, \qquad (\lambda+\mu).x=\lambda.x+\mu.x
 }
$$ 

\Remarque{} 1 : Un tel ensemble sera appelé plus simplement un $\ob K$-espace vectoriel. 
\bigskip

\Remarque{} 2 : En bref, $(E,+,.)$ est un $\ob K$-espace vectoriel si, et seulement si :
$(E,+)$ est un groupe commutatif muni d'une loi externe $.$ sur $\ob K$, associative et distributive sur $+$. 
\bigskip

\Exemple. $\ob R$ est un $\ob R$-espace vectoriel  et  $\ob C$ est un $\ob C$-espace vectoriel . 

\bigskip

\Remarque{} 3 : Les éléments d'un $\ob K$-espace vectoriel $E$ sont appelés ``vecteurs" alors que les éléments du corps $\ob K$ sont appelés ``scalaires''. Ne pas mélanger. 
\bigskip

\Remarque{} 4 : Pour simplifier, un espace vectoriel est un ensemble $E$ plutôt sympathique dont les éléments peuvent être ajoutés ou multipliés 
par un élément de $\ob K$. 
\bigskip

\Remarque{} 5 : un espace vectoriel contient au moins un élément : l'élément neutre $0_E$. 
\bigskip

\Remarque{} 6 : pour simplifier les notations, on écrira $\lambda x$ à la place de $\lambda.x$. 
\bigskip

\Subsection PropEV, Propriétés.
\bigskip

\noindent
Dans toute la suite de ce chapitre, $E$ designe un $\ob K$-espace vectoriel. 

\Concept [] Elément neutre de $E$

\noindent
Dans un $\ob K$-espace vectoriel $E$, l'élément neutre $0_E$ pour l'addition $+$ est unique (car l'élément neutre du groupe $(E,+)$ est unique). 

\Concept [] Opposés uniques dans $E$. 

\noindent
Dans un $\ob K$-espace vectoriel $E$, chaque vecteur $x$ admet un unique opposé $y$ vérifiant $$
x+y=y+x=0_E.
$$ 
Cet opposé unique, qui est égal à $(-1).x$, est noté $-x$ et permet de définir une soustraction dans $E$ en posant
$$
\forall (x,x')\in E^2, \qquad x'-x:=x'+(-x).
$$

\Concept [] Produit exterieur nul

\noindent
Pour $\lambda\in\ob K$ et $x\in E$, on a 
$$
{
\lambda.x=0\Longleftrightarrow \lambda=0_K\mbox{ ou }x=0_E}. 
$$
Le produit d'un scalaire par un vecteur est nul si, et seulement si le scalaire est nul ou le vecteur est nul. 
\bigskip

\Concept [] Stabilité par combinaisons linéaires

Soient $n\ge1$ un entier, $(\lambda_1,\cdots,\lambda_n)\in\ob K^n$ des scalaires et $(x_1,\cdots, x_n)\in\ob E^n$ des vecteurs. Alors, la quantité 
$$
{\sum_{k=1}^n\lambda_k.x_k=\lambda_1.x_1+\lambda_2x_2+\cdots+\lambda_n.x_n}, 
$$
est appelée une combinaison linéaire des vecteurs $x_1,\cdots,x_n$ à coefficients dans $\ob K$. 
\medskip
Un $\ob K$-espace vectoriel est stable par combinaison linéaires (à coefficients dans $\ob K$). 

\noindent Ainsi, 
$$
\forall n\ge1, \quad\forall (\lambda_1,\cdots,\lambda_n)\in\ob K^n, \quad \forall (x_1,\cdots,x_n)\in E^n, \qquad \sum_{k=1}^n\lambda_kx_k\in E.
$$

\Concept [] Espaces vectoriels complexes

Tout espace vectoriel sur $\ob C$ est également un espace vectoriel sur $\ob R$. 
\bigskip

\Exemple. $\ob C$ est un espace vectoriel sur $\ob R$ ou sur $\ob C$. 
\bigskip

\Concept [] Espaces vectoriels produits

Soient $(E,+,.)$ et $(F,+,.)$ deux $\ob K$ espaces vectoriels. Alors, on punit l'ensemble $E\times F$ d'une structure de $\ob K$-espace vectoriel en posant 
$$
\eqalign{
\forall(x,y)\in E\times F , \qquad  \forall (x',y')\in E\times F , \quad (x,y)+(x',y'):=(x+x',y+y'),
\cr
\forall \lambda\in\ob K , \qquad  \forall (x,y)\in E\times F , \qquad \lambda.(x,y):=(\lambda.x,\lambda.y).
}
$$

\Exemple. Pour $n\ge1$, $\ob K^n$ est un $\ob K$-espace vectoriel et $\ob C^n$ est un $\ob R$-espace vectoriel. 
\bigskip

\Concept [] Espaces vectoriels de fonctions


Soient $A$ un ensemble et $(E,+,.)$ un $\ob K$-espace vectoriel.Alors, l'ensemble $\sc F(A,E)$ des fonctions $f:A\to E$ 
forme un $\ob K$-espace vectoriel si on le muni des lois $+$ et $.$ définies par 
$$
\eqalign{
&\forall (f,g)\in\sc F(A,E)^2, \qquad f+g\mbox{ est l'application définie par }
\eqalign{f+g:A&\to E\cr  x&\mapsto  f(x)+g(x)} 
\cr 
&\forall \lambda\in\ob K , \qquad \forall f\in\sc F(A,E) , \qquad \lambda.f \mbox{ est l'application
définie par } \eqalign{\lambda.f:A&\to E\cr  x&\mapsto  \lambda.f(x)} 
} 
$$

\Remarque : En particulier, on a 
$$
\forall x\in A, \qquad (f+g)(x):=f(x)+g(x)\qquad \mbox{ et }\qquad (\lambda.f)(x):=\lambda.f(x).
$$

\Remarque : Le zéro de l'espace vectoriel $\sc F(A,E)$ est noté $0$, c'est l'application nulle 
$$
\eqalign{0: A&\to E\cr x&\mapsto  0_E}.
$$ 

\Section SEV, Sous-espaces vectoriels. 

\Subsection SSEV, Caractérisation.

Un sous-espace vectoriel d'un $\ob K$-espace vectoriel $(E,+,\cdot)$ est un espace vectoriel $F$ inclus dans $E$ tels que les lois de $F$ soient la restriction à $F$ des lois de~$E$. 
\bigskip

\Remarque : pour simplifier, un sous-espace vectoriel $F$ de $E$ est un espace vectoriel plus petit que $E$ (au sens de l'inclusion), muni des mêmes opérations (de la même structure). 
\bigskip

\Remarque : Prouver que $F$ est un espace vectoriel est beaucoup trop bourrin via les propriétés i) à x) alors que 
c'est sans douleur via les sous-espaces vectoriels. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel. Alors $\{0\}$ et $E$ sont deux sous-espaces vectoriels de $E$. 
\bigskip

\Propriete []  Si $F$ est un sous-espace vectoriel de $E$, alors $F$ contient $0_E$. 
\bigskip


\Propriete []  Un ensemble $F$ forme un sous-espace vectoriel d'un $\ob K$-espace vectoriel $E$ si, et~seulement~si
$F\neq\emptyset$, $F\subset E$ et si $F$ est stable par combinaisons linéaires 
(à~coefficients dans $\ob K$) , autrement dit
$$
\forall (\lambda,\mu)\in\ob K^2 , \qquad  \forall (x,y)\in F^2 ,\qquad \lambda. x+\mu. y\in F, 
$$
c'est-à-dire stable par addition et par la multiplication exterieure
$$
\Q\{\eqalign{
	&\forall (x,y)\in F^2, \qquad x+y\in F
	\cr
	&\forall \lambda\in\ob K, \forall x\in E, \qquad \lambda.x\in F
}\W.
$$

\Exemple. Pour chaque intervalle $I$, l'ensemble $\sc C(I,\ob K)$ des fonctions continues $f:I\to\ob K$ forme un $\ob K$-
espace vectoriel.

 \Exercice. Pourver que l'ensemble des solutions complexes sur $\ob R$ de l'équation différentielle 
$$
x y''+\arctan(x)y'+y=0
$$ 
forme un $\ob C$-espace vectoriel. 
\bigskip

\Subsection ISEV, Intersection et espace vectoriel engendré.
\bigskip

\Propriete []  Soient $I$ un ensemble et $\{F_i\}_{i\in I}$ des sous espaces d'un $\ob K$-espace vectoriel $E$. 
Alors, l'intersection $\cap_{i\in I}F_i$ est également un sous-espace vectoriel de $E$. 
\bigskip
\Remarque : l'intersection de sous-espaces vectoriels de $E$ est un sous-espace vectoriel de~$E$. 
\bigskip


\Definition []  Soit $E$ un $\ob K$-espace vectoriel. 
Alors, le (sous)-espace vectoriel (de $E$) engendré par une partie $A\subset E$ est le plus petit sous-espace vectoriel de $E$ (pour l'inclusion) contenant $A$. 
Cet espace vectoriel est noté Vect$(A)$ ou $\langle A\rangle$ et on a 
$$
\mbox{Vect}(A):=\mathop{\cap}\limits_{\ss F \mbox{ sev de }E\atop\ss A\subset F}F.
$$

\Propriete []  Soient $n\in\ob N^*$ et $E$ un $\ob K$-espace vectoriel. Alors, le sous-espace vectoriel de~$E$ engendré par une partie finie 
$\{x_1,\cdots x_n\}$ de $E$ est 
$$
{
\mbox{Vect}\Q(\{ x_1, \cdots, x_n\}\W):=\Q\{\sum_{1\le k\le n}\lambda_k.x_k:(\lambda_1,\cdots,\lambda_n)\in\ob K^n\W\}}.
$$

\Propriete []  Soient $E$ un $\ob K$-espace vectoriel. Alors, le sous-espace vectoriel de $E$ engendré par une partie $A$ de $E$ est 
$$
\mbox{Vect}(A):=\Q\{\sum_{1\le k\le n}\lambda_k.x_k:n\in\ob N^*, (\lambda_1,\cdots,\lambda_n)\in\ob K^n, (x_1,\cdots, x_n)\in A^n\W\}.
$$

\Remarque : Si $E$ est un $\ob K$ espace vectoriel, le sous espace vectoriel engendré par une partie~$A$ de $E$ est l'ensemble des combinaisons linéaires à coefficients dans $\ob K$ qu'il est possible de former avec les éléments de $A$. 
\bigskip

\Subsection SommeEV, Somme d'espaces vectoriels. 
\bigskip

\noindent
Soient $E$ un $\ob K$-espace vectoriel. Alors, la somme de deux sous-espaces vectoriels $F$ et $G$ de $E$ est l'ensemble 
$$
F+G:=\{x+y:x\in F,y\in G\}. 
$$

\Propriete []  La somme $F+G$ est le sous espace vectoriel de $E$ engendré par $F$ et $G$. 
$$
F+G=\mbox{Vect}(F\cup G).
$$
En particulier, c'est un espace vectoriel. 
\bigskip


Soit $E$ un $\ob K$-espace vectoriel. On dit que la somme $F+G$ de deux sous--espaces vectoriels $F$ et $G$ de $E$ est directe, 
et on la note alors $F\oplus G$ si, et seulement si $F\cap G=\{0\}.$
\bigskip

\centerline{\bf Méthode classique pour prouver que $F\oplus G$}
\medskip\noindent
On prend $x$ dans $F\cap G$ et on se débrouille pour montrer que $x=0$. 

\noindent
{\it Cela montre que $F\cap G\subset\{0\}$ et comme $F\cap G$ est un EV, on~a $\{0\}\subset F\cap G$ et 

on en déduit donc que $F\cap G=\{0\}$}
\bigskip

Soient $E$ un $\ob K$-espace vectoriel. On dit que deux sous--espaces vectoriels $F$ et $G$ de $E$ 
sont supplémentaires (dans $E$) si, et seulement si $E=F\oplus G$, c'est-à-dire si 
$$
E=F+G\quad\mbox{et}\quad F\cap G=\{0\}.
$$

\Remarque : Décomposer un espace vectoriel en deux espaces supplémentaires, \c ca peut servir à carractériser simplement 
certaines applications (symétries, projections, involutions, rotations...), qui laissent des espaces vectoriels globalement invariants.
\bigskip

\centerline{\bf Méthode classique pour prouver que $E=F+G$}
\medskip\noindent
1) On montre que $F+G\subset E$. C'est facile, il suffit de montrer que $F\subset E$ et que $G\subset E$. \pn
2) On montre que $E\subset F+G$ : on prend $x$ dans $E$ et on cherche $y\in F$ et $z\in G$ tels 
que $x=y+z$. {\it Il suffit de trouver le bon $y$ puisque $z=x-y$ est 
déterminé 

\quad par le choix de $x$ et de $y$. }
\bigskip

 \Exercice. Montrer que l'ensemble $G$ de fonctions constantes sur $\ob R$ et que l'ensemble $F$ 
des fonctions $f:\ob R\to\ob C$ vérifiant $f(0)=0$ sont deux $\ob C$-espaces vectoriels. Sont-ils en somme directe ? Quelle est leur somme ? 

\Section GAL, Applications linéaires. 

Dans cette section $E$, $F$ et $G$ désignent des $\ob K$-espaces vectoriels. 
\bigskip

\Subsection Applin, Addition et multiplication externe. 
\bigskip

\Concept [] Definition

Soient $E$ et $F$ deux $\ob K$-espaces vectoriels. Une application $f:E\to F$ est linéaire si, et~seulement si 
$$
\forall (\lambda,\mu)\in\ob K^2 ,\qquad \forall (x,y)\in E^2 , \qquad f(\lambda.x+\mu.y)=\lambda.f(x)+\mu.f(y).
$$ 

\Exemples. les homothéties, symétries, projections, rotations, les applications 
$$
\eqalign{
	\sc F(\ob R,\ob C)&\to\ob C\cr 
	f&\mapsto  f(0)
}
\qquad 
\eqalign{
	\sc C(\ob [0,1], \ob R)&\to \ob R\cr  
	f&\mapsto  \int_0^1f(x)\d x
}
\qquad 
\eqalign{
	\sc C^1(\ob R)&\to\sc C^0(\ob R)\cr 
	f&\mapsto  f'
}
\qquad 
\eqalign{
	\ob R^2&\to\ob R^3\cr
	(x,y)&\mapsto  (x,y,x+y)
}
$$

\centerline{\bf Méthode classique pour prouver que $f:E\to F$ est linéaire}
\medskip\noindent
1) On commence par s'assurer (rapidement) que $E$ et $F$ sont bien des espaces vectoriels 
et que $f:E\to F$ est une application. 

\noindent
2) On prend $(\lambda,\mu)$ dans $\ob K^2$ ainsi que $(x,y)$ dans $E^2$, on calcule $f(\lambda x+\mu y)$ 
\noindent\ 
ainsi que $\lambda f(x)+\mu f(y)$ à l'aide de la définition de $f$ et on essaye de montrer 

que ces deux quantités sont égales. 
\bigskip

\Concept [] Vocabulaire

Soit $E$ et $F$ deux espaces vectoriels et $f:E\to F$ une application linéaire. Alors, 
\smallskip\noindent
$f$ est un morphisme d'espaces vectoriels (elle conserve la structure d'espace vectoriel). 
\smallskip\noindent $f$ est unendomorphisme (d'espaces vectoriels) si, et seulement si $E=F$. 
\smallskip\noindent
$f$ est un isomorphisme (d'espaces vectoriels) si, et seulement si $f:E\to F$ est bijective. 
Dans ce cas, on dit que les espaces $E$ et $F$ sont isomorphes
\smallskip\noindent 
$f$ est un automorphisme (d'espace vectoriel) si, et seulement si $E=F$ et si $f:E\to E$ est bijective.
Autrement dit, si $f$ est un endomorphisme et un isomorphisme. 
\smallskip\noindent $f$ est uneforme linéaire si, et seulement si $F=\ob K$. 
\bigskip

\Exemple. L'application nulle $\eqalign{0: E&\to F\cr  x&\mapsto  0_F}$ est linéaire, 
l'application nulle $\eqalign{0: E&\to E\cr  x&\mapsto  0_E}$ est un endomorphisme 
et l'identité $\eqalign{\Id_E:E&\to E\cr x&\mapsto  x}$ est un automorphisme de $E$. 
\bigskip

Soient $E$ et $F$ deux $\ob K$-espaces vectoriels. Alors, l'ensemble $\sc L(E,F)$ des applications linéaires $f:E\to F$ 
forme un $\ob K$-espace vectoriel, s'il est muni de l'addition et de la multiplication externe des applications. 
\bigskip
\Remarque : L'espace vectoriel $\sc L(E,E)$ des endomorphismes $f:E\to E$ est noté $\sc L(E)$. 
\bigskip

\Subsection Comp, Composition.

Soient $E$, $F$, $G$ des $\ob K$-espaces vectoriels. Alors, la composée $g\circ f:E\to G$ de deux applications linéaires $f:E\to F$ et $g:F\to G$ 
est une application linéaire . 
\bigskip

\Propriete : Soit $E$, $F$ deux espaces vectoriels et $f:E\to F$ une application linéaire bijective (un isomorphisme). Alors, sa bijection réciproque $f^{-1}:F\to E$ est linéaire. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel. Alors, l'ensemble $\mbox{Gl}(E)$ des automorphismes de $E$ est un groupe (non-commutatif), si on le munit de la composition des applications, appelé groupe linéaire de $E$.  
\bigskip


\Remarque : l'élément neutre du {groupe non commutatif $(\mbox{Gl}(E),\circ)$ est l'identité $\mbox{Id} _E$}. 
\bigskip


\Remarque : la loi de composition peut intéragir avec la loi $+$ et la loi $.$ des applications. Ainsi, pour les applications linéaires, elle est distributive sur la loi $+$
$$
\Q\{\eqalign{
\forall (f,g)\in\sc L(F,G)^2, \qquad \forall h\in\sc L(G,H), \qquad h\circ (f+g)=h\circ f+h\circ g. 
\cr
\forall h\in\sc L(H,F),\qquad \forall (f,g)\in\sc L(F,G)^2, \qquad (f+g)\circ h=f\circ h+g\circ h. 
}\W.
$$ 
Pour les applications linéaires, elle commute avec la loi $.$ ainsi
$$
\forall \lambda\in\ob K,\qquad \forall f\in\sc L(E,F), \qquad \forall g\in\sc L(F,G), \qquad \lambda.g\circ f=(\lambda g)\circ f=g\circ (\lambda f). 
$$

Soit $E$ un $\ob K$-espace vectoriel et $u\in\sc L(E)$. Alors, on note $u^0:=\mbox{Id}_E$ et, on pose 
$$
\forall n\in\ob N^*, \qquad {u^n:=\underbrace{u\circ u\circ u\circ\cdots\circ u}_{n \mbox{ fois}}}.
$$
De plus, si $u$ est inversible, i.e bijective, on pose 
$$
\forall n\in\ob N^*, \qquad{u^{-n}=(u^n)^{-1}\underbrace{u^{-1}\circ u^{-1}\circ u^{-1}\circ\cdots\circ u^{-1}}_{n \mbox{ fois}}}.
$$

\Theoreme [Title=binôme de Newton;$u$ et $v$ endomorphismes vérifiant {$u\circ v=v\circ u$}] 
$$
\forall n\in\ob N, \qquad {(u+v)^n=\sum_{0\le k\le n}{n\choose k}u^k\circ v^{n-k}}.
$$

\Subsection Noyim, Noyau et image.

\Concept [] Image réciproque

Soit $f:E\to F$ une application linéaire. Pour chaque sous-espace vectoriel $F'$ de $F$, 
l'image réciproque $f^{-1}(F')$ de $F'$ par $f$ est un sous-espace vectoriel de $E$. 
\bigskip

\Concept [] Noyau.

\noindent
Soit $f:E\to F$ un morphisme d'espaces vectoriels (une application linéaire). Alors, le~noyau de $f$ 
est l'espace vectoriel défini par 
$${
\mbox{Ker}(f):=\{x\in E:f(x)=0_F\}}
$$ 

\Remarque : Déterminer le noyau d'une application linéaire $u$, c'est résoudre l'équation 
$$
u(x)=0.
$$ 
\bigskip


 Exercice :  Déterminer le noyau de l'application $\eqalign{f: \ob R^3&\to\ob R^3\cr(x,y,z)&\mapsto  (x+y,y+z,x+y+z)}$. 
\bigskip

\Concept [] Injectivité

\noindent
Soit $f:E\to F$ une application linéaire. Alors, 
$${
f\mbox{ est injective }\Longleftrightarrow \mbox{Ker}(f)=\{0\}}.
$$ 


\Remarque : pour étudier si une application linéaire est injective, un excellent reflexe est de regarder son noyau $\mbox{Ker }f$. 
\bigskip

 Exercice :  Prouver que l'application $\eqalign{\ob R^2&\to\ob R^2\cr(x,y)&\mapsto  (x+y,x-y)}$ est injective .
\bigskip
\Concept [] Image 

\Propriete []  Soit $f\in\sc L(E,F)$. Pour chaque sous-espace vectoriel $E'$ de $E$, l'image~$f(E')$ est un sous-espace vectoriel de $F$. 
\bigskip
\noindent
Pour $f\in\sc L(E,F)$, on note $\mbox{Im}(f)$ l'espace vectoriel défini par 
$$
{\mbox{Im}(f):=f(E)=\{f(x):x\in E\}}.
$$

\Concept [] Surjectivité

\Propriete []  Soit $f\in\sc L(E,F)$. Alors, 
$$
f\mbox{ est surjective}\Longleftrightarrow \mbox{Im}(f)=F. 
$$

\Remarque : déterminer le noyau (l'injectivité) d'une application linéaire est en général beaucoup plus facile que détarminer son image (sa surjectivité). La dimension des espaces vectoriels, que nous introduirons plus tard, va nous permettre de déterminer l'image (la surjectivité) BEAUCOUP plus facilement. 
\bigskip

 Exercice :  Déterminer l'image de la forme linéaire $\eqalign{\ob R^3&\to\ob R^3\cr(x,y,z)&\mapsto  (x-y,y-z,z-x)}$.
\bigskip

\Concept [] Equation linéaire avec second membre $u(x)=b$

\Propriete [] Soit $u\in\sc L(E,F)$, soit $b\in F$ et soit $S:=\{x\in E:u(x)=b\}$ l'ensemble solution de l'équation $u(x)=b$. 
Alors, deux cas se produisent : 
\smallskip\noindent
Si $b\notin\mbox{Im (u)}$, alors  $S=\emptyset$ . 
\smallskip
\noindent
Si $b\in\mbox{Im}(u)$, alors il existe au moins une solution $x_0\in E$ de l'équation $u(x_0)=b$ et alors 
$$
S=x_0+\mbox{Ker}(u)=\Q\{x_0+y:y\in\mbox{Ker}(u)\W\}=\Q\{x_0+y:u(y)=0\W\}.
$$ 

\Subsection Sympro, homothétie, projecteurs, symétries.


\Concept [] Homothétie

Soit $E$ un $\ob K$-espace vectoriel. Alors, l'homothétie (vectorielle) de $E$ de rapport $\lambda\neq0$ est 
{l'automorphisme $h_\lambda=\lambda\mbox{Id}_E$} de l'espace $E$, c'est à dire l'application définie par
$$
\eqalign{h_\lambda:E&\to E\cr x&\mapsto \lambda x}.
$$ 


\Remarque : la bijection réciproque de l'homothétie $\lambda\mbox{Id}_E$ est l'homothétie $\lambda^{-1}\mbox{Id}_E$. 
\bigskip


\Remarque : une propriété remarquable des homothéties vectorielles est qu'elles commutent avec tous les endomorphismes de $E$. 
Autrement dit, 
$$
\forall f\in\sc L(E), \qquad (\lambda\mbox{Id}_E)\circ f=f\circ(\lambda\mbox{Id}_E)=\lambda f.
$$
En particulier, on peut utiliser le binôme de Newton pour les homothéties. 
\bigskip

\Concept [Index=Projection] Projecteurs

\noindent
Soit $E$ un $\ob K$-espaces vectoriels et soient $F$ et $G$ deux sous espaces vectoriels sup\-plé\-men\-tai\-res de $E$ (i.e. tels que $E=F\oplus G$). 
Alors, $p\in\sc L(E)$ est un projecteur sur $F$ parallèlement à $G$ si, et~seulement si, 
$$
\forall x\in F ,\qquad  p(x)=x \qquad\mbox{et}\qquad\forall y\in G , \qquad  p(y)=0 .
$$

\Remarque{} 1. Si $p$ est un projecteur de $E$ sur $F$ parallèlement à $G$, on a 
$$
F=\Ker(p) \qquad\mbox{et}\qquad  G=\IM(p)=\Ker(p-Id_E) .
$$
Ainsi, l'image de $p$ est l'espace vectoriels $G$ des vecteurs invariants par $p$ et on a 
$$
E=\Ker(p)\oplus\IM(p)=\Ker(p)\oplus\Ker(p-\Id_E).
$$

\Remarque{} 2. Pour un projecteur $p\in\sc L(E)$, la donnée des ensembles $\Ker(p)$ et $\IM(p)$ determinent compétement $p$ sur tout l'espace $E$. En effet, 
$$
\forall x\in E, \qquad p(x)=p(\underbrace{y}_{\in\mbox{\sevenrm Ker(p)}}+\underbrace{z}_{\in\mbox{\sevenrm Im(p)}})=\underbrace{p(y)}_0+\underbrace{p(z)}_{z}=z,
$$
où $x=y+z$ est l'unique décomposition de $x$ dans $E=\Ker(p)\oplus\IM(p)$. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel et $p\in\sc L(E)$ un endomorphisme. Alors, 
$$
{p\mbox{ est un projecteur de }E\quad \Longleftrightarrow\quad p\circ p=p}.
$$

\Remarque{} 3. Un projecteur n'est pas bijectif, sauf dans les cas extremement particuliers ou on a $p=\Id_E$ ou $E=\{0\}$. 
\bigskip

\Exercice. Prouver que l'application $\eqalign{\ob R^2&\to \ob R^2\cr (x,y)&\mapsto  \Q({4x-2y\F5},{y-2x\F5}\W)}$ est un projecteur. Quel est son noyau et son image ?
\bigskip

\Concept [] Symétries 

\noindent
Soit $E$ un $\ob K$-espaces vectoriels et soient $F$ et $G$ deux sous espaces vectoriels sup\-plé\-men\-tai\-res de $E$ (i.e. tels que $E=F\oplus G$). 
Alors, $s\in\sc L(E)$ est une symétrie par rapport à $F$ suivant la direction $G$ si, et~seulement si, 
$$
\forall x\in F ,\qquad  s(x)=x \qquad\mbox{et}\qquad\forall y\in G , \qquad  s(y)=-y .
$$

\Propriete []  Toutes les symétries de $E$ sont des automorphismes.
\bigskip

\Remarque{} 1. Si $s\in\sc L(E)$ est une symétrie par rapport à $F$ parallèlement à $G$, on a 
$$
F=\Ker(s-\Id_E) \qquad\mbox{et}  G=\Ker(s+\Id_E) . 
$$
Autrement dit, $F$ est l'espace des vecteurs invariants par la symétrie et la direction $G$ est l'espace des vecteurs qui sont transformés en leur opposé par $s$. De plus, on a 
$$
{E=\mbox{Ker}(s-\Id_E)\oplus\mbox{Ker}(s+\Id_E)}.
$$

\Remarque{} 2. Pour une symétrie $s\in\sc L(E)$, la donnée des ensembles $\Ker(s-\Id_E)$ et $\Ker(s+\Id_E)$ determinent compétement $s$ sur tout l'espace $E$. En effet, 
$$
\forall x\in E, \qquad s(x)=s(\underbrace{y}_{\in\mbox{\sevenrm Ker(s-Id)}}+\underbrace{z}_{\in\mbox{\sevenrm Ker(s+Id)}})=\underbrace{s(y)}_y+\underbrace{s(z)}_{-z}=y-z,
$$
où $x=y+z$ est l'unique décomposition de $x$ dans $E=\Ker(s-\Id_E)\oplus\Ker(s+\Id_E)$. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel et $s\in\sc L(E)$ un endomorphisme. Alors, 
$$
{s\mbox{ est une symétrie de }E\quad \Longleftrightarrow\quad s\circ s=\Id_E}.
$$


\Exercice. Prouver que l'application $\eqalign{\ob R^2&\to \ob R^2,\cr(x,y)&\mapsto  \Q({3x-4y\F5},{-3y-4x\F5}\W)}$ est une symétrie. Selon quelle direction ? Par rapport à quel espace ?



\hautspages{Olus Livius Bindus}{Espaces vectoriels de dimension finie}

\pagetitretrue


\Chapter EV, Espaces vectoriels de dimension finie.
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip

\Section GEV, Familles de vecteurs.
\bigskip

\noindent
\mbox{Rappel.} Une combinaison linéaire des $n$ vecteurs $x_1,\cdots, x_n$ de $E$ à coefficients dans $\ob K$ est une somme de la forme
$$
\sum_{1\le k\le n}\lambda_kx_k=\lambda_1x_1+\lambda_2x_2+\cdots+\lambda_nx_n, \eqdef{leena2711}
$$
où $\lambda_1,\cdots,\lambda_n$ sont des scalaires, i.e. des éléments du corps $\ob K$. 
\bigskip

\Remarque : Soit $F$ un $\ob K$ espace vectoriel. L'image par une application linéaire $u:E\to F$ d'une combinaison linéaire de $n$ vecteurs 
de $E$ à coefficients dans $\ob K$ est une combinaison linéaire de $n$ vecteurs de $F$ à coefficients dans $\ob K$. En particulier, l'image de \eqref{leena2711} par u est 
$$
\sum_{1\le k\le n}\lambda_ku(x_k)=\lambda_1u(x_1)+\lambda_2u(x_2)+\cdots+\lambda_nu(x_n).
$$

\Subsection Femmelibre, Famille libre. 
\bigskip

\Definition []  Une famille $(x_1,x_2,\cdots,x_n)$ de vecteurs d'un $\ob K$-espace vectoriel $E$ est dite libre (ou linéairement indépendante) si, et~seulement~si 
$$
\forall(\lambda_1,\cdots,\lambda_n)\in\ob K^n, \qquad\qquad {\sum_{1\le k\le n}\lambda_kx_k=0\quad \Longrightarrow\quad (\lambda_1,\lambda_2,\cdots,\lambda_n)=(0,0,\cdots,0)}
$$
Autrement dit, une famille de vecteurs est libre si, et seulement si, la seule combinaison linéaire nulle faite avec ces vecteurs 
est la combinaison linéaire dont tous les coefficients sont nuls. 
\bigskip

\Definition []  Une relation de dépendance linéaire de la famille $(x_1,x_2,\cdots,x_n)$ de vecteurs est une relation du type
$$
\sum_{1\le k\le n}\lambda_kx_k=0\quad\mbox{ avec }\quad(\lambda_1,\lambda_2,\cdots,\lambda_n)\neq(0,0,\cdots,0). 
$$

\Definition []  Soit $\sc F:=(x_1,x_2,\cdots,x_n)$ une famille de vecteurs d'un $\ob K$-espace vectoriel $E$. Alors, 
$$
\eqalign{
\mbox{La famille $\sc F$ est liée }&\Longleftrightarrow\mbox{ La famille $\sc F$ admet une relation de dépendance linéaire}\cr
& \Longleftrightarrow\mbox{ La famille $\sc F$ n'est pas libre }
}
$$

\Remarque. On dit qu'une famille infinie de vecteur est libre si, et~seulement~si, chacune de ses sous-familles finie de vecteurs est libre
\bigskip

\Exercice. Prouver que la famille de fonctions $(x\mapsto\e^{\alpha x})_{\alpha\in\ob R}$ est libre. 
\bigskip

\Propriete L'image d'une famille libre de $E$ par une application linéaire injective $f:E\to F$ est une
famille libre de $F$

\Subsection Femmelibre, Famille génératrice. 
\bigskip

\Definition []  Une famille finie $(e_1,\cdots,e_n)$ de vecteurs d'un $\ob K$-espace vectoriel $E$ est dite 
génératrice si, et~seulement~si $E=\Vect\b(\{e_1,\cdots,e_n\}\b)$, c'est~à~dire si et seulement si
$$
\forall x\in E , \qquad  \exists (\lambda_1,\cdots, \lambda_n)\in\ob K^n :\qquad {x=\sum_{1\le k\le n}\lambda_ke_k}.
$$

\Remarque : Si $E=\Vect\b(\{e_1,\cdots,e_n\}\b)$ on dit aussi que la famille $(e_1,\cdots,e_n)$ engendre ou génére l'espace vectoriel $E$. 
\bigskip

\Definition []  Une famille (éventuellement infinie) $(e_i)_{i\in I}$ de vecteurs d'un $\ob K$-espace vectoriel $E$ 
est dite génératrice si, et~seulement~si chaque vecteur $x$ de $E$ peut s'écrire comme une combinaison 
linéaire (finie) des vecteurs $(e_i)_{i\in I}$ 
$$
\forall x\in E, \qquad \exists \mbox{ un ensemble fini $J\subset I$ et }
\exists \mbox{ des scalaires $(\lambda_j)_{j\in J}$ tels que }\quad
x=\sum_{j\in J}\lambda_je_j. 
$$

\Exemple. La famille constitué des vecteurs $(1,0,0)$, $(0,1,0)$ et $(0,0,1)$ engendrent $\ob R^3$. 
\bigskip

\Propriete []  Soient $E$ et $F$ deux $\ob K$-espaces vectoriels et soit $(e_i)_{i\in I}$ une famille génératrice de $E$. 
Alors, une application linéaire $u\in\sc L(E,F)$ est entièrement déterminée 
par la donnée des images 
$$
u(e_i)\qquad(i\in I)
$$ 
des vecteurs $(e_i)_{i\in I}$ par l'application $u$. 
\bigskip

\Propriete []  Soient $E$ et $F$ deux $\ob K$-espaces vectoriels. Deux applications linéaires $(u,v)\in\sc L(E,F)$ 
sont identiques si, et seulement~si elles coincident sur une famille génératrice de $E$. Autrement dit
$$
{\exists \mbox{ famille génératrice }(e_i)_{i\in I}\mbox{ de E telle que }\forall i\in I, \quad u(e_i)=v(e_i)\quad\Longleftrightarrow \quad u=v}.
$$

\Remarque : l'interet d'avoir une famille génératrice $(e_1,\cdots,e_n)$ est qu'il suffit 
de démontrer $n$ égalités simples $u(e_1)=v(e_1),\cdots, u(e_n)=v(e_n)$ pour prouver que $u=v$, au lieu de prouver l'égalité compliquée $u(x)=v(x)$ pour tous les $x\in E$. 
\bigskip


\Propriete L'image d'une famille génératrice de $E$ par une application linéaire surjective $f:E\to
F$ est une famille génératrice de $F$. 

\Subsection fambas, bases. 
\bigskip

\Definition []  Une famille de vecteurs d'un $\ob K$-espace vectoriel $E$ est une base si, et seulement si, elle est libre et génératrice. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel. Alors, 
$$
{(e_1,\cdots,e_n)\mbox{ est une base de }E\Longleftrightarrow \forall x\in E, \qquad \exists!(\lambda_1,\cdots,\lambda_n):\qquad x=\sum_{1\le k\le n}\lambda_ke_k}.
$$
Les nombres $(\lambda_1,\cdots,\lambda_n)$ sont alors appelés coordonnées (ou composantes) du vecteur $x$ 
dans la base $(e_1,\cdots,e_n)$. 
\bigskip


\Exemple.  Le $\ob K$-'espace vectoriel $\ob K^n$ est muni d'une base "canonique" (qui s'introduit naturellement) : la famille $(e_1,\cdots, e_n)$ constituée par les vecteurs définis par 
$$
\forall i\in\{1,\cdots,n\},\qquad e_i:=(0,\cdots, 0,\underbrace{1}_{i},0,\cdots, 0). 
$$

\Exemple.  $\{1\}$ est la base "canonique" du $\ob R$-espace vectoriel $\ob R$ et du $\ob C$-espace vectoriel $\ob C$. La famille $\{1,i\}$ est la base canonique du $\ob R$-espace vectoriel $\ob C$. 
\bigskip

\Propriete []  L'image d'une base de $E$ par une application linéaire bijective $f:E\to F$ est une base de $F$. 
\bigskip

\Theoreme [$\{e_1,\cdots,e_n\}$ base d'un $\ob K$-espace vecoriel $E$ avec $n\ge1$]
Pour chaque éléments $f_1,\cdots, f_n$ d'un $\ob K$-espace vectoriel $F$, il existe une unique application linéaire $u\in\sc L(E,F)$ telle que 
$$
\forall k\in\{1,\cdots, n\}, \qquad u(e_i)=f_i.
$$

\Remarque : Soit $(x_1,\cdots,x_n)$ une famille de vecteurs d'un $\ob K$-espace vectoriel $E$ et soit $f\in\sc L(\ob K^n,E)$ l'application linéaire définie par 
$$
\eqalign{
	f:\ob K^n&\to E,\cr
	(\lambda_1,\cdots,\lambda_n)&\mapsto \ds\sum_{1\le k\le n}\lambda_ix_i
}.\eqdef{cattechaude}
$$
Alors, l'image de $f$ est le sous-espace vectoriel 
$$
\IM(f)=\Vect(x_1,\cdots,x_n)=\Q\{\sum_{1\le k\le n}\lambda_kx_k:(\lambda_1,\cdots,\lambda_n)\in\ob K^n\W\}
$$ 
de $E$ engendré par la famille $(x_1,\cdots,x_n)$ et le noyau de $f$ est l'ensemble 
$$
\Ker(f)=\Q\{(\lambda_1,\cdots,\lambda_n)\in\ob K^n:\sum_{1\le k\le n}\lambda_kx_k=0\W\}
$$
des coefficients associés à $0$ et aux relations de dépendance linéaire des $(x_1,\cdots,x_n)$. 
\bigskip
$$
\eqalign{
\mbox{la famille $(x_1,\cdots, x_n)$ est libre }\Longleftrightarrow 
\mbox{ l'application \eqref{cattechaude} est injective}
\cr
\mbox{la famille $(x_1,\cdots, x_n)$ est génératrice }\Longleftrightarrow 
\mbox{ l'application \eqref{cattechaude} est surjective}
\cr
\mbox{la famille $(x_1,\cdots, x_n)$ est une base }\Longleftrightarrow 
\mbox{ l'application \eqref{cattechaude} est bijective}
}
$$
\bigskip

\Section EVDIM, Espaces vectoriels de dimension finie. 
\bigskip

\Subsection DIME, Dimenson d'un espace vectoriel. 
\bigskip

\Definition []  Un $\ob K$-espace vectoriel $E$ est de dimension finie si, et~seulement~s'il contient une famille 
génératrice finie. 

\Propriete []  Si un espace $E$ contient une famille libre de $m$ éléments et une famille génératrice
de $n$ éléments, alors on a $m\le n$. 

\noindent
Une conséquence de cette propriété est que :  
$$
E \mbox{ est un espace vectoriel de dimension infinie }\Leftrightarrow E \mbox{ contient une famille
libre infinie.} 
$$

\Theoreme [Title=Théorème de la base incomplète;{$\sc E=(e_1,\cdots e_m)$} famille libre d'un $\ob K$-espace vectoriel $E$, engendré par une famille {$\sc F=(f_1,\cdots,f_n)$}]  
On peut compléter la famile $\sc E$ avec des vecteurs de la famille $F$ pour obtenir une base $\sc B=\{e_1,\cdots,e_m,g_\alpha,g_\beta,\cdots,g_\aleph\}$ de l'espace $E$. 

\Remarque : en particulier, les bases existent et on sait comment en construire. 
\bigskip

\Theoreme 
[$E$ $\ob K$-espace vectoriel engendré par une famille finie]
Il existe un unique nombre entier positif ou nul, 
noté $\mbox{Dim}_{\ob K}(E)$ et appelé dimension de $E$ (sur le corps $\ob K$), tel que 
$$
\eqalign{\mbox{Dim}_{\ob K}(E)&= \mbox{nombre d'éléments de toutes les bases (d'une base) de $E$}
\cr
&= \mbox{nombre d'éléments de la plus petite (en cardinal) famille génératrice de $E$}
\cr
&= \mbox{nombre d'éléments de la plus grande (en cardianl) famille libre de $E$}
}
$$

\Remarque : Le théorème précédent ne s'applique pas à l'espace vectoriel $\{0\}$, qui par convention sera dit de dimension $0$. 
\bigskip

\Theoreme 
$$
\dim_{\ob R}(\ob R)=1, \qquad\dim_{\ob C}(\ob C)=1\quad \mbox{et}\quad\dim_{\ob R}(\ob C)=2.
$$



\Propriete []  Soit $E$ un $\ob K$-espace vectoriel de dimension finie $n\ge1$ et soit $\sc F:=\{e_1,\cdots,e_n\}$ une famille de vecteurs de $E$. 
Alors, 
$$
\sc F\mbox{ est une famille libre }\Longleftrightarrow\sc F\mbox{ est une famille génératrice}
\Longleftrightarrow \sc F\mbox{ est une base}
$$ 

\Theoreme [$E$ $\ob K$-espace vectoriel de dimension finie $n\ge1$]
L'espace vectoriel $E$ est isomorphe avec $\ob K^n$. 
Plus précisément , pour chaque base $\sb B=\{e_1,\cdots,e_n\}$ de $E$, l'application
$$
\eqalign{\ob K^n&\to E\cr 
(\lambda_1,\cdots, \lambda_n)&\mapsto \ds\sum_{1\le k\le n}\lambda_ke_k}
$$
est un isomorphisme d'espaces vectoriels. 

\Remarque : le théorème précédent affirme que : \pn
1) tous les $\ob K$-espaces vectoriels de même dimension $n\ge1$ ont la m�me structure. \pn
2) plutot que de travailler dans un espace théorique $E$, on peut fixer une base et travailler dans l'espace $\ob K^n$ avec les coordonnées. \pn
3) Le choix de la base détermine l'isomorphisme. 
\bigskip

\Theoreme Deux espaces vectoriels de dimension finie sont isomorphes si, et~seulement~s'ils ont même dimension. 

\Theoreme [$E$ et $F$ deux $\ob K$-espaces vectoriels de dimension finie]  
L'espace vectoriel $E\times F$ est de dimension finie et 
$$
{\Dim_{\ob K}(E\times F)=\Dim_{\ob K}(E)+\Dim_{\ob K}(F)}
$$ 
Si $\{e_1,\cdots,e_n\}$ et $\{f_1,\cdots,f_p\}$ sont des bases respectives de $E$ et $F$, alors $E\times F$ admet pour base la famille $\b\{(e_1,0),\cdots(e_n,0),(0,f_1),\cdots, (0,f_p)$. 

\Remarque : pour $n\in\ob N^*$, on a 
$$
{\dim_{\ob R}(\ob R^n)=n}, \qquad {\dim_{\ob C}(\ob C^n)=n}\quad \mbox{et}\quad{\dim_{\ob R}(\ob C^n)=2n}.
$$
\medskip

 Un étudiant qui écrit $\Dim_{\ob K}(E\times F)=\Dim_{\ob K}(E)\times\Dim_{\ob K}(F)$ devra prier dix fois tous les soirs pendant une semaine la déesse des mathématiques de bien vouloir le pardonner... 
\bigskip

\Remarque : Soit $E$ un $\ob K$-espace vectoriel de dimension finie $n\ge1$, muni d'une base $\{e_1,\cdots,e_n\}$, 
soit $F$ un $\ob K$-espace vectoriel de dimension finie $p\ge1$, muni d'une base $\{f_1,\cdots,f_p\}$ et soit $u\in\sc L(E,F)$ une application linéaire. Alors, pour chaque vecteur 
$$
x=\sum_{1\le k\le n}\lambda_k\e_k=\lambda_1e_1+\cdots+\lambda_n\e_n
$$ 
de l'espace vectoriel $E$, on a 
$$
\eqalign{
u(x)=&\Q(\sum_{1\le i\le n}\lambda_ie_i\W)=\sum_{1\le i\le n}\lambda_iu(e_i),
\cr
=&\lambda_1e_1+\cdots+\lambda_n\e_n=\lambda_1u(e_1)+\cdots+\lambda_nu(e_n)
}
$$
Or, pour chaque entier $i\in\{1,\cdots, n\}$, le vecteur $u(e_i)$ se décompose sur la base $\{f_1,\cdots,f_p\}$ de manière unique. 
Ainsi, il existe $(a_{i,j})_{1\le j\le p}$ tel que 
$$
u(e_i)=\sum_{1\le j\le p}a_{i,j}f_j=a_{i,1}f_1+\cdots+a_{i,n}f_n. \eqdef{Garwall}
$$
Et on a alors
$$
u(x)=\sum_{1\le i\le n}\lambda_i\sum_{1\le j\le p}a_{i,j}f_j=\sum_{1\le i\le n}\sum_{1\le j\le p}\lambda_ia_{i,j}f_j
= \sum_{1\le j\le p}\underbrace{\Q(\sum_{1\le i\le n}\lambda_ia_{i,j}\W)}_{\mu_j}f_j
$$

\Remarque : C'est cette propiété ainsi que le théorème suivant 
pour définir la matrice d'une application linéaire un peu plus loin dans le cours. 
\bigskip

\Remarque : Pour chaque $(i,j)\in\{1,\cdots n\}\times\{1,\cdots,p\}$, on note $g_{i,j}$ l'unique application linéaire uniquement déterminée par 
$$
\forall k\in\{1,\cdots,n\}, \qquad g_{i,j}(e_k)=\Q\{\eqalign{f_j\mbox{ si i=k},\cr
=0\mbox{ sinon}}\W.\eqdef{Abyssus}
$$
Alors, on déduit de l'identité  
$$
u(x)= \sum_{1\le j\le p}\sum_{1\le i\le n}\lambda_ia_{i,j}f_j= \sum_{1\le j\le p}\sum_{1\le i\le n}a_{i,j}g_{i,j}(x)
$$
que l'application $u$ satisfait l'identité fonctionnelle 
$$
u=\sum_{\ss1\le i\le n\atop\ss1\le j\le p}a_{i,j}g_{j,j}, 
$$
pour les coefficients $\{a_{i,j}\}_{\ss1\le i\le n\atop\ss1\le j\le p}$ définis par \eqref{Garwall}. 
\bigskip

\Theoreme [$E$ et $F$ deux $\ob K$-espaces vectoriels de dimension finie]  L'espace vectoriel $\sc L(E,F)$ est de dimension 
$$
{\dim_{\ob K}\sc L(E,F)=\dim_{\ob K}(E)\times\dim_{\ob K}(F)}=np. 
$$
Si $\{e_1,\cdots,e_n\}$ et $\{f_1,\cdots,f_p\}$ sont des bases respectives de $E$ et $F$, la famille $\{g_{i,j}\}_{\ss1\le i\le n\atop\ss1\le j\le p}$ définie par \eqref{Abyssus} forme une base 
de $\sc L(E,F)$. 

\Remarque : L'application linéaire $g_{i,j}$ est définie apr 
$$
g_{i,j}(\lambda_1\e_1+\cdots+\lambda_n\e_n)=\lambda_if_j
$$ 
C'est une "sorte de projection" (ce n'en est pas une) de l'espace $E$ sur la droite $\Vect(f_j)$...
\bigskip


\Theoreme [$E$ $\ob K$-espace vectoriel muni d'une base $\{e_1,\cdots,e_n\}$] 
 Pour chaque entier $1\le k\le n$, on note $f_i$ la forme linéaire définie par 
$$
\eqalign{
	g_k:E&\to\ob K\cr  
	x=\lambda_1e_1+\cdots+\lambda_ne_n&\mapsto  \lambda_k
}
$$
Alors, la famille $\{f_1,\cdots,f_n\}$ forme une base de l'espace vectoriel $\sc L(E,\ob K)$ des formes linéaires sur $E$. Ainsi, on a 
$$
\forall f\in\sc L(E,\ob K), \qquad f=\sum_{1\le k\le n}f(e_k)g_k.
$$

\Remarque : Les formes linéaires les plus utiles sont les formes linéaires sur $\ob K^n$. Une application $f:\ob K^n\to\ob K$ est linéaire si, et seulement si, il existe $(a_1,\cdots,a_n)\in\ob K^n$ tels que 
$$
\forall (x_1,\cdots,x_n)\in\ob K^n, \qquad f(x_1,\cdots,x_n)=\sum_{1\le k\le n}a_kx_k=a_1x_1+\cdots+a_nx_n.
$$
Les formes linéaires interviennet particulierrement dans les systèmes d'équations linéaires de plusieurs inconnues. 
\bigskip

\Remarque : Une forme linéaire $u:E\to K$ non nulle est surjective. 
\bigskip

\Remarque : Si $E$ est de dimension finie $n$, on appelle hyperplan de $E$ tou t espace vectoriel du type $\Ker(u)$ ou $u:E\to\ob K$ est une forme linéaire non nulle. Un hyperplan de $E=\ob K^n$ est ainsi un ensemble du type 
$$
\{(x_1,\cdots,x_n)\in\ob K^n:a_1x_1+a_2x_2+\cdots+a_nx_n=0\},
$$
ou $(a_1,\cdots,a_n)\neq (0,\cdots,0)$ est un $n$-uplet d'éléments de $\ob K$. 
\bigskip


\Subsection DimSEV, Dimension d'un sous-espace vectoriel. 
\bigskip

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel de dimension finie $n\ge0$ et soit $F$ un sous-espace vectoriel de $E$. 
Alors, $F$ est un espace vectoriel de dimension finie et $$
{\dim_{\ob K}(F)\le\dim_{\ob K}(E)}. 
$$
De plus, on a 
$$
{F=E\Longleftrightarrow \dim_{\ob K}(F)=\dim_{\ob K}(E)}.
$$


\Remarque : Pour prouver que deux ensembles $E$ et $F$ sont égaux, on procède en général par double inclusion mais pour les espaces vectoriels, on utilise plutot la méthode suivante : 
\bigskip


\centerline{\bf Pour prouver deux espaces vectoriels $E$ et $F$ sont egaux}
\medskip\noindent
1) Prouver que $F\subset E$ (cela implique que $\dim_{\ob K}(F)\le \dim_{\ob K}(E)$).
\medskip
\noindent
2) Prouver l'égalité des dimensions ou établir que $\dim_{\ob K}(F)\ge \dim_{\ob K}(E)$=:n.
 
Par exemple, en exhibant une famille libre comportant $n$ éléments de $F$.
\bigskip

\Definition []  Soit $E$ un $\ob K$-espace vectoriel de dimension finie. Le rang d'une famille $\{x_1,\cdots, x_p\}$ de vecteurs de $E$ est la dimension de l'espace qu'ils engendrent 
$$
{\mbox{rg}(\{x_1,\cdots,x_n\})=\dim_{\ob K}\Vect\b(\{x_1,\cdots,x_p\}\b)}.
$$ 
C'est le nombre d'éléments de la plus grande sous famille libre de la famille $\{x_1,\cdots,x_p\}$. 
\bigskip


\Theoreme [$F$ et $G$ sous espaces, en somme directe, 
d'un espace vectoriel de dimension finie] 
$$
\dim_{\ob K}(F\oplus G)=\dim_{\ob K}(F)+\dim_{\ob K}(G).
$$

\Theoreme [$E$ $\ob K$-espace vectoriel de dimension finie] 
Pour chaque sous-espace vectoriel $F$ de $E$, il existe au moins un espace vectoriel $G$ supplémentaire de $F$ dans $E$, i.e. tel que $E=F\oplus G$. 

\Remarque : A part lorsque $F=E$ ou $F=\{0\}$, il existe même une infinité d'espaces vectoriels $G$ tels quie $E=F\oplus G$. 
\bigskip

\Theoreme [$F$ et $G$ deux sous-espaces d'un espace vectoriel de dimension finie]
$$
\dim_{\ob K}(F+G)=\dim_{\ob K}(F)+\dim_{\ob K}(G)-\dim_{\ob K}(F\cap G).
$$


\Subsection DimSEV, Rang d'une application linéaire. 
\bigskip

\Theoreme [Title=Théorème du rang;$u:E\to F$ application linéaire]
Si $E$ est un espace vectoriel de dimension finie, alors $\IM(u)$ est un 
sous-espace vectoriel de dimension finie de $F$ et  
$$
\dim E=\dim\Ker(u)+\dim\IM(u).
$$ 


\Remarque. Pour chaque supplémentaire $E'$ de l'espace $\Ker(u)$ dans $E$, i. e. tel que $E=E'\oplus\Ker(u)$, la restriction $\tilde u:E'\to E''$ de l'application $u:E\to F$ à $E'$ au départ et à $\IM(u)$ à l'arrivée est un isomorphisme. 
\bigskip

\Definition []  Soit $u:E\to F$ une application linéaire d'image $\IM(u)$ de dimension finie. 
Alors, on appelle rang de l'application linéaire $u$, et on note $\mbox{rg}(u)$ la dimension de son espace image
$$
{\mbox{rg}(u):=\dim\IM(u)}. 
$$

\Propriete []  Soit $E$ et $F$ deux $\ob K$ espaces vectoriels de même dimension finie $n\ge1$ et soit $u\in\sc L(E,F)$ une application linéaire. Alors, 
$$
u\mbox{ est un isomorphisme}\Longleftrightarrow \mbox{rg}(u)=n
$$

\Propriete []  Soit $E$ et $F$ deux $\ob K$ espaces vectoriels de même dimension finie $n\ge1$ et soit $u\in\sc L(E,F)$ une application linéaire. Alors, 
$$
{
u\mbox{ est bijective}\Longleftrightarrow 
u\mbox{ est injective}\Longleftrightarrow 
u\mbox{ est surjective}}
$$

\Propriete []  On ne change pas le rang d'une application linéaire $v:F\to G$ de rang fini $n\ge1$ en composant à gauche ou/et à droite par un isomorphisme. Autrement dit, si $u:E\to F$ et $w:G\to H$ sont des isomorphismes, les applications linéaires $v\circ u$, $w\circ v$ et $\circ v\circ u$ sont de rang $n$. 
\bigskip



\hautspages{Olus Livius Bindus}{Calcul matriciel}

\pagetitretrue


\Chapter Matrices, Calcul Matriciel. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$ et 
les lettres $n$ et $p$ désignent deux entiers strictement positifs. 
\bigskip

\Section Gen, Opérations algèbriques sur les matrices. 

\Subsection Mat1, Matrices à $n$ lignes et $p$ colonnes. 

\Definition []  Soient $n\ge1$ et $p\ge1$ deux entiers. Une matrice, à $n$ lignes et à $p$ colonnes, d'éléments de $\ob K$ est une famille $a=(a_{i,j})_{1\le i\le n\atop1\le j\le p}\in\ob K^{np}$ déléments de $\ob K$, qui sera représenté sous la forme 
$$
A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}=\underbrace{\Q.\pmatrix{a_{1,1}&a_{1,2}&\ldots&a_{1,p}\cr
a_{2,1}&a_{2,2}&\ldots&a_{2,p}\cr
\vdots&&&\vdots\cr
a_{n,1}&a_{n,2}&\ldots&a_{n,p}
}\W\}}_{p \mbox{ colonnes}} n\mbox{ lignes}
$$
L'ensemble des matrices, à $n$ lignes et à $p$ colonnes, d'éléments de $\ob K$ est noté $\sc M_{n,p}(\ob K)$. 
\bigskip

\Remarque : Les matrices, à $n$ lignes et à $1$ colonne, d'éléments de $\ob K$ sont appelés des vecteurs colonnes à $n$ lignes/composants. 
\bigskip

\Remarque : Les matrices, à $n$ lignes et à $n$ colonnes, d'éléments de $\ob K$ sont appelés matrices carrées, de taille $n$, d'éléments de $\ob K$. Par convention, on note $\sc M_n(\ob K)=\sc M_{n,n}(\ob K)$. 
\bigskip


\Subsection Mat2, Addition des matrices.

\Definition []  Etant données deux matrices $A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}$ et $B=(b_{i,j})_{1\le i\le n\atop1\le j\le p}$ 
de $\sc M_{n,p}(\ob K)$, on note $A+B$ la matrice de $\sc M_{n,p}(\ob K)$ définie par 
$$
A+B:=(a_{i,j}+b_{i,j})_{1\le i\le n\atop1\le j\le p}=\pmatrix{a_{1,1}+b_{1,1}&\ldots&a_{1,p}+b_{1,p}\cr
\vdots&&\vdots\cr
\vdots&a_{i,j}+b_{i,j}&\vdots\cr
\vdots&&\vdots\cr
a_{n,1}+b_{n,1}&\ldots&a_{n,p}+b_{n,p}
}
$$

\Propriete []  L'ensemble $\sc M_{n,p}(\ob K)$ muni de l'addition des matrices $+$ forme un groupe commutatif. 
\bigskip

\Subsection Mat3, Multiplication externe des matrices. 

\Definition []  Etant données une matrice $A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}$ et un nombre $\lambda\in\ob K$, 
on note $\lambda.A$ la matrice de $\sc M_{n,p}(\ob K)$ définie par 
$$
\lambda.A:=(\lambda.a_{i,j})_{1\le i\le n\atop1\le j\le p}=\pmatrix{\lambda a_{1,1}&\ldots&\lambda a_{1,p}\cr
\vdots&&\vdots\cr
\vdots&\lambda a_{i,j}&\vdots\cr
\vdots&&\vdots\cr
\lambda a_{n,1}&\ldots&\lambda a_{n,p}
}
$$

\Propriete []  L'ensemble $\sc M_{n,p}(\ob K)$ muni de l'addition $+$ et de la multiplication externe $.$ des matrices forme un espace vectoriel de dimension 
$$
\dim\sc M_{n,p}(\ob K)=np, 
$$ 
muni de la base canonique constituée par les matrices définies par 
$$
\forall i\in\{1,\cdots, n\},\quad 
\forall j\in\{1,\cdots, p\}, \qquad 
E_{i,j}:=\pmatrix{
0&\ldots&\ldots&0\cr
\vdots&&&\vdots\cr
\vdots&&1%\rput{0}(0,-1.5){\uparrow}\rput{0}(0,-2){\mbox{colonne $j$}}\rlap{\qquad\quad$\longleftarrow$\mbox{ ligne }i} % Fix
it 
 &\vdots\cr \vdots&&&\vdots\cr 0&\ldots&\ldots&0 
 } 
 $$



\Subsection Mat4, Produit Matriciel. 


\Definition []  Soient $m$, $n$ et $p$ trois entiers strictement positifs. 
Etant données deux matrices $A=(a_{i,j})\in \sc M_{m,n}(\ob K)$ et $B=(b_{j,k})\in \sc M_{n,p}(\ob K)$, alors le produit (non commutatif) de la matrice $B$ par la matrice $A$ est la matrice $A\times B=(c_{i,k})$ de $\sc M_{m,p}(\ob K)$ définie par 
$$
A\times B=\Q(\underbrace{\sum_{1\le j\le n}a_{i,j}b_{j,k}}_{c_{i,k}}\W)_{1\le i\le m\atop1\le k\le p}
$$ 

\Remarque : le nombre de colonnes de $A$ DOIT être égal au nombre de lignes de $B$, sinon, le produit $A\times B$ n'a aucun sens. 
On remarque que $1\le i\le m$, $1\le j\le n$ et $1\le k\le p$. 
\bigskip

\Remarque : pour effectuer rapidement un produit de deux matrices, je vous recommande la construction mentale suivante : 
\bigskip
\IGNORE
$$
\eqalign{
&{\blue \Q.\pmatrix{b_{1,1}&\vdots&&\vdots&b_{1,k}\cr
b_{2,1}& \vdots&\rput{-90}(0,1){\rlap{{\blue $\mbox{Colonne $k$}\longrightarrow$}}}&\vdots &b_{2,k}\cr\vdots &\vdots&&\vdots&\vdots\cr b_{n,1}&\vdots&&\vdots&b_{n,k}\cr}\W\}B}
\cr{\red \underbrace{
\pmatrix{a_{1,1}&\ldots&\ldots&a_{1,n}\cr \ldots& \ldots&\ldots&\ldots \cr \rlap{${\red\longrightarrow \mbox{ Ligne $i$ }\longrightarrow}$}\cr \ldots &\ldots&\ldots&\ldots\cr a_{m,1}&a_{m,2}&\ldots&a_{m,n}\cr}}_A}&\underbrace{
\pmatrix{c_{1,1}&\vdots&&\vdots&c_{1,k}\cr
\ldots&&&&\ldots\cr
&&\!\!\!\!\!\!\rlap{\mbox{${\black c_{ i , k }}$}}&&\cr
\ldots&&&&\ldots\cr
c_{m,1}&\vdots&&\vdots&c_{m,k}\cr}}_{C={\red A}\times  B }
}
$$
\IGNORE %Fix here

\Theoreme [Title=associativité du produit matriciel;$m$, $n$, $p$ et $q$ nombres entiers strictements positifs]
$$
\forall A\in\sc M_{m,n}(\ob K), \quad \forall B\in\sc M_{n,p}(\ob K), \quad \forall C\in\sc M_{p,q}(\ob K), \qquad (A\times B)\times C=A\times(B\times C). 
$$

\Propriete [Title=bilinéarité du produit matriciel;$m$, $n$ et $p$ nombres entiers strictement positifs]
$$
\forall (\lambda,\mu)\in \ob K^2, \quad \forall (A,B)\in\sc M_{m,n}(\ob K)^2,\quad\forall C\in\sc M_{n,p}(\ob K), \quad (\lambda A+\mu B)C=\lambda AC+\mu BC.
$$
De même, on a 
$$
\forall (\lambda,\mu)\in \ob K^2, \quad \forall (A,B)\in\sc M_{n,p}(\ob K)^2,\quad\forall C\in\sc M_{m,n}(\ob K), \quad C(\lambda A+\mu B)=\lambda CA+\mu CB.
$$

\Propriete [Title=Loi du scalaire mobile;$m$, $n$ et $p$ nombres entiers strictement positifs] 
$$
\forall \lambda\in \ob K, \quad \forall A\in\sc M_{m,n}(\ob K),\quad\forall B\in\sc M_{n,p}(\ob K), \quad (\lambda A)B=A(\lambda B)=\lambda AB.
$$

\Subsection Mat5, Transposition. 

\Definition []  Soient $n$ et $p$ deux entiers strictement positifs. Alors, la transposée d'une matrice $A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}$ de $\sc M_{n,p}(\ob K)$ 
est la matrice $\NULL^tA:=(a_{j,i})_{1\le i\le p\atop1\le j\le n}$ de $\sc M_{p,n}(\ob K)$. 
\bigskip

\Remarque : plus visuellement, la transposée de la matrice 
$$
A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}=\underbrace{\Q.\pmatrix{a_{1,1}&a_{1,2}&\ldots&a_{1,p}\cr
a_{2,1}&a_{2,2}&\ldots&a_{2,p}\cr
\vdots&&&\vdots\cr
a_{n,1}&a_{n,2}&\ldots&a_{n,p}
}\W\}}_{\mbox{$p$ colonnes}} n\mbox{ lignes}
$$
est la matrice obtenue par symétrie de la matrice $A$ par rapport à sa diagonale principale
$$
\NULL^tA:=(a_{j,i})_{1\le i\le p\atop1\le j\le q}=\underbrace{\Q.\pmatrix{a_{1,1}&a_{2,1}&\ldots&a_{n,1}\cr
a_{1,2}&a_{2,2}&\ldots&a_{n,2}\cr
\vdots&&&\vdots\cr
a_{1,p}&a_{2,p}&\ldots&a_{n,p}
}\W\}}_{\mbox{$n$ colonnes}} p\mbox{ lignes}
$$
Les lignes deviennent colonnes et réciproquement les colonnes deviennent des lignes. 
\bigskip

\Exemple. $\ds\NULL^t\pmatrix{1&2&3\cr4&5&6}=\pmatrix{1&4\cr2&5\cr3&6}$. 
\bigskip


\Propriete []  Soient $m$, $n$ et $p$ des entiers strictements positifs. Alors, 
$$
\eqalign{
	\forall A\in\sc M_{m,n}(\ob K), \qquad \NULL^t(\NULL^tA)=A. \qquad &\mbox{(Involution)}.
	\cr
	\forall (\lambda,\mu)\in\ob K^2, \qquad \forall (A,B)\in\sc M_{m,n}(\ob K)^2, \qquad \NULL^t(\lambda A+\mu B)=\lambda^tA+\mu^tB
	\qquad &\mbox{(linéarité)}
	\cr
	\forall A\in\sc M_{m,n}(\ob K),\quad \forall B\in\sc M_{n,k}(\ob K), \qquad \NULL^t(AB)=\NULL^tB^tA.
}
$$

\Subsection Mat5, Matrices carrées. 

\Remarque : on rappelle que l'ensemble $\sc M_n(\ob K)$ des matrices carrées de taille $n$, muni des opérations $+$ et $\cdot$, forme un $\ob K$-espace vectoriel. 
\bigskip

\Propriete []  Soit $n\in\ob N^*$. Alors, l'addition et le produit matriciel est une loi interne à $\sc M_n(\ob K)$. autrement dit : 
$$
\forall A\in\sc M_n(\ob K), \quad \forall B\in\sc M_n(\ob K), \qquad A\times B\in\sc M_n(\ob K). 
$$ 

\Remarque : la transposée d'une matrice carrée de taille $n$ est une matrice carrée de taille~$n$. 

\Definition []  Pour chaque entier $n\ge1$, on note $I_n$ la matrice contenant des $0$ partout sauf sur sa diagonale principale (qui va du coin en haut à gauche au coin en bas à droite) où elle comporte des $1$. Autrement dit : 
$$
I_n:=\underbrace{\pmatrix{1& 0& \ldots& \ldots& 0\cr
0& 1& \ddots&0 & \vdots\cr
\vdots& \ddots& \ddots& \ddots& \vdots\cr
\vdots&0&\ddots&1&0\cr
0&\ldots&\ldots&0&1}}_{\mbox{$n$ colonnes}}\Q.\rlap{$\phantom{\pmatrix{\vdots\cr\vdots\cr\cr\cr\cr}}$}\W\}\mbox{$n$ lignes}
$$

\Propriete []  Pour chaque entier $n\ge1$, la matrice $I_n$ est l'élément neutre pour le produit matriciel dans $\sc M_n(\ob K)$. 
Autrement dit :
$$
\forall n\ge1, \qquad \forall A\in\sc M_n(\ob K), \qquad I_n\times A=A\times I_n=A.
$$

\Definition []  Soit $n\in\ob N^*$. Une matrice $A\in\sc M_n(\ob K)$ est dite inversible si, et seulement si, il existe une matrice $B\in\sc M_n(\ob K)$ telle que 
$$
A\times B=B\times A=I_n
$$
Une telle matrice $B$ est unique : on la note $A^{-1}$ et on l'appelle matrice inverse de la matrice $A$. 
\bigskip

\Propriete []  Pour $n\in\ob N^*$, L'ensemble des matrices inversibles $M\in_sc M_n(\ob K)$, qui est noté $\sc Gl_n(\ob K)$, forme un groupe pour le produit matriciel $\times$, appelé groupe linéaire. 
\bigskip

\Concept [] Identité algèbrique


\Theoreme [$A$ et $B$ matrices carrées de même taille vérifiant {$AB=BA$}]
$$
\forall n\ge0,\qquad A^n-B^n=(A-B)\sum_{k=0}^{n-1}A^kB^{n-1-k}.
$$

\Remarque : 
cette égalité est vraie pour $n=0$ avec la convention selon laquelle $A^0=B^0=I_n$ et selon laquelle une somme vide est nulle. 
\bigskip

\Theoreme [Title=Binôme de Newton;$A$ et $B$ matrices carrées de même taille vérifiant {$AB=BA$}] 
$$
\forall n\ge0,\qquad (A+B)^n=\sum_{0\le k\le n}{n\choose k}A^kB^{n-k}.
$$

\Remarque. Pour $\lambda\in\ob K$, la matrice $\lambda I_n$ commute avec toutes les matrices carrées de taille $n$. 
\bigskip

\Subsection Mat5, Matrices diagonales et triangulaires. 

\Definition []  Soit $n\in\ob N^*$. Une matrice $M\in\sc m_n(\ob K)$ est dite diagonale si ses coefficients, qui ne sont pas sur la diagonale principale, sont nuls. Ainsi, une matrice $A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}$ est diagonale si, et seulement si, 
$$
i\neq j\quad \Longrightarrow\quad a_{i,j}=0.
$$

\Propriete []  Le produit de deux matrices diagonales est une matrice diagonale. 
$$
\forall (A,B)\in\sc M_n(\ob K)^2, \qquad A\mbox{ et }B \mbox{ sont diagonales }\Longrightarrow AB\mbox{ est diagonale}
$$

Exercice :  Montrer que l'ensemble des matrices carrées diagonales de taille $n$ est un espace vectoriel et déterminer sa dimension. 
\bigskip

\Propriete []  Une matrice diagonale est inversible si, et seulement si, ses termes diagonaux sont tous non nuls. 
\bigskip

\Definition []  Soit $n\in\ob N^*$. Une matrice $M\in\sc M_n(\ob K)$ est dite triangulaire supérieure (resp. inférieure) si ses coefficients en dessous de la diagonale principale (resp. au dessus) sont tous nuls. Ainsi, une matrice $A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}$ est triangulaire supérieure (resp. inférieure) si, et seulement si, 
$$
i> j\qquad \mbox{(resp. $i<j$)}\quad \Longrightarrow\quad a_{i,j}=0.
$$

\Propriete []  Le produit de deux matrices triangulaires supérieures (resp. inférieures) est une matrice 
triangulaire supérieure (resp. inférieure). 
\bigskip

\Propriete []  Une matrice est triangulaire supérieure si, et seulement si sa transposée est triangulaire inférieure.
\bigskip 

Exercice :  Montrer que l'ensemble des matrices carrées triangulaires supérieures de taille $n$ est un espace vectoriel et déterminer sa dimension. 
\bigskip

\Propriete []  Une matrice triangulaire (inférieure ou supérieure) est inversible si, et seulement si touts ses termes diagonaux sont non nuls. 
\bigskip


\Subsection Mat6, Matrices symétriques et anti-symétriques. 


\Definition []  Soit $n\in\ob N^*$. Une matrice $M\in\sc M_n(\ob K)$ est dite symétrique (resp. anti-symétrique) 
si, et seulement si, elle vérifie 
$$
^tM=M\qquad \mbox{(resp. $^tM=-M$)}.
$$ 


\Propriete []  L'ensemble $S$ des matrices symétriques $M\in\sc M_n(\ob K)$ et l'ensemble $A$ des matrices anti-symétriques $M\in\sc M_n(\ob K)$ forment deux espaces vectoriels supplémentaires, i.e. qui vérifient $\sc M_n(\ob K)=A\oplus S$. 
\bigskip



\Subsection Mat7, Trace d'une matrice carrée. 


\Definition []  Soit $n\in\ob N^*$. La trace d'une matrice carré $A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}$ d'éléments de $\ob K$ est la somme de ses coefficients diagonaux (i.e. sur la diagonale principale) 
$$
\mbox{Tr}(A):=\sum_{1\le i\le n}a_{i,i}.
$$ 

\Theoreme [$n\in\ob N^*$]
La trace $\ds \eqalign{\mbox{Tr}: \sc M_n(\ob K)&\to K\cr  A&\mapsto  \mbox{Tr}(A)}$ est une forme linéaire. 

\Propriete []  Soit $n\in\ob N^*$. Alors, on a 
$$
\forall (A,B)\in\sc M_n(\ob K)^2, \qquad \mbox{Tr}(AB)=\mbox{Tr}(BA). 
$$


\Section Gahh, Matrices et applications linéaires. 

\Subsection Mat5, Vecteur. 

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel de dimension $p$ muni d'une base $\sc E:=\{e_1,\cdots, e_p\}$. 
Alors, pour chaque vecteur $x\in E$, il existe une unique famille $(x_i)_{1\le i\le p}$ de scalaires tels que 
$$
x=\sum_{1\le i\le p}x_ie_i\eqdef{decx}
$$
et on lui associe la matrice $\sc Mat_{\sc E}(x):=(x_i)_{1\le i\le p\atop j=1}\in\sc M_{p,1}(\ob K)$, appelée matrice des coordonnées du vecteur $x$ dans la base $\sc E$. 
De plus, l'application définie par 
$$
\eqalign{: E&\to\sc M_{p,1}(\ob K)\cr  x&\mapsto\sc Mat_{\sc E}(x)}
$$
est un isomorphisme. 
\bigskip
\Remarque : pour trouver la matrice $X$ de $x$ sur la base $\sc E$ (c'est à dire $\sc Mat_{\sc E}(x)$), il faut réussir à obtenir la décomposition \eqref{decx} de $x$ et on a alors
$$
X=\pmatrix{x_1\cr x_2\cr x_3\cr \vdots\cr x_p}.
$$

Exercice :  Déterminer la matrice du vecteur $(1,3,2)$ de $\ob R^3$ dans la base $\sc E:=\{(1,1,1),(1,-1,0),(0,1,-1)\}$; 
\bigskip

\Remarque : parfois, on identifiera les matrices colonnes de $\sc M_{n,1}(\ob K)$ avec les vecteurs de l'espace vectoriel $\ob K^n$. 
C'est pratique mais il faut savoir que c'est dangereux d'identifier des vecteurs colonnes à des $n$-uplets 
qu'on écrit en ligne...
\bigskip


\Subsection Mat8, Famille de vecteurs. 

\Definition []  Soit $E$ un $\ob K$-espace vectoriel de dimension $p$ muni d'une base $\sc E:=\{e_1,\cdots, e_p\}$ et soit 
$(v_1,\cdots, v_n)$ une famille finie de $n$ vecteurs de $E$. Pour $1\le j\le n$, on note $V_j:=\sc Mat_{\sc E}(v_j)$ la matrice des coordonnées du vecteur $v_j$ dans la base $\sc E$. Alors, la matrice dans la base $\sc E$ de la famille 
de vecteurs $(v_1,\cdots,v_n)$ est la matrice à $p$ lignes et $n$ colonnes 
$$
\sc Mat_{\sc E}(v_1,\cdots,v_n):=\pmatrix{V_1&V_2&\ldots&V_n}. 
$$

Exercice :  Matrice de la famille 
de vecteurs $\B((1,3,2), (3,2,1), (1,-1,1), (4,2,1)\B)$ de $\ob R^3$ dans la base $\sc E:=\{(1,1,1),(1,-1,0),(0,1,-1)\}$ ? 
\bigskip


\Subsection Mat6, Application linéaire. 

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel de dimension $p$ muni d'une base $\sc E:=\{e_1,\cdots, e_p\}$ 
et soit $F$ un $\ob K$-espace vectoriel de dimension $n$ muni d'une base $\sc F:=\{f_1,\cdots, f_n\}$. Alors, 
pour chaque $u\in\sc L(E,F)$, il existe une unique famille $(a_{i,j})_{1\le i\le n\atop 1\le j\le p}$ de scalaires tels que 
$$
\forall j\in\{1,\cdots, p\}, \qquad u(e_j)=\sum_{1\le i\le n}a_{i,j}f_i.
$$
Cette famille $(a_{i,j})_{1\le i\le n\atop 1\le j\le p}$ définit une unique matrice, appelée matrice de l'application linéaire $u$ associée aux bases $\sc E$ et $\sc F$, qui sera notée $\sc Mat_{\sc E,\sc F}(u)$. De plus, l'application définie par 
$$
\eqalign{\sc L(E,F)&\to\quad\sc M_{n,p}(\ob K)\cr  u&\mapsto\sc Mat_{\sc E,\sc F}(u)}
$$
est un isomorphisme. 
\bigskip

\Remarque : la propriété précédente parait ésotérique et inutile. Pourtant elle est fondamentale. En effet, elle permet de transformer un problème théorique portant sur des applications linéaires en problème calculatoire et concret portant sur des matrices. 
\bigskip

\Remarque : En bref, le fait de fixer une base $\sc E$ de $E$ et une base $\sc F$ de $F$ permet d'associer de manière 
{\bf UNIQUE} à chaque application linéaire $u:E\to F$ une matrice $U$ de $\sc M_{n,p}(\ob K)$ et réciproquement. 
\bigskip


\Remarque  : L'application précédente étant un isomorphisme, on a bien sur 
$$
\forall (u,v)\in\sc L(E,F)^2, \qquad \forall(\lambda,\mu)\in\ob K^2, \qquad \sc Mat_{\sc E,\sc F}(\lambda u+\mu v)=\lambda\sc Mat_{\sc E,\sc F}(u)+\mu\sc Mat_{\sc E,\sc F}(v). 
$$
\bigskip

\Remarque  : Si $E$ est un $\ob K$ espace vectoriel de dimension finie $n$, muni d'une base $\sc E$, l'application 
$$
\eqalign{\sc L(E)&\to\sc M_n(\ob K)\cr  u&\mapsto \sc Mat_{\sc E}(u)}
$$
est un isomorphisme, où $\sc Mat_{\sc E}(u)$ 
désigne la matrice $\sc Mat_{\sc E,\sc E}(u)$ (cela revient à prendre $F=E$ et $\sc F=\sc E$ dans le cas précédent). 
\bigskip

\Remarque : Pour bien comprendre et pour calculer correctement la matrice d'une application linéaire $u:E\to F$ dans les bases $\sc E$ et $\sc F$ de $E$ et de $F$, je vous recommande la présentation suivante (ce qui est en couleur restera au brouillon) : 

\IGNORE
$$
\sc Mat_{\sc E,\sc F}(u)=\vbox{
{\blue \mbox{\qquad $\!\!\overbrace{e_1\quad\ e_2 \quad\ \ldots \quad e_j\quad \ldots\quad\ e_p}^{\mbox{Base $\sc E$ de $E$}}$}}
\mbox{$\pmatrix{a_{1,1}&a_{1,2}&\ldots&\ \ 
\rput{90}(0,-1){
\mbox{decomp. de $u(e_j)$}}
\ \ &\ldots&a_{1,p}\cr
a_{2,1}&a_{2,2}&\ldots&&\ldots&a_{2,p}\cr
a_{3,1}&a_{3,2}&\ldots&&\ldots&a_{3,p}\cr
\vdots&\vdots&&&&\vdots\cr
a_{n,1}&a_{n,2}&\ldots&&\ldots&a_{n,p}
}$}
}{\red \Q.\vcenter{\mbox{$f_1$}\mbox{$f_2$}\mbox{$f_3$}\mbox{\vdots}\mbox{$f_n$}}\W\}\mbox{Base $\sc F$ de $F$}}
$$
\IGNORE

Exercice :  Déterminer la matrice dans les bases canoniques de $\ob R^3$ et $\ob R^2$ de l'application $u:\ob R^3\to\ob R^2$ définie par 
$$
\forall (x,y,z)\in\ob R^3, \qquad u(x,y,z):=\Q(x+3y-2z,4x-z\W).
$$
Même exercice dans les bases $\sc E=\{(1,1,1),(1,-1,0),(0,1,-1)\}$ et $\sc F=\{(1,1),(1,-1)\}$. 
\bigskip 

\Propriete []  Soient $E$ et $F$ des $\ob K$-espace vectoriel de dimension finie muni debase $\sc E$ et $\sc F$ et soit $u\in\sc L(E,F)$. Alors, on a 
$$
\forall x\in E, \qquad \sc Mat_{\sc F}\B(u(x)\B)=\sc Mat_{\sc E,\sc F}(u)\times \sc Mat_{\sc E}(x).
$$ 

\Remarque : Autrement dit, si $X$ est la matrice des coordonnéees de $x\in E$ dans la base~$\sc E$, si $U$ est la matrice de l'endomorphisme $u$ dans les bases $\sc E$ et $\sc F$ et si $Y$ est la matrice 
des coordonnées de $y\in F$ dans la base $\sc F$, on a 
$$
y=u(x) \Longleftrightarrow Y=UX.
$$

\Remarque : pour schématiser, on a 
$$
%\psset{xunit=1cm,yunit=1cm}
\pspicture*[](-4.5,-1.8)(6.5,0.5)
\rput{0}(-1.5,0){\llap{Espace vectoriel}}
\rput{0}(-1.5,-.5){\llap{Base}}
\rput{0}(-1.5,-1){\llap{Vecteur}}
\rput{0}(-1.5,-1.5){\llap{Matrice}}
\rput{0}(0,0){$F$}
\rput{0}(0,-.5){$\sc F$}
\rput{0}(0,-1){$u(x)$}
\rput{0}(0,-1.5){$\sc Mat_{\sc F}u(x)$}
\psline[linewidth=.3pt]{<-}(1,0)(5,0)
\rput{0}(3,.3){$u$}
\rput{0}(3,-1.5){$\sc Mat_{\sc E,\sc F}(u)$}
\rput{0}(5.5,0){$E$}
\rput{0}(5.5,-.5){$\sc E$}
\rput{0}(5.5,-1){$x$}
\rput{0}(5.5,-1.5){$\sc Mat_{\sc E}(x)$}
\rput{0}(1.5,-1.5){$=$}
\rput{0}(4.2,-1.5){$\times$}
\endpspicture
$$ 
\medskip


\Theoreme [Title=Matrice d'une composée;$u:E\to F$ et $v:F\to G$ applications linéaires et $\sc E$, $\sc F$ et $\sc G$ bases respectives de $E$, $F$ et $G$]
$$
\sc Mat_{\sc E,\sc G}(v\circ u)=\sc Mat_{\sc F,\sc G}(v)\times \sc Mat_{\sc E,\sc F}(u).
$$ 

\Remarque : pour schématiser, on a 
$$
%\psset{xunit=1cm,yunit=1cm}
\pspicture*[](-6.5,-5.8)(6.5,.5)
\rput{0}(0,0){$F$}
\rput{0}(0,-.5){$\sc F$}
\rput{0}(0,-1){$u(x)$}
\psline[linewidth=.3pt]{->}(-0.5,-1.5)(-3,-4.5)
\rput{0}(-4,-4){$G$}
\rput{0}(-4,-4.5){$\sc G$}
\rput{0}(-4,-5){$v\circ u(x)$}
\psline[linewidth=.3pt]{<-}(-3,-5)(3,-5)
\rput{0}(0,-4.7){$v\circ u$}
\rput{0}(0,-5.5){$\sc Mat_{\sc E,\sc G}(v\circ u)$}
\rput{0}(4,-4){$E$}
\rput{0}(4,-4.5){$\sc E$}
\rput{0}(4,-5){$x$}
\psline[linewidth=.3pt]{<-}(0.5,-1.5)(3,-4.5)
\rput{0}(1,-3){$u$}
\rput{0}(2.5,-2.5){$\sc Mat_{\sc E,\sc F}(u)$}
\rput{0}(-1,-3){$v$}
\rput{0}(-2.5,-2.5){$\sc Mat_{\sc F,\sc G}(v)$}
\endpspicture
$$ 
\medskip

\Subsection Mat8, Famille de formes linéaires. 

\Definition []  Soit $E$ un $\ob K$-espace vectoriel de dimension $p$ muni d'une base $\sc E:=\{e_1,\cdots, e_p\}$ et soit 
$(f_1,\cdots, f_n)$ une famille finie de $n$ vecteurs formes linéaires de de $E$. Autrement dit, 
$$
\forall i\in\{1,\cdots, n\}, \qquad f_i\in\sc L(E,\ob K)
$$
Pour $1\le j\le n$, on note $F_i:=\sc Mat_{\sc E,\sc {Bc}}(f_i)$ la matrice (ligne) de la forme linéaire $f_i:E\to\ob K$ 
dans la base $\sc E$ et la base canonique $\sc {BC}$ de $\ob K$. Alors, la matrice dans la base $\sc E$ de la famille 
de formes linéaires $(f_1,\cdots,f_n)$ est la matrice à $n$ lignes et $p$ colonnes 
$$
\sc Mat_{\sc E}(f_1,\cdots,f_n):=\pmatrix{F_1\cr F_2\cr\vdots\cr F_n}. 
$$

Exercice :  Matrice dans la base $\sc E:=\{(1,1,1,1),(1,1,1,0),(1,1,0,0),(1,0,0,0)\}$ de la famille de formes linéaires de $\ob R^4$ 
$$
f_1:(x,y,z,t)\mapsto 2x+3y-z+t, \qquad f_2:(x,y,z,t)\mapsto y+z+t, \qquad f_3:(x,y,z,t)\mapsto t.  
$$


\Subsection Mat11, Matrices de passage. 

\Definition []  Soit $E$ un espace vectoriel de dimension finie $n$ muni de deux bases $\sc A$ (comme ancienne) et $\sc N$ (comme nouvelle). Alors, La matrice de passage de la base $\sc A$ à la base $\sc N$ de l'espace vectoriel $E$ est la matrice de la (nouvelle) base $\sc N$ décomposée sur (l'ancienne) base $\sc A$. En particulier, c'est la matrice 
$$
\mbox{Matrice de passage de la base $\sc A$ à la base $\sc N$ de l'espace vectoriel $E$}=\sc Mat_{\sc N,\sc A}\mbox{Id}_E.
$$ 

\Propriete []  Soit $E$ un espace vectoriel de dimension finie $n$ muni de deux bases $\sc A$ et $\sc N$. alors, on a 
$$
\underbrace{\sc Mat_{\sc A, \sc N}}_{Mat. \sc N\to\sc A}\times\underbrace{\sc Mat_{\sc N,\sc A}}_{Mat. \sc A\to\sc N}=I_n
$$
En particulier, l'inverse de la matrice de passage de la base $\sc A$ à la base $\sc N$ est la matrice de passage de la base $\sc N$ à la base $\sc A$. 
\bigskip

\Concept [] Effet du changement de base sur la matrice d'un vecteur. 

\Propriete []  Soit $E$ un espace vectoriel de dimension finie $n$ muni de deux bases $\sc A$ et $\sc N$ et soit $x$ un vecteur de $E$. alors, on a 
$$
\sc Mat_{\sc N}(x)=\underbrace{\sc Mat_{\sc A, \sc N}\mbox{Id}_E}_{\mbox{Mat. $\sc N\to\sc A$}}\times\quad \sc Mat_{\sc A}(x).
$$

\Remarque : Pour schématiser, on a  : 
$$
%\psset{xunit=1cm,yunit=1cm}
\pspicture*[](-4.5,-1.8)(6.5,0.5)
\rput{0}(-1.5,0){\llap{Espace vectoriel}}
\rput{0}(-1.5,-.5){\llap{Base}}
\rput{0}(-1.5,-1){\llap{Vecteur}}
\rput{0}(-1.5,-1.5){\llap{Matrice}}
\rput{0}(0,0){$E$}
\rput{0}(0,-.5){$\sc N$}
\rput{0}(0,-1){$x$}
\rput{0}(0,-1.5){$\sc Mat_{\sc N}(x)$}
\psline[linewidth=.3pt]{<-}(1,0)(5,0)
\rput{0}(3,.3){Id$_E$}
\rput{0}(3,-1.5){$\sc Mat_{\sc A,\sc N}\mbox{Id}_E$}
\rput{0}(5.5,0){$E$}
\rput{0}(5.5,-.5){$\sc A$}
\rput{0}(5.5,-1){$x$}
\rput{0}(5.5,-1.5){$\sc Mat_{\sc A}(x)$}
\rput{0}(1.5,-1.5){$=$}
\rput{0}(4.2,-1.5){$\times$}
\endpspicture
$$ 
\medskip


\Concept [] Effet du changement de base sur la matrice d'une application linéaire. 

\Propriete []  Soit $E$ un espace vectoriel de dimension finie $n$ muni de deux bases $\sc E_a$ (ancienne) et $\sc E_n$ (nouvelle), 
Soit $F$ un espace vectoriel de dimension finie $p$ muni de deux bases $\sc F_a$ (ancienne) et $\sc F_n$ (nouvelle) 
et soit $u\in\sc L(E,F)$ une application linéaire. Alors, on a 
$$
\sc Mat_{\sc E_n, \sc F_n}u=\underbrace{\sc Mat_{\sc F_a, \sc F_n}\mbox{Id}_F}_{\mbox{Mat. $\sc F_n\to\sc F_a$}}\times\sc Mat_{\sc E_a, \sc F_a}u\times\underbrace{\sc Mat_{\sc E_n, \sc E_a}\mbox{Id}_E}_{\mbox{Mat. $\sc E_a\to\sc E_n$}}.
$$

\Remarque : Pour schématiser, on a : 
$$
%\psset{xunit=1cm,yunit=1cm}
\pspicture*[](-6.5,-5.5)(6.5,.5)
\rput{0}(-4,0){$F$}
\rput{0}(-4,-.5){$\sc F_a$}
\rput{0}(-4,-1){$u(x)$}
\psline[linewidth=.3pt]{<-}(-3,0)(3,0)
\rput{0}(0,.3){u}
\rput{0}(0,-.5){$\sc Mat_{\sc E_a,\sc F_a}u$}
\rput{0}(4,0){$E$}
\rput{0}(4,-.5){$\sc E_a$}
\rput{0}(4,-1){$x$}
\psline[linewidth=.3pt]{->}(-4,-1.5)(-4,-3.5)
\rput{0}(-4,-4){$F$}
\rput{0}(-4,-4.5){$\sc F_n$}
\rput{0}(-4,-5){$u(x)$}
\psline[linewidth=.3pt]{<-}(-3,-4)(3,-4)
\rput{0}(0,-3.7){u}
\rput{0}(0,-4.5){$\sc Mat_{\sc E_n,\sc F_n}u$}
\rput{0}(4,-4){$E$}
\rput{0}(4,-4.5){$\sc E_n$}
\rput{0}(4,-5){$x$}
\psline[linewidth=.3pt]{<-}(4,-1.5)(4,-3.5)
\rput{0}(3.5,-2.5){$\mbox{Id}_E$}
\rput{0}(5.2,-2.5){$\sc Mat_{\sc E_n,\sc E_a}\mbox{Id}_E$}
\rput{0}(-3.5,-2.5){$\mbox{Id}_F$}
\rput{0}(-5.2,-2.5){$\sc Mat_{\sc F_a,\sc F_n}\mbox{Id}_F$}
\endpspicture
$$ 
\medskip

\Section Gah2, Opérations élémentaires sur les matrices. 

\Subsection OPL, Opérations sur les lignes. 

\Concept [] Addition d'un multiple d'une ligne à une autre ligne

\Propriete []  Soient $n\ge1$ et $p\ge1$ des nombres entiers et soit $M\in\sc M_{n,p}(\ob K)$ une matrice. 
Pour faire l'opération $ L_i \leftarrow  L_i + \alpha L_j $, 
c'est à dire pour ajouter à la ligne $L_i$ le produit d'un scalaire $\alpha\in\ob K$ par la ligne $L_j$ de la matrice $M$, il suffit de multiplier la~matrice $M$ {\bf à gauche} par la matrice 
\IGNORE
$$
I_n+\alpha E_{i,j}=\overbrace{\Q.\pmatrix{1&0&\ldots&\ldots&0\cr
0&\ddots&\ddots&1\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,-0.1)(-0.1,-1.8)\rput{0}(0,-2){{\blue\mbox{colonne $j$}}}\llap{{\red\mbox{ ligne }i}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(0.1,0.1)(2.6,0.1)\qquad\qquad\qquad\qquad}&\vdots\cr
\vdots&\ddots&\ddots&\ddots&\vdots\cr
\vdots&&\ddots&\ddots&0\cr
0&\ldots&\ldots&0&1
}\W\}}^{\mbox{$n$ colonnes}}\mbox{$n$ lignes}
$$
\IGNORE%fix here
\bigskip

\Remarque : Pour effecturer l'opération $L_i\leftarrow L_i+\sum_{j\neq i}\alpha_jL_j$, c'est à dire pour ajouter 
à la ligne $L_i$ une combinaison linéaire des autres lignes, il suffit de multiplier la matrice~$M$ {\bf à gauche} par la matrice 
\IGNORE
$$
I_n+\sum_{\ss1\le j\le n\atop\ss j\neq i}\alpha_jE_{i,j}=\underbrace{\Q.\pmatrix{1&0&\ldots&\ldots&\ldots&\ldots&0\cr
0&1&0&\ldots&\ldots&\ldots&0\cr
\vdots&\ddots&\ddots&\ddots&&&\vdots\cr
\alpha_1&\ldots&\alpha_{i-1}&1&\alpha_{i+1}&\ldots&\alpha_n\cr
\vdots&&&\ddots&\ddots&\ddots&\vdots\cr
\vdots&&&&0&1&0\cr
0&0&\ldots&\ldots&0&0&1
}\W\}}_{\mbox{$n$ colonnes}}\raise2em\rlap{$n$ lignes}{\red\leftarrow\mbox{ligne $i$}}
$$      
\IGNORE%Fix here

\Remarque : Cette matrice est aussi égale au produit (commutatif) $\ds\prod_{\ss1\le j\le n\atop j\neq i}(I_n+\alpha_jE_{i,j})$. 
\bigskip

\Concept [] Multiplication d'une ligne par un scalaire

\Propriete []  Soient $n\ge1$ et $p\ge1$ des nombres entiers et soit $M\in\sc M_{n,p}(\ob K)$ une matrice. 
Pour faire l'opération $ L_i \leftarrow  \alpha L_i $, 
c'est à dire pour multiplier la ligne $L_i$ par le scalaire $\alpha\in\ob K$, il suffit de multiplier la~matrice $M$ {\bf à gauche} par la matrice 
\IGNORE$$
I_n+(\alpha-1) E_{i,i}=\overbrace{\Q.\pmatrix{
1&0&\ldots&\ldots&\ldots&\ldots&0\cr
0&\ddots&\ddots&&&&\vdots\cr
\vdots&\ddots&1&\ddots&&&\vdots\cr
\vdots&&\ddots&\alpha\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,-0.1)(-0.1,-1.8)\rput{0}(0,-2){{\blue\mbox{colonne $i$}}}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(0,0.1)(2.6,0.1)\rlap{\qquad\qquad\qquad\qquad{\red\mbox{ ligne }i}}&\ddots&&\vdots\cr
\vdots&&&\ddots&1&\ddots&0\cr
\vdots&&&&\ddots&\ddots&0\cr
0&\ldots&\ldots&\ldots&\ldots&0&1
}\W\}}^{\mbox{$n$ colonnes}}\raise2em \mbox{$n$ lignes}
$$\IGNORE%Fix here
\bigskip


\Concept [] Multiplication de chaque ligne par un scalaire


\Remarque : Pour effectuer simultanément les opérations $L_i\leftarrow \alpha L_i (1\le i\le n)$, qui commuttent, 
c'est à dire pour multiplier la ligne $L_i$ par $\alpha_i$ pour $1\le i\le n$, il suffit de multiplier la matrice~$M$ {\bf à gauche} par la matrice 
$$
I_n+\sum_{1\le i\le n}(\alpha_i-1) E_{i,i}=\sum_{1\le i\le n}\alpha_iE_{i,i}=\pmatrix{
\alpha_1&0&\ldots&0\cr
0&\alpha_2&\ddots&\vdots\cr
\vdots&\ddots&\ddots&0\cr
0&\ldots&0&\alpha_n}
$$

\Concept [] Echange de lignes

\Propriete []  Soient $n\ge1$ et $p\ge1$ des nombres entiers et soit $M\in\sc M_{n,p}(\ob K)$ une matrice. 
Pour faire l'opération $ L_i \leftrightarrow  L_j $, 
c'est à dire échanger la ligne $L_i$ et la ligne $L_j$, il suffit de multiplier la~matrice $M$ {\bf à gauche} par la matrice 
\IGNORE$$
I_n-E_{i,i}-E_{j,j}+E_{i,i}+E_{j,i}=\overbrace{\Q.\pmatrix{
1&0&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&0\cr
0&\ddots&\ddots&&&&&&&&\vdots\cr
\vdots&\ddots&1&\ddots&&&&&&&\vdots\cr
\vdots&&\ddots&0\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,-0.1)(-0.1,-4)\rput{0}(0,-4.2){{\blue\mbox{colonne $i$}}}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(-2.4,0.1)(2.8,0.1)\llap{{\red\mbox{ ligne }i}\qquad\qquad\qquad\qquad}&\ddots&&&1&&&\vdots\cr
\vdots&&&\ddots&1&\ddots&&&&&\vdots\cr
\vdots&&&&\ddots&\ddots&\ddots&&&&\vdots\cr
\vdots&&&&&\ddots&1&\ddots&&&\vdots\cr
\vdots&&&1&&&\ddots&0\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,2.2)(-0.1,-1.7)\rput{0}(-0,-1.9){{\blue\mbox{colonne $j$}}}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(-5.5,0.1)(-0.1,0.1)\llap{{\red\mbox{ ligne }j}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ }&\ddots&&\vdots\cr
\vdots&&&&&&&\ddots&1&\ddots&\vdots\cr
\vdots&&&&&&&&\ddots&\ddots&0\cr
0&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&0&1
}\!\!\W\}}^{\mbox{$n$ colonnes}}\raise2em \rlap{$n$}\raise1em\rlap{lignes}
$$    \IGNORE%Fix here
\bigskip


\Subsection OPL, Opérations sur les colonnes. 

\Concept [] Addition d'un multiple d'une colonne à une autre colonne

\Propriete []  Soient $n\ge1$ et $p\ge1$ des nombres entiers et soit $M\in\sc M_{n,p}(\ob K)$ une matrice. 
Pour faire l'opération $ C_i \leftarrow  C_i + \alpha C_j $, 
c'est à dire pour ajouter à la colonne $C_i$ le produit d'un scalaire $\alpha\in\ob K$ par la colonne $c_j$ de la matrice $M$, il suffit de multiplier la~matrice $M$ {\bf à droite} par la matrice 
$$                  \IGNORE
I_p+\alpha E_{j,i}=\overbrace{\Q.\pmatrix{1&0&\ldots&\ldots&0\cr
0&\ddots&\ddots&1\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,-0.1)(-0.1,-1.8)\rput{0}(0,-2){{\blue\mbox{colonne $i$}}}\llap{{\red\mbox{ ligne }j}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(0.1,0.1)(2.6,0.1)\qquad\qquad\qquad\qquad}&\vdots\cr
\vdots&\ddots&\ddots&\ddots&\vdots\cr
\vdots&&\ddots&\ddots&0\cr
0&\ldots&\ldots&0&1
}\W\}}^{\mbox{$p$ colonnes}}\mbox{$p$ lignes}
\IGNORE%fix here 
$$
\bigskip

\Remarque : Pour effecturer l'opération $C_i\leftarrow C_i+\sum_{j\neq i}\alpha_jC_j$, c'est à dire pour ajouter 
à la colonne $C_i$ une combinaison linéaire des autres colonnes, il suffit de multiplier la~matrice~$M$ {\bf à droite} par la matrice 
\IGNORE$$
I_p+\sum_{\ss1\le j\le p\atop\ss j\neq i}\alpha_jE_{j,i}=\overbrace{\Q.\pmatrix{
1&0&\ldots&\alpha_1&\ldots&\ldots&0\cr
0&1&0&\ldots&\ldots&\ldots&0\cr
\vdots&\ddots&\ddots&\alpha_{i-1}&&&\vdots\cr
\vdots&&\ddots&1\rput{0}(0,-2){{\red\uparrow}}\rput{0}(0,-2.2){{\red\mbox{Colonne $i$}}}&\ddots&&\vdots\cr
\vdots&&&\alpha_{i+1}&\ddots&\ddots&\vdots\cr
\vdots&&&\ldots&0&1&0\cr
0&0&\ldots&\alpha_p&0&0&1
}\W\}}^{\mbox{$p$ colonnes}}\mbox{$p$ lignes}
$$    \IGNORE%Fix here
\bigskip


\Remarque : Cette matrice est aussi égale au produit (commutatif) $\ds\prod_{\ss1\le j\le p\atop j\neq i}(I_n+\alpha_jE_{j,i})$. 
\bigskip

\Concept [] Multiplication de chaque colonne par un scalaire


\Remarque : Pour effectuer simultanément les opérations $C_i\leftarrow \alpha C_i (1\le i\le n)$, qui commuttent, 
c'est à dire de multiplier la colonne $C_i$ par $\alpha_i$ pour $1\le i\le n$, 
il suffit de multiplier la matrice~$M$ {\bf à droite} par la matrice 
$$
I_n+\sum_{1\le i\le n}(\alpha_i-1) E_{i,i}=\sum_{1\le i\le n}\alpha_iE_{i,i}=\pmatrix{
\alpha_1&0&\ldots&0\cr
0&\alpha_2&\ddots&\vdots\cr
\vdots&\ddots&\ddots&0\cr
0&\ldots&0&\alpha_n}
$$

\Concept [] Echange de Colonnes

\Propriete []  Soient $n\ge1$ et $p\ge1$ des nombres entiers et soit $M\in\sc M_{n,p}(\ob K)$ une matrice. 
Pour faire l'opération $ C_i \leftrightarrow  C_j $, 
c'est à dire échanger la colonne $C_i$ et la colonne $C_j$, il suffit de multiplier la~matrice $M$ {\bf à droite} par la matrice 
\IGNORE$$
I_n-E_{i,i}-E_{j,j}+E_{i,i}+E_{j,i}=\overbrace{\Q.\pmatrix{
1&0&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&0\cr
0&\ddots&\ddots&&&&&&&&\vdots\cr
\vdots&\ddots&1&\ddots&&&&&&&\vdots\cr
\vdots&&\ddots&0\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,-0.1)(-0.1,-4)\rput{0}(0,-4.2){{\blue\mbox{colonne $i$}}}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(-2.4,0.1)(2.8,0.1)\llap{{\red\mbox{ ligne }i}\qquad\qquad\qquad\qquad}&\ddots&&&1&&&\vdots\cr
\vdots&&&\ddots&1&\ddots&&&&&\vdots\cr
\vdots&&&&\ddots&\ddots&\ddots&&&&\vdots\cr
\vdots&&&&&\ddots&1&\ddots&&&\vdots\cr
\vdots&&&1&&&\ddots&0\psline[linecolor=blue, linestyle=dashed,dash=4pt 9pt,linewidth=1pt]{-}(-0.1,2.2)(-0.1,-1.7)\rput{0}(-0,-1.9){{\blue\mbox{colonne $j$}}}\psline[linecolor=red, linestyle=dashed, dash=4pt 9pt,linewidth=1pt]{-}(-5.5,0.1)(-0.1,0.1)\llap{{\red\mbox{ ligne }j}\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\ }&\ddots&&\vdots\cr
\vdots&&&&&&&\ddots&1&\ddots&\vdots\cr
\vdots&&&&&&&&\ddots&\ddots&0\cr
0&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&\ldots&0&1
}\!\!\W\}}^{\mbox{$n$ colonnes}}\raise2em \rlap{$n$}\raise1em\rlap{lignes}
$$    \IGNORE%Fix here
\bigskip


\Subsection OPL, Algorythme d'inversion d'une matrice inversible.

\Remarque : Si la matrice $M$ est inversible, on la transforme en lui appliquant les opérations élémentaires pour obtenir la matrice identié et la matrice obtenue en faisant subir les mêmes opérations élémentaires (dans le même ordre) à la matrice unité est la matrice~$M^{-1}$ inverse de $M$. 
$$
I_n=E_p\times E_{p-1}\times\cdots E_2\times E_1\times M\quad\Longrightarrow\quad M^{-1}=E_p\times E_{p-1}\times\cdots E_2\times E_1\times I_n.
$$


\Section Gah3, Rang d'une matrice. 


\Subsection DARR, Définition. 

\Definition []  Soient $n$ et $p$ deux nombres entiers stricetement positifs. Alors, le rang d'une matrice $M\in\sc M_{n,p}(\ob K)$ est le rang de la famille constituée par ses colonnes $C_1, \cdots, C_p$ dans l'espace vectoriel des colonnes à $n$ lignes $\sc M_{n,1}(\ob K)$, c'est à dire la dimension de l'espace vectoriel engendré par cette famille.
$$
\mbox{Rg}(M)=\mbox{Rg}(C_1,\cdots, C_p)=\mbox{Dim}\Vect(C_1,\cdots, C_p) 
$$

\Propriete []  Soient $E$ et $F$ des $\ob K$-espaces vectoriels de dimensionf finie munis des bases $\sc E=\{e_1,\cdots,e_p\}$ et $\sc F$, 
soit $u\in\sc L(E,F)$ une application linéaire de matrice $U=\sc Mat_{\sc E,\sc F}(u)$. Alors, on a 
$$
\mbox{rg}(U)=\mbox{rg}(u)=\mbox{rg}\b(u(e_1),\cdots, u(e_p)\b)=\dim\IM(u).
$$

\Propriete []  Soit $n\in\ob N^*$. alors, une matrice $M\in\sc M_n(\ob K)$ est inversible si, et seulement si, elle est de rang $n$. 
$$
M\in\sc M_n(\ob K)\mbox{ est inversible}\quad\Longleftrightarrow\quad \mbox{rg}(M)=n. 
$$

\Propriete []  Le rang d'une matrice $M\in\sc M_{n,p}(\ob K)$ ne change pas si on la multiplie par une matrice carrée inversible (de taille $n$ à gauche et de taille $p$ à droite). 
\bigskip

\Theoreme  [$n$ et $p$ entiers strictements positifs]
$$
\forall M\in\sc M_{n,p}(\ob K), \qquad \mbox{rg}(M)=\mbox{rg}(\NULL^tM).
$$
Le rang d'une matrice est égal au rang de sa transposée.

\Propriete [$n$ et $p$ entiers strictements positifs]
$$
\forall M\in\sc M_{n,p}(\ob K),\qquad \mbox{rg}(M)\le \min(n,p).
$$

\Propriete []  Si une matrice $M$ contient une sous-matrice de rang $r$, alors $\mbox{rg}(M)\ge r$. 
\bigskip

\Propriete []  Une matrice $M\in\sc M_{n,p}$ est de rang $r$ si et seulement si il existe $P\in\sc Gl_n(\ob K)$ et $Q\in\sc Gl_p(\ob K)$ tel que 
\IGNORE$$
P\times M\times Q=J_r:=\pmatrix{
1\rput{0}(0.6,0.5){\overbrace{\qquad\qquad\quad}^{r}}&0&\ldots&\ldots&\ldots&0\cr
0&\ddots&\ddots&&&\vdots\cr
\vdots&\ddots&1&\ddots&&\vdots\cr
\vdots&&\ddots&0&\ddots&\vdots\cr
\vdots&&&\ddots&\ddots&0\cr
0&\ldots&\ldots&\ldots&0&0\cr
}
$$    \IGNORE%Fix here
la matrice $J_r=(\alpha_{i,j})$ étant définie par les relations
$$
\alpha_{i,j}:=\Q\{
\eqalign{&1\mbox{ si $i=j\le r$}\cr
&0\mbox{ sinon}}
\W.
$$

\Remarque : pour calculer le rang d'une matrice $M$, on peut la transformer à l'aide des opérations élémentaires sur les lignes ET les colonnes afin d'obtenir une matrice du type $J_r$. Le rang de la matrice est alors le nombre entier $r$ obtenu pour la matrice de type $J_r$. 
\bigskip

%\IGNORE
\Section Gah4, Système d'équations linéaires. 

\Definition []  Soient $n\ge1$ et $p\ge1$ deux entiers et soit $A=(a_{i,j})_{1\le i\le n\atop1\le j\le p}\in\ob K^{np}$. Un système d'équations linéaires à $n$ équations lignes 
et à $p$ inconnues, avec second membre, est un système du type 
$$
\Q\{\eqalign{
&a_{1,1}x_1+a_{1,2}x_2+\ldots+a_{1,p}x_p=b_1\cr
&a_{2,1}x_1+a_{2,2}x_2+\ldots+a_{2,p}x_p=b_2\cr
&\quad\vdots\qquad\qquad\vdots\qquad\qquad\qquad\vdots\qquad\ \vdots
\cr
&a_{n,1}x_1+a_{n,2}x_2+\ldots+a_{n,p}x_p=b_p
}\W.\leqno{(E)}
$$
Etant données les matrices $X:=(x_i)_{1\le i\le p}$ et $B:=(b_i)_{1\le i\le p}$, 
ce système peut se mettre sous la forme matricielle 
$$
AX=B.\leqno{(E)}
$$ 


\Definition []  Le système d'équations linéaires homogène associés au système $E$ est 
$$
\Q\{\eqalign{
&a_{1,1}x_1+a_{1,2}x_2+\ldots+a_{1,p}x_p=0\cr
&a_{2,1}x_1+a_{2,2}x_2+\ldots+a_{2,p}x_p=0\cr
&\quad\vdots\qquad\qquad\vdots\qquad\qquad\qquad\vdots\qquad\ \vdots
\cr
&a_{n,1}x_1+a_{n,2}x_2+\ldots+a_{n,p}x_p=0
}\W.\leqno{(H)}
$$
Ce système peut se mettre sous la forme matricielle 
$$
AX=0\leqno{(H)}
$$ 
\bigskip

\Theoreme 
L'ensemble des solutions $\sc S_H$ du système homogène $(H)$, est le noyau de l'endomorphisme $\ds\eqalign{\phi: \sc M_{p,1}(\ob K)&\to\sc M_{p,1}(\ob K)\cr X&\mapsto  AX}$. Ainsi, $\sc S$ est un sous espace vectoriel de $\sc M_{p,1}(\ob K)$, 
de dimension 
$$
\dim\sc S_H=p-\mbox{rg}(A). \leqno{(*)}
$$
\bigskip

\Remarque : le nombre $\mbox{rg}(A)$ est également appelé rang des systèmes $(H)$ et $(E)$. On a bien sur 
$$
0\le \mbox{rg}(A)\le \min(p,n).
$$
Ce rang reprèsente le nombre de lignes et de colonnes linéairement indépendantes de la matrice $A$. En particulier, le rang $\mbox{rg}(A)$ indique le nombre des équations linéaires du système qui sont linéairement indépendantes. On peut alors interpreter 
$(*)$ comme suit : 
$$
\eqalign{&\qquad\mbox{nb de libertés}\cr&\mbox{des solutions du système}}=\mbox{nb d'inconnues}-\eqalign{&\qquad\mbox{nb d'équations }\cr&\mbox{linéairement indépendantes}}.
$$
\bigskip


\Theoreme  L'ensemble des solutions $\sc S_E$ du système avec second membre $(E)$ est : \pn
$\sc S_E=\{X_0+X: X\in\sc S_H\}$ s'il existe $X_0\in\sc M_{p,1}(\ob K)$ tel que $AX_0=B$ (si $B\in\IM(\phi)$).\pn
$\sc S_E=\emptyset$ sinon, c'est-à-dire si $B\notin \IM(\phi)$. 
\bigskip

\Propriete [Title=système de Cramer] 
Si $n=p=\mbox{rg}(A)$, on dit que les systèmes $H$ et $E$ sont de Cramer. Ils admettent alors chacun une unique solution : $\sc S_H=\{0\}$ et $\sc S_E=\{A^{-1}B\}$. En effet, la matrice $A$ est alors carrée et inversible (car de rang égal à sa taille) et on a 
$$
AX=0\Longleftrightarrow X=0\qquad\mbox{et}\qquad AX=B\Longleftrightarrow X=A^{-1}B.
$$

\Remarque : Lorsque le système n'est pas de Cramer et que son ensemble solution est non vide, il admet une infinité de solutions, qu'il est bon de paramétrer en exprimant les inconnues en fonctions des libertés du système. 
\bigskip

\Exemple. 
Résoudre le système 
$$
\Q\{\eqalign{
&x+y+z+t+u=0\cr
&x+2y+3z+4t+5u=0\cr
&5x+4y+3z+2t+u=0
}\W.
$$


\Remarque { (propriété utile)} : Soit $A$ une matrice carrée de taille $n$. Alors $A$ est inversible, si et seulement si 
$$
AX=0\Longleftarrow X=0.
$$
 
\Remarque : Pour résoudre les systèmes, on pourra utiliser la méthode du pivot de Gauss. Par ailleurs, 
certains systèmes sont plus faciles et plus rapides à résoudre que d'autres : les systèmes diagonaux et les systèmes triangulaires. 



\Section DeT, Déterminant d'ordres $2$ et $3$.

\Concept [] Déterminant d'une matrice carrée de taille inférieure à $3$. 

\Definition []  Le déterminant d'une matrice carrée de taille inférieure à $3$ est défini par 
$$
\eqalign{
\det\pmatrix{a}&:=a, \cr
\det\pmatrix{a&b\cr c&d}&:=ad-bc,\cr
\det\pmatrix{a_1&a_2&a_3\cr b_1&b_2&b_3\cr c_1&c_2&c_3}&:=a_1b_2c_3+a_2c_1b_3+a_3b_1c_2-a_3b_2c_1-a_2b_1c_3-a_2b_3c_2.
}
$$


\Propriete []  Pour $n\in\{1,2,3\}$, on a $\det(\mbox{I}_n)=1$. 
\bigskip

\Theoreme [$n\in\{1,2,3\}$] 
$$
\forall (A,B)\in\sc M_n(\ob K)^2, \qquad \det(A\times B)=\det(A)\times\det(B). 
$$

\Propriete []  Une matrice $M$ de taille $n\in\{1,2,3\}$ est inversible si, et seulement si, son déterminant est non nul. 
Et dans ce cas, on a 
$$
\det\b(M^{-1}\b)={1\F \det(M)}.
$$

\Definition []  Soit $E$ un $\ob K$-espace vectoriel de dimension $n\in\{1, 2,3\}$ muni d'une base 
$\sc B=\{\vec e_1,\cdots,e_n\}$. Alors, le déterminant de la famille de $n$ vecteurs $(f_1,\cdots, f_n)$ dans la base $\sc B$ est 
le déterminant de la matrice de cette famille dans la base $\sc B$ : 
$$
\det_{\sc B}(f_1,\cdots, f_n):=\det\Q(\sc Mat_{\sc B}(f_1,\cdots, f_n)\W). 
$$ 

\Propriete []  Soit $E$ un $\ob K$-espace vectoriel de dimension $n\in\{1, 2,3\}$ muni d'une base 
$\sc B$, alors la famille de $n$ vecteurs $(f_1,\cdots, f_n)$ est une base de $E$ si, et~seulement~si le déterminant de cette famille dans la base $\sc B$ n'est pas nul. 
$$
\mbox{$(f_1,\cdots,f_n)$ est une base }\Longleftrightarrow \det_{\sc B}(f_1,\cdots,f_n)\neq0
$$

\Propriete []  Soit $n\in\{2,3\}$, soit $A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}$ une matrice inversible 
et soit $B=(b_i)_{1\le i\le n}\in\sc M_n(\ob R)$. Alors, le système $AX=B$ est de Cramer. Notant $X=(x_i)_{1\le i\le n}$ son unique solution, on a 
$$
\forall i\in\{1,\cdots, n\}, \qquad x_i={1\F \det A}\det\pmatrix{a_{1,1}&\ldots&a_{1,i-1}&b_1&a_{1,i+1}&\ldots&a_{1,n}\cr
\vdots&\ldots&\vdots&\ldots&\vdots&\ldots&\vdots\cr
a_{n,1}&\ldots&a_{n,i-1}&b_n&a_{n,i+1}&\ldots&a_{n,n}\cr
}
$$

\Definition []  Soit $E$ un $\ob K$-espace vectoriel de dimension finie $n\in\{1, 2,3\}$ muni d'une base 
$\sc B=\{\vec e_1,\cdots,e_n\}$. Alors, le déterminant d'un endomorphisme $u\in\sc L(E)$ dans la base $\sc B$ est le
déterminant de la matrice de $u$ dans la base $B$ au départ et à l'arrivé.  
$$ 
\det_{\sc B}(u):=\det\Q(\sc Mat_{\sc B,\sc B}(u)\W).  
$$

\Theoreme [$\sc B$ base d'un $\ob K$-espace vecoriel $E$ de dimension $n\in\{1,2,3\}$] 
$$
\forall (u,v)\in\sc L(E)^2, \qquad \det_{\sc B}(u\circ v)=\det_{\sc B}(u)\times\det_{\sc B}(v).
$$

\Remarque : En pratique, la base $\sc B$ utilisée pour calculer le déterminant est la base canonique (et parfois on utilise des bases orthonormées). 
\bigskip

\Theoreme [$\sc B$ base d'un $\ob K$-espace vecoriel $E$ de dimension $n\in\{1,2,3\}$] 
un endomorphisme $u:E\to E$ est bijectif (automorphisme) si, et~seulement~si $\det_{\sc B}(u)\neq 0$.  
$$ 
u\in\sc L(E)\mbox{ automorphisme }\Longleftrightarrow\det_{\sc B}(u)\neq0. 
$$


\Concept [] Propriétés du déterminant en dimension 2

\noindent
Le déterminant $(\vec u,\vec v)\mapsto \det_{\sc B}(\vec u,\vec v)$ est une application à valeurs réelles vérifiant
$$
\eqalignno{
&\qquad\qquad\qquad{\forall \vec u \mbox{ et } \vec v \mbox{ vecteurs de $\sc P$}}, \qquad
{\det_{\sc B}(\vec u,\vec v) =-\det_{\sc B}(\vec v,\vec u)}&\mbox{(anti-symétrie)} \cr
&\qquad\qquad\qquad\forall \vec u \mbox{ vecteur de $\sc P$}, \qquad\qquad {\det_{\sc B}(\vec
u,\vec u)=0}&\mbox{(alternée)} } $$ et vérifiant $\forall \vec u, \vec v \mbox{ et } \vec w \mbox{
vecteurs de $\sc P$}$ et $\forall (\lambda,\mu)\in\ob R^2$ les relations $$\Q\{ \eqalign{
&{\det_{\sc B}(\lambda\vec u+\mu\vec v,\vec w)=\lambda\det_{\sc B}(\vec u,\vec w) +\mu\det_{\sc B}(\vec v,\vec
w)} \cr &{\det_{\sc B}(\vec w,\lambda\vec u+\mu\vec v)=\lambda\det_{\sc B}(\vec w,\vec u)+\mu\det_{\sc B}(\vec
w,\vec v)} }\W.\leqno{\mbox{(bilinéarité)}} $$


\Concept [] Propriétés du déterminant en dimension 3

\noindent
Le déterminant {$(\vec u,\vec v,\vec w)\mapsto \det_{\sc B}(\vec u,\vec v,\vec w)$ est une application à valeurs réelles} vé\-ri\-fi\-ant 
$$
{\forall \vec u, \vec v \mbox{ et } \vec w \mbox{ vecteurs de $\sc E$}}, 
\qquad {\eqalign{\det_{\sc B}(\vec u,\vec v,\vec w)&=-\det_{\sc B}(\vec u,\vec w,\vec v)\cr&=-\det_{\sc B}(\vec
v,\vec u,\vec w)\cr &=-\det_{\sc B}(\vec w,\vec v,\vec u)}}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\leqno{\mbox{(anti-
symétrie)}} $$

$$ 
\forall \vec u, \vec v \mbox{ vecteurs de $\sc E$}, \qquad\qquad {\Q\{\eqalign{\det_{\sc B}(\vec u,\vec u,\vec v)=0\cr
\det_{\sc B}(\vec u,\vec v,\vec u)=0\cr
\det_{\sc B}(\vec v,\vec u,\vec u)=0
}\W.}\leqno{\mbox{(alternée)}} 
$$
et vérifiant $\forall \vec u, \vec v, \vec w \mbox{ et } \vec x \mbox{ vecteurs de $\sc E$}$ 
et $\forall (\lambda,\mu)\in\ob R^2$ les relations 
$$
\Q\{ \eqalign{ &{\det_{\sc B}(\lambda\vec u+\mu\vec v,\vec w,\vec
x)=\lambda\det_{\sc B}(\vec u,\vec w,\vec x) +\mu\det_{\sc B}(\vec v,\vec w,\vec x)} \cr &{\det_{\sc B}(\vec
w,\lambda\vec u+\mu\vec v,\vec x)=\lambda\det_{\sc B}(\vec w,\vec u,\vec x)+\mu\det_{\sc B}(\vec w,\vec v,\vec x)} \cr
&{\det_{\sc B}(\vec w,\vec x,\lambda\vec u+\mu\vec v)=\lambda\det_{\sc B}(\vec w,\vec x,\vec u)+\mu\det_{\sc
B}(\vec w,\vec x,\vec v)} }\W.\leqno{\mbox{(trilinéarité)}} 
$$
                                    




                                    
\hautspages{Olus Livius Bindus}{Suites de nombres}

\pagetitretrue
     

\Chapter Suites, Suites de nombres. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne $\ob K=\ob R$ ou $\ob K=\ob C$. 
\bigskip

\Section CorpsR, Corps $\ob R$.
\bigskip


\Theoreme [PT=admis]
L'ensemble $R$ des nombres réels, muni de l'addition $+$ et de la multiplication $\times$ usuelles, 
forme un corps commutatif noté $(\ob R,+,\times)$. 

\Concept [] Relations binaires

\Definition []  Soit $E$ un ensemble. Une relation binaire $\sc R$ de $E$ est un graphe de $E\times E$, c'est à dire une partie, un sous ensemble $\sc R$ de $E$. \pn
Pour une relation $\sc R$ de $E$, on adopte la notation 
$$
x\ \sc R\ y\Longleftrightarrow (x,y)\in\sc R. 
$$

\Exemple. Dans $E=\ob R$, la relation $<$ définie par $x<y\Longleftrightarrow y-x\in\ob R_+^*$


\Concept [] Relation d'équivalence


\Definition []  soit $E$ un ensemble et $\sc R$ une relation binaire de $E$. Alors, $\sc R$ est une relation d'équivalence de $E$ si, et seulement si
la relation $\sc R$ vérifie les trois propriétés suivantes : 
$$
\eqalignno{
&\forall x\in E, \qquad\qquad x\ \sc R\ x,& \mbox{(reflexivité)}
\cr
&\forall (x,y)\in E^2, \qquad (x\ \sc R\ y)\Longrightarrow (y\ \sc R\ x),&\mbox{(symétrie)}
\cr
&\forall (x,y,z)\in E^3, \qquad (x\ \sc R\ y)\mbox{ et }(y\ \sc R\ z)\quad\Longleftrightarrow\quad (x\ \sc R\ z).&\mbox{(transitivité)}
}
$$

                                            
\Exemple. La relation d'égalité définie par 
$$
\forall(x,y)\in\ob R^2, \qquad x=y\Longleftrightarrow x-y\in\{0\} \eqdef{(a)}
$$
est une relation déquivalence de $\ob R$. 
\bigskip


\Exemple. Soit $\delta\in\ob R$. La relation de congruence modulo $\delta$ définie par 
$$
\forall(x,y)\in\ob R^2,\qquad x\equiv y\quad [\delta]\Longleftrightarrow x-y\in\delta\ob Z\eqdef{(b)}
$$
est une relation déquivalence de $\ob R$. 
\bigskip


\Concept [] Relation d'ordre

\Definition []  soit $E$ un ensemble et $\sc R$ une relation binaire de $E$. Alors, $\sc R$ est une relation d'ordre de $e$ si, et seulement si
la Relation $\sc R$ vérifie les trois propriétés suivantes : 
$$
\eqalignno{
&\forall x\in E, \qquad\qquad x\ \sc R\ x,& \mbox{(reflexivité)}
\cr
&\forall (x,y)\in E^2, \qquad (x\ \sc R\ y)\mbox{ et }(y\ \sc R\ x)\quad\Longrightarrow\quad x=y,&\mbox{(antisymétrie)}
\cr
&\forall (x,y,z)\in E^3, \qquad (x\ \sc R\ y)\mbox{ et }(y\ \sc R\ z)\quad\Longleftrightarrow\quad (x\ \sc R\ z).&\mbox{(transitivité)}
}
$$

\Definition []  soit $E$ un ensemble. Une relation d'ordre $\sc R$ est totale si, et~seulement~si
$$
\forall (x,y)\in E^2, \qquad (x\ \sc R\ y)\mbox{ ou }(y\ \sc R\ x)\leqno{(totale)}
$$

\Theoreme La relation $\le$ définie par 
$$
\forall (x,y)\in \ob R^2, \qquad x\le y\Longleftrightarrow y-x\in\ob R_+\eqdef{(c)}
$$
est une relation d'ordre totale de $\ob R$. 

\Concept [] Compatibilité d'une relation binaire $\sc R$ avec une loi $\diamond$

\Definition []  Soit $E$ un ensemble. Alors une relation binaire $\sc R$ de $E$ est compatible avec une loi interne $\diamond$ de $E$ si, et~seulement~si 
$$
\forall (x,x',y,y')\in E, \qquad (x\ \sc R\ y)\mbox{ et }(x'\ \sc R\ y')\quad\Longrightarrow\quad (x\diamond x')\sc R(y\diamond y'). 
$$

\Remarque. Soit $E$ un ensemble. Si la relation $\sc R$ de $E$ est compatible avec la loi interne $\diamond$ de $E$, alors 
$$
\forall (x,y,a)\in E, \qquad (x\ \sc R\ y)\Longrightarrow\quad (x\diamond a)\sc R(y\diamond a). 
$$

\Propriete []  Les relations d'équivalences \eqref{(a)} et \eqref{(b)} sont compatibles avec l'addition $+$ et la multiplication $\times$. 
\bigskip

\Propriete []  La relation d'ordre naturelle $\le$ de $\ob R$ est compatible avec l'addition (mais pas avec la multiplication). 
De plus, on a 
$$
\forall a\ge0, \qquad \forall (x,y)\in\ob R^2, \qquad x\le y\Longrightarrow \Q\{\eqalign{ax\le ay\cr -ay\le -ax}\W. 
$$

\Concept [] Valeur absolue 

\Definition []  Pour chaque nombre réel $x$, la valeur absolue de $x$ est le nombre réel positif ou nul défini par
$$
|x|:=\Q\{\eqalign{x\mbox{ si }x\ge 0,\cr
-x \mbox{ si }x< 0.\cr
}\W.
$$

\Propriete []  La valeur absolue $x\mapsto|x|$ est une norme sur $\ob R$, en effet, c'est une application vérifiant 
$$
\eqalignno{
&\qquad\qquad \qquad \qquad \forall z\in\ob C,  \qquad |z|\ge0 &\mbox{(positivité)}\cr
&\qquad\qquad \qquad \qquad \forall z\in\ob C, \qquad|z|=0\ \Longleftrightarrow\ z=0&\mbox{(définie)}\cr
&\qquad\qquad \qquad \qquad \forall (s,z)\in\ob C^2 , \qquad \B| |s|-|z|\B| \le |s+z|\le |s|+|z| &\mbox{(inégalité triangulaire)}\cr
&\qquad\qquad \qquad \qquad \forall x\in\ob R, \quad \forall y\in\ob R \qquad |x.y|=|x|.|y| 
&\mbox{(positivité homogène)}
}
$$

\Definition []  La distance entre deux nombres réels $x$ et $y$ est la valeur absolue $|x-y|$. 
\bigskip

\Concept [] majorants, minorants. 

\Definition []  Soit $(E,\sc R)$ un ensemble muni d'une relation d'ordre. Alors, $M\in E$ est un majorant d'un ensemble $D\subset E$ si, et seulement si 
$$
\forall x\in D, \qquad x\ \sc R\ M
$$
et $m\in E$ est un minorant d'un ensemble $D\subset E$ si, et seulement si 
$$
\forall x\in D, \qquad m\ \sc R\ x.
$$

\Remarque : Dans $E:=\ob R$ muni de sa relation d'ordre naturelle $\sc R=\le$, on a 
$$
\eqalign{
M\in\ob R\mbox{ est un majorant de }D\subset\ob R\Longleftrightarrow \forall x\in D, \quad x\le M,\cr
m\in\ob R\mbox{ est un minorant de }D\subset\ob R\Longleftrightarrow \forall x\in D, \quad m\le x,\cr
}
$$

\Remarque : On dit qu'une partie $D\subset\ob R$ est majorée (resp. minorée) si elle admet un majorant (resp. un minorant). 
\bigskip

\Concept [] plus petits et plus grands éléments. 

\Definition []  Soit $(E,\sc R)$ un ensemble muni d'une relation d'ordre. Alors, $M\in E$ est le plus grand (resp. plus petit) élément 
d'un ensemble $D\subset E$ si, et seulement si $M\in D$ et si $M$ est un majorant (resp. minorant) de $D$. 
\bigskip

\Remarque : Dans $E:=\ob R$ muni de sa relation d'ordre naturelle $\le$, on a 
$$
\eqalign{
M\in\ob R\mbox{ est le plus grand élément de }D\subset\ob R\Longleftrightarrow M\in D\mbox{ et }\forall x\in D, \quad x\le M,\cr
m\in\ob R\mbox{ est le plus petit élément de }D\subset\ob R\Longleftrightarrow m\in D\mbox{ et }\forall x\in D, \quad m\le x,\cr
}
$$

\Remarque : S'IL EXISTE, le plus grand (resp. petit) élément de $D$ est unique et on le note $\max D$ (resp. $\min D$). 
\bigskip

\Remarque : Un ensembe fini et non vide de nombres réels admet toujours un plus petit et un plus grand élément. 
\bigskip

\Remarque : le plus petit (resp. le plus grand) élément de $D$ est le plus grand des minorants (resp le plus petit des majorants) de $D$. 
\bigskip 

\Concept [] borne inférieure et borne supérieure. 

              
\Definition []  Un nombre réel $S$ est la borne supérieure d'un ensemble $D\subset \ob R$ si, et~seulement~si 
$$
\eqalign{
\forall x\in D , \qquad  x\le S \cr
\forall\epsilon> 0 , \quad \exists x\in D :\quad S-\epsilon\le x\le S
}
$$
Si elle existe, la borne supérieure de $D$ est unique et on la note $\sup D$. 
\bigskip


\Definition []  Un nombre réel $s$ est la borne inférieure d'un ensemble $D\subset \ob R$ si, et~seulement~si 
$$
\eqalign{
\forall x\in D , \qquad & s\le x \cr
\forall\epsilon> 0 , \quad \exists x\in D :\quad &s\le x\le s+\epsilon
}
$$
Si elle existe, la borne inférieure de $D$ est unique et on la note $\inf D$. 
\bigskip


\Propriete []  Toute partie majorée (resp. minorée) non vide de $\ob R$ admet une borne supérieure
(resp. inférieure). 

\Remarque : une borne supérieure (resp. inférieure) de $D$ n'est pas forcément un plus petit (resp. un plus grand) élément de $D$. 
Ainsi,
$$
\sup \{x\in\ob Q:x\le \sqrt2\}=\sqrt2\qquad\mbox{et}\qquad\sqrt2\notin\ob Q.
$$ 

\Remarque :  Par convention, on note $\sup\emptyset=-\infty$ (resp. $\inf\emptyset=+\infty$). \pn
De même, on note $\sup D=+\infty$ 
(resp. $\inf D=-\infty$) pour chaque partie $D$ de $\ob R$ non majorée (resp. non minorée). 
\bigskip

\Concept [] Intervalles

\Definition []  On appelle droite réelle achevée et l'on note $\overline{\ob R}$ ou même $[-\infty,+\infty]$ l'ensemble 
$$
\overline{\ob R}:=\ob R\cup\{-\infty,+\infty\}
$$
On convient que $-\infty<+\infty$ et que $-\infty< x$ et que $x<+\infty$ pour chaque nombre réel $x$. 
\bigskip

\Remarque : C'est juste une convention pratique....rien de plus. 
\bigskip

\Definition []  soient $(a,b)\in\overline{ \ob R}^2$ tels que $a\le b$. Alors, l'intervalle ouvert $\Q]a,b\W[$ est l'ensemble défini par 
$$
\Q]a,b\W[:=\{x\in\ob R:a< x< b\}.
$$
Si $a\in\ob R$ (resp. si $b\in\ob R$), l'intervalle semi-ouvert $\Q[a,b\W[$, fermé à gauche et ouvert à droite (resp $\Q]a,b\W]$, ouvert à droite et fermé à gauche), est l'ensemble défini par 
$$
[a,b[:=\{x\in\ob R:a\le x< b\}\qquad(\mbox{resp. }]a,b]:=\{x\in\ob R:a < x\le b\}).
$$
Enfin, si $(a,b)\in\ob R^2$, l'intervalle fermé $[a,b]$ est l'ensemble défini par 
$$
[a,b]:=\{x\in\ob R:a\le x\le b\}.
$$


\Propriete []  Pour chaque couple $(a,b)$ de nombres réels vérifiant $a< b$, l'intervalle $\Q]a,b\W[$ contient au moins un nombre rationnel ${p\F q}\in\ob Q$ et un nombre irrationnel $x\in\ob R\ssm\ob Q$. 
\bigskip

\Concept [] Approximation décimale des nombres réels. 

\Definition []  La partie entière d'un nombre réel $x$ est le nombre entier défini par
$$
[x]:=\max\{n\in\ob Z:n\le x\}
$$
et la partie fractionnaire de $x$ est le nombre de l'intervalle $\Q[0,1\W[$ défini par 
$$
\langle x\rangle:=x-[x]\qquad \mbox{(Attention : notation non standard)}.
$$

\Remarque : la partie entière d'un nombre réel $x$ est l'unique entier $n\in\ob Z$ tel que 
$$
n\le x< n+1.
$$ 
La partie fractionnaire d'un nombre réel $x$ est la quantité $x-n$ qu'il reste quand on lui retranche sa partie entière. 
\bigskip

\Propriete []  Pour chaque nombre réel $x$ et pour chaque entier $n\ge1$, il existe un unique entier $N\in\ob Z$ tel que 
$$
{N\F 10^n}\le x< {N+1\F10^n}.
$$ 
Le nombre $N\times10^{-n}$ s'appelle approximation décimale de $x$ par défaut à $10^{-n}$ près et s'ecrit sous la forme 
$$
{N\F 10^n}=
\underbrace{a_p\cdots a_2a_1}_{[x]}\ ,\underbrace{b_1b_2b_3b_4b_5b_6\cdots b_{n-2},b_{n-1},b_n}_{\mbox{développement décimal de $\langle x\rangle$}}
$$
Le nombre $(N+1)\times10^{-n}$ s'appelle approximation décimale de $x$ par excès à $10^{-n}$ près. 

\Section Suitesnr, Suites de nombres.

Dans cette section on a $\ob K=\ob R$ ou $\ob K=\ob C$. 

\Concept [] Suites

\noindent
Soit $I\subset\ob N$ un ensemble non-vide. Une suite $u$ d'éléments de $\ob K$ indicée par l'ensemble~$I$ est une application $u: I\to\ob K$, qui sera noté plus simplement $u=(u_n)_{n\in I}$. 
\bigskip

\Remarque : la plupart du temps on prendra $I=\ob N$ ou $I=\ob N^*$. 
\bigskip

\Exemples. $u=(n^2)_{n\in\ob N}$, \quad $v=(\e^n)_{n\in\ob N}$ \ et\ $w=({1\F n})_{n\in\ob N^*}$
\bigskip

\Concept [] Espace vectoriel des suites $(\ob K^I,+,.)$

\Definition []  Etant données deux suites $u\in\ob K^I$, $v\in\ob K^I$ et un nombre $\lambda\in\ob K$, 
on définit les suites $u+v$, $\lambda.u$ et $u\times v$ en posant 
$$
\eqalign{
&\forall n\in I , \qquad  (u+v)_n=u_n+v_n ,
\cr
&\forall n\in I , \qquad  (\lambda.u)_n=\lambda.u_n ,
\cr
&\forall n\in I , \qquad  (u\times v)_n=u_n\times v_n .
}
$$

\Exemple. on a $\ds2.(n^2)_{n\in\ob N}+(\e^n)_{n\in\ob N}\times\Q({1\F n+1}\W)_{n\ob N}=\Q(2n^2+{\e^n\F n+1}\W)_{n\in\ob N}$. 
\bigskip

\Propriete []  L'ensemble des suites d'éléments de $\ob K$ indicées par $I$ est noté $\ob K^I$. 
C'est un $\ob K$-espace vectoriel pour les lois $+$ et $.$ usuelles. 
\bigskip

\Remarque : 
Bien qu'elles soient définies à l'aide d'applications, nous n'allons pas nous servir des suites comme nous nous servons des fonctions. 
\bigskip


\Concept [] Suites bornées

\Definition []  Une suite $u\in\ob K^{\ob N}$ est bornée si, et~seulement~si, il existe un nombre $M\ge0$ tel que 
$$
\forall n\in\ob N ,\qquad  |u_n|\le M .
$$

\Exemple. pour $z\in\ob C$, la suite $(z^n)_{n\in\ob N}$ si et seulement si $|z|\le 1$. 
\bigskip

\Propriete [] L'ensemble des suites $u\in\ob K^{\ob N}$ forme un sous espace vectoriel de $(\ob K^{\ob N},+,.)$. 
De plus, le produit de deux suites bornées est une suite bornée. 
\bigskip

\Propriete []  Soient $u\in\ob C^{\ob N}$ et $(x,y)\in\ob R^{\ob N}$ tels que 
$$
\forall n\in\ob N,\qquad u_n=x_n+iy_n.
$$
Alors, la suite complexe $u$ est bornée si, et~seulement~si les suites réelles $x$ et $y$ (de ses parties réelles et imaginaires) 
sont bornées. 
\bigskip


\Concept [] Suites minorées, suites majorées

\Definition []  Une suite $u\in\ob R^{\ob N}$ est majorée (resp. minorée) si, et~seulement~si, 
il existe un nombre $M\in\ob R$ (resp. $m\in\ob R$) tel que 
$$
\forall n\in\ob N ,\qquad  u_n\le M \qquad \qquad\b(\mbox{resp. } m\le u_n\b).
$$

\Exemple. la suite $(a n^2+n+1)_{n\in\ob N}$ est minorée si $a> 0$ et majorée si $a< 0$. 
\bigskip

 Comme il n'y a pas de relation d'ordre (utile) dans $\ob C$, 
parler de suites complexes minorées ou majorées n'a aucun sens. 
\bigskip

\Propriete []  Une suite réelle $u$ (i. e. dont les éléments sont des nombres réels) est bornée si, et~seulement~si elle est majorée et minorée. 
\bigskip

\Remarque : Une suite $u$ est majorée (resp. minorée) si, et~seulement~si l'ensemble $\{|u_n|\}_{n\in\ob N}$ 
admet un majorant (resp. un minorant). 
\bigskip

\Concept [] Décalage d'indice

\Propriete []  Soit $u\in\ob K^{\ob N}$ une suite et soit $d\ge0$ un entier. alors, la suite $u$ est bornée (resp. minorée, majorée) si, et~seulement~si 
la suite $v:=(u_{n+d})_{n\in\ob N}$ est bornée (resp. minorée, majorée). 
\bigskip

\Remarque : le fait qu'une suite $\{u_n\}_{n\in\ob N}$ soit bornée, minorée ou majorée ne dépend pas du tout de ses premiers éléments, mais plutôt de $u_n$ pour les grandes valeurs de $n$. 
\bigskip


\Concept [] Relation d'ordre

\Definition []  La relation $\le$ de l'ensemble $\ob R^{\ob N}$ définie par 
$$
\forall u\in\ob R^{\ob N}, \quad\forall v\in\ob R^{\ob N}, \qquad\qquad u\le v\ \Longleftrightarrow \ \forall
n\in\ob N, \quad u_n\le v_n.  
$$
est une relation d'ordre. \bigskip

\Remarque : cette relation d'ordre des suites réelles va permettre d'écrire simplement certaines propriétés fondamentales du calcul de limite (Théorème des gendarmes...). 
\bigskip

\Exemple. pour $u=(1)_{n\in\ob N}$ et $v=(\e^n)_{n\in\ob N}$ , on a $u\le v$. 
\bigskip

\Concept [] Suites monotones.

\Definition []  Une suite réelle $u\in\ob R^{\ob N}$ est strictement croissante (resp croissante) si, et~seulement~si,
$$
\forall n\in\ob N, \qquad {u_{n+1}> u_n}\qquad\b(\mbox{resp. }{u_{n+1}\ge u_n}\b).
$$

\Definition []  Une suite réelle $u\in\ob R^{\ob N}$ est strictement décroissante (resp décroissante) si, et~seulement~si,
$$
\forall n\in\ob N, \qquad {u_{n+1}< u_n}\qquad\b(\mbox{resp. }{u_{n+1}\le u_n}\b).
$$

\Definition []  Une suite réelle $u\in\ob R^{\ob N}$ est strictement monotone (resp monotone) si, et~seulement~si,
elle est strictement décroissante ou strictement croissante (resp. décroissante ou croissante). 
\bigskip

{\vtop{\hsize=125mm\noindent\centerline{\bf Méthode}
\smallskip
Pour étudier la monotonie d'une suite $\{u_n\}_{n\in\ob N}$, on étudie le signe de $u_{n+1}-u_n$. 
}}
\bigskip

\Exemple. La suite $\ds u=\Q({2+n\F 3n+1}\W)_{n\in\ob N}$ est décroissante. 
\bigskip

\Section Limi, Convergence des suites. 

\Subsection Convergence, divergence et limites. 

\Definition []  Une suite $u\in\ob K^{\ob N}$ converge vers une limite $\ell\in\ob K$ si, et~seulement~si
$$
\forall\epsilon>0, \qquad \exists N_\epsilon\in\ob N:\qquad \forall n\ge N_\epsilon, \quad |u_n-\ell|\le \epsilon.\eqdef{limlim}
$$
Si elle existe, la limite $\ell$ de la suite $u$ est unique et on la note $\ell=\lim u$ ou $\ell=\lim_{n\to+\infty}u_n$. 
\bigskip

\Remarque : Lorsque $u$ converge vers $\ell\in\ob K$, on dit que la suite converge/est convergente. 
Ainsi, la suite $u$ converge $\Leftrightarrow$ il existe $\ell\in\ob K$ tel que $u$ converge vers $\ell\Leftrightarrow$
$$
\exists \ell\in\ob K:\qquad \forall\epsilon>0, \qquad \exists N\in\ob N,\qquad \forall n\ge N, \quad |u_n-\ell|\le \epsilon.
$$

\Remarque : Si la suite $u$ ne converge pas, on dit que la suite $u$ diverge/est divergente. 
Alors, la suite $u$ diverge $\Leftrightarrow$ pour tout $\ell\in\ob K$, la suite $u$ ne converge pas vers $\ell\Leftrightarrow$
$$
\forall \ell\in\ob K: \qquad\exists\epsilon>0, \qquad \forall N\in\ob N,\qquad \exists n\ge N, \quad |u_n-\ell|> \epsilon.
$$

\Remarque : En fran\c cais , une suite $u=(u_n)_{n\in\ob N}$ converge vers une limite $\ell$ si, et~seulement~si pour``marge d'erreur'' $\epsilon> 0$, on peut trouver un rang $N_\epsilon$ (qui dépend en général de $\epsilon$) tel que la différence entre $u_n$ et sa limite $\ell$ soit plus petite que la marge d'erreur $\epsilon$ quand $n$ est plus grand que le rang $N_\epsilon$. 
\bigskip

{\vtop{\hsize129mm\quad {\bf Méthode} : Pour prouver, via \eqref{limlim}, qu'une suite $u$ converge vers $\ell$ : 

\noindent
1) Commencer par intuiter la limite $\ell$ vers laquelle la suite tends. 

\noindent
2) Fixer un nombre $\epsilon> 0$ quelconque

\noindent
3) Majorer la quantité $|u_n-\ell|$, en cherchant à la rendre plus petite que $\epsilon$ pour des entiers $n$ plus grand 
qu'un entier $N_\epsilon$, à déterminer, 
sur lequel on pourra jouer 

pour obtenir une bonne estimation de $u_n$. }}
\bigskip

\Propriete []  Soit $u\in\ob K^{\ob N}$ une suite et $\ell\in\ob K$. Alors, la suite $u$ converge vers $\ell$ si, et~seulement~si, 
la suite de terme général $v_n:=u_n-\ell$ converge vers $0$. 
\bigskip

\Remarque : cette propriété illustre un principle important en mathématiques : il est équivalent (mais beaucoup plus facile) de montrer que la suite $u-\ell$ converge vers $0$ que de montrer que la suite $u$ converge vers $\ell$. 
\bigskip

\Propriete []  Si $u\in \ob K^{\ob N}$ est une suite convergente, alors $u$ est une suite bornée. 
\bigskip

\Propriete []  Si $u\in\ob K^{\ob N}$ est une suite convergeant vers $\ell\neq 0$, alors, il existe un rang $N\in\ob N$ à partir duquel on a 
$$
\forall n\ge N,\qquad u_n\neq0, 
$$
Si $u\in\ob R^{\ob N}$ est une suite convergeant vers $\ell> 0$, alors, il existe un rang $N\in\ob N$ à partir duquel on a 
$$
\forall n\ge N,\qquad u_n\ge{\ell\F 2}>0. 
$$

\Theoreme [$u$ et $v$ suites convergentes d'éléments de $\ob K$, $\lambda\in\ob K$] 
Les suites $\lambda.u$, $u+v$ et $u\times v$ sont convergentes et 
$$
\eqalign{
&\quad\lim(\lambda.u)=\lambda.\lim u,\cr
&\lim(u+v)=\lim u+\lim v,\cr
&\lim(u\times v)=\lim u\times\lim v.
}
$$
De plus, si $\lim v\neq 0$, la suite $u/v=(u_n/v_n)_{n\ge N}$ est définie à partir d'un certain rang $N$ et converge vers la limite 
$$
{\lim\Q({u\F v}\W)={\lim u\F\lim v}}.
$$

\Theoreme [$u$ suite complexe]
Soient $x$ et $y$  les suites réelles $x_n=\re(u_n)$ et $y_n=\im(u_n)$, implicitement définies par
$$
\forall n\in\ob N,\qquad u_n=x_n+iy_n.
$$
Alors, $x$ converge vers $\ell\in\ob R$ et $y$ converge vers $\ell'\in\ob R$ si, et~seulement~si la suite $u$ converge vers $\ell+i\ell'$. 

\Propriete []  Soient $u\in\ob K^{\ob N}$ une suite convergente et $v\in\ob K^{\ob N}$ une suite divergente. Alors, la suite $u+v$ est divergente. 
\bigskip

\Remarque : si $u$ et $v$ sont deux suites divergentes, la suite $u+v$ peut converger (par exemple pour $u_n=n$ et $v_n=-n$) ou diverger (par exemple pour $u_n=v_n=n$). 
\bigskip


\Propriete [Title=Conservation des inégalités larges par passage à la limite]
Soient $u\in\ob R^{\ob N}$ et $v\in\ob R^{\ob N}$ deux suites 
{\bf Réelles} et convergentes. alors, on a 
$$
\forall n\in\ob N, \quad u_n\le v_n\Leftrightarrow u\le v\Longrightarrow \lim u\le \lim v. 
$$ 


\Theoreme [$x$ nombre réel]
Il existe une suite de nombres rationnels qui converge vers $x$. 

\Propriete []  L'ensemble des suites $u\in\ob K^{\ob N}$ convergeant vers $0$ forme un espace vectoriel pour l'addition et la multiplication externe des suites. 
De plus, si $u\in\ob K^{\ob N}$ converge vers $0$ et si $v\in\ob K^{\ob N}$ est une suite bornée, alors, la suite $u.v$ converge vers $0$. 
\bigskip


\Propriete []  Soient $u\in\ob C^{\ob N}$ une suite complexe et $\alpha\in\ob R^{\ob N}$ une suite réelle telles que 
$$
\forall n\in\ob N, \qquad |u_n|\le \alpha_n\qquad \mbox{et}\qquad \lim_{n\to+\infty}\alpha_n=0.
$$
Alors, la suite $u$ converge vers $0$. 
\bigskip


\Propriete [Title=Principe des gendarmes]
Soient $u$, $v$ et $w$ trois suites de nombre réels telles que $u\le v\le w$. Si $u$ et $w$ convergent vers le même nombre réel $\ell$, alors la suite $v$ converge vers $\ell$. 
\bigskip


\Propriete []  Si $u\in\ob K^{\ob N}$ converge vers une limite $\ell\in\ob K$, alors la suite de terme général $v_n=|u_n|$ converge vers la limite $\ell':=|\ell|$. 
\bigskip

\Subsection rahhh, Limites infinies. 

\Definition []  On dit qu'une suite réelle $u\in\ob R^{\ob N}$ tends vers $+\infty$, et on note $\lim u=+\infty$ si, et~seulement~si
$$
\forall m\in\ob R, \qquad \exists N\in\ob N:\qquad\forall n\ge N, \qquad u_n\ge m.
$$
On dit alors que $u$ diverge vers $+\infty$. 
\bigskip

\Definition []  On dit qu'une suite réelle $u\in\ob R^{\ob N}$ tends vers $-\infty$, et on note $\lim u=-\infty$ si, et~seulement~si
$$
\forall M\in\ob R, \qquad \exists N\in\ob N:\qquad\forall n\ge N, \qquad u_n\le M.
$$
On dit alors que $u$ diverge vers $-\infty$. 
\bigskip


\Propriete []  La suite $u\in\ob R^{\ob N}$ diverge vers $+\infty\Longleftrightarrow$ la suite $-u$ diverge vers $-\infty$. 
\bigskip

\Propriete []  Soit $u\in\ob R^{\ob N}$ une suite divergeant vers $+\infty$. Alors, on a 
$$
\lim_{n\to+\infty}{1\F u_n}=0^+\qquad\mbox{et}\qquad\forall \lambda> 0, \quad\lim_{n\to+\infty}(\lambda u_n)=+\infty
$$
Soit $v\in\ob R^{\ob N}$ une suite minorée (par exemple $v$ convergente ou $v$ divergeant vers~$+\infty$), alors on a 
$$
\lim(u+v)=+\infty.
$$ 
De plus, si $v$ est minorée à partir d'un certain rang par un nombre $m>0$, on a 
$$
\lim(uv)=+\infty.
$$

\Propriete []  Si $u\in\ob R^{\ob N}$ converge vers $0^+$ (autrement dit si $\lim u=0$ et si $u_n$ est strictement positif à partir d'un certain rang), on a $\ds\lim_{n\to+\infty}\Q({1\F u_n}\W)=+\infty$. 
\bigskip


\Propriete []  Soient $u\in\ob R^{\ob N}$ et $v\in\ob R^{\ob N}$ telles que $u\le v$. Alors, 
$$
\lim u=+\infty\Longrightarrow\lim v=+\infty.
$$

\Subsection gahhhh, Suites extraites. 


\Definition []  Une suite extraite d'une suite $u\in\ob C^{\ob N}$ est une suite de terme général $v_n=u_{\varphi(n)}$ où $\varphi:\ob N\to\ob N$ est une application strictement croissante. 
\bigskip

\Remarque : En gros une suite extraite de $u=(u_0,u_1,u_2,u_3,u_4,u_5,u_6,...)$ est une suite fabriquée en rayant certains éléments 
de $u$ et en gardant les autres (dans le meme ordre) pour constituer une nouvelle suite. Par exemple
$$
v=(u_1, u_4, u_5, u_{15}, u_{16}, u_{18}, u_{19}, u_{20}, u_{45}, \cdots)
$$
est une suite extraite de $u$. 
\bigskip

\Propriete []  Si $u\in\ob K^{\ob N}$ converge vers $\ell\in\ob K$, alors toute suite extraite $v$ de la suite $u$ converge également vers $\ell$. 
\bigskip


\Propriete []  Si $u\in\ob K^{\ob N}$ diverge vers $\ell\in\{-\infty,+\infty\}$, alors toute suite extraite $v$ de la suite $u$ diverge également vers $\ell$. 
\bigskip

{\bf Une méthode classique} pour montrer que $u$ diverge 
est d'exhiber deux suites $v$ et $w$ extraites de $u$ convergeant respectivement vers $\ell$ et $\ell'$ avec $\ell\neq\ell'$. 
\bigskip

\Application : La suite de terme général $u_n=(-1)^n$ diverge. 


\Section Relat, Relations de comparaison. 

\Subsection Morbid, Domination. 

\Definition []  On dit qu'une suite $u\in\ob C^{\ob N}$ est dominée par une suite $v\in\ob C^{\ob N}$ si, et~seulement~si il existe une constante $c\ge0$ telle que 
$$
\forall n\in\ob N , \qquad  |u_n|\le c|v_n| . \eqdef{Garwall}
$$
On note alors $u_n=O(v_n)$ (une notation pratique mais ``pas'' standard est $u_n\ll v_n$). 
\bigskip

\Remarque : En pratique, on comparera presque toujours $u_n$ à une suite $v_n$ dont les termes sont (strictement) positifs. Dans ce cas particulier, la relation \eqref{Garwall} devient 
$$
\forall n\in\ob N , \qquad  |u_n|\le cv_n . 
$$
De plus, on peut toujours se ramener à cette situation en posant $w_n:=|v_n|$ dans \eqref{Garwall}. 
Ainsi, on a 
$$
u_n=O(v_n)\Longleftrightarrow u_n=O\b(|v_n|\b).
$$ 
\bigskip

\Propriete []  Une suite $u\in\ob C^{\ob N}$ est dominée par une suite $v\in\ob C^{\ob N}$ 
dont tous les termes sont non nuls si, et~seulement~si la suite de terme général $\ds w_n:={u_n\F v_n}$ est bornée. 

\Propriete []  Soient $u$ , $v$, $u'$, $v'$ et $w$ des suites complexes. 
$$
\eqalign{
&\mbox{Si $u_n=O(v_n)$ et si $v_n=O(w_n)$ alors $u_n=O(w_n)$,}
\cr
&\mbox{Si $u_n=O(w_n)$ et si $v_n=O(w_n)$ alors, $\forall(\lambda,\mu)\in\ob C^2$, 
on a $\lambda u_n+\mu v_n=O(w_n)$,}
\cr
&\mbox{Si $u_n=O(u_n')$ et si $v_n=O(v_n')$ alors $u_nv_n=O(u_n'v_n')$,}
\cr
&\mbox{Si $u_n=O(v_n)$ alors, pour chaque entier $n\in\ob N^*$, on a $(u_n)^k=O\b((v_n)^k\b)$,}
\cr
&\mbox{Si $u_n=O(v_n)$ et si $\forall n\in\ob N, \quad u_n\neq0$, alors on a $\ds{1\F v_n}=O\Q({1\F u_n}\W)$.}
}
$$

\Subsection Angel, suites négligeables. 

\Definition []  On dit qu'une suite $u\in\ob C^{\ob N}$ est négligeable devant une suite $v\in\ob C^{\ob N}$ si, et~seulement~si il existe un entier $N\in\ob N$ et une suite complexe $\epsilon=(\epsilon_n)_{n\ge N}$ telle que
$$
\forall n\ge N , \qquad  u_n=\epsilon_nv_n \qquad\mbox{et}\qquad{\lim_{n\to+\infty}\epsilon_n=0}. \eqdef{philo}
$$
On note alors $u_n=o(v_n)$. 
\bigskip

\Remarque : En pratique, on comparera presque toujours $u_n$ à une suite $v_n$ dont les termes sont (strictement) positifs. On peut toujours se ramener à cette situation car la suite définie par 
$$
w_n:=\Q\{
\eqalign{
0\mbox{ si }v_n=0,\cr
{v_n\F |v_n|}\mbox{ si }v_n\neq0
}
\W.
$$
satisfait les relations $\forall n\in\ob N, \ |w_n|\le 1$ et $v_n=\epsilon_nw_n$ de sorte que 
$$
\forall n\ge N,\qquad u_n=\underbrace{\epsilon_n}_{\to 0}v_n=\underbrace{(\epsilon_n w_n)}_{\to0}|v_n|
$$
En particulier, on a 
$$
u_n=o(v_n)\Longleftrightarrow u_n=o(|v_n|)
$$
\bigskip


\Propriete []  $u\in\ob C^{\ob N}$ est négligeable devant une suite $v\in\ob C^{\ob N}$ dont tous les termes sont non nuls 
si, et~seulement~si la suite de terme général $\ds w_n:={u_n\F v_n}$ converge vers $0$. 


\Propriete []  Soient $u$ , $v$, $u'$, $v'$ et $w$ des suites complexes. 
$$
\eqalign{
&\mbox{Si $u_n=o(v_n)$ et si $v_n=o(w_n)$ alors $u_n=o(w_n)$,}
\cr
&\mbox{Si $u_n=o(w_n)$ et si $v_n=o(w_n)$ alors, $\forall(\lambda,\mu)\in\ob C^2$, 
on a $\lambda u_n+\mu v_n=o(w_n)$,}
\cr
&\mbox{Si $u_n=o(u_n')$ et si $v_n=o(v_n')$ alors $u_nv_n=o(u_n'v_n')$,}
\cr
&\mbox{Si $u_n=o(v_n)$ alors, pour chaque entier $n\in\ob N^*$, on a $(u_n)^k=o\b((v_n)^k\b)$,}
\cr
&\mbox{Si $u_n=o(v_n)$ et si $\forall n\in\ob N, \quad u_n\neq0$, alors on a $\ds{1\F v_n}=o\Q({1\F u_n}\W)$.}
}
$$

\Subsection Angel, suites équivalentes. 

\Propriete []  La relation $\sim$ définie pour deux suites $u$ et $v$ de nombres complexes par 
$$
u_n\sim v_n\Longleftrightarrow u_n=v_n+o(v_n)
$$
est une relation d'équivalence appelée ``équivalence des suites''. La phrase mathématique $u_n\sim v_n$ 
se lit ``la suite $u$ est équivalente à la suite $v$'' ou plutôt "$u_n$ est équivalent à $v_n$''. 
\bigskip
 
\Propriete []  Deux suites $u\in\ob C^{\ob N}$ et $v\in\ob C^{\ob N}$ sont équivalentes 
si, et~seulement~s'il existe un entier $N\in\ob N$ et une suite complexe $\alpha=(\alpha_n)_{n\ge N}$ telle que
$$
\forall n\ge N , \qquad  u_n=\alpha_nv_n \qquad\mbox{et}\qquad{\lim_{n\to+\infty}\alpha_n=1}. 
$$

\Propriete []  $u\in\ob C^{\ob N}$ est équivalente àune suite $v\in\ob C^{\ob N}$ dont tous les termes sont non nuls si, et~seulement~si 
la suite de terme général $\ds w_n:={u_n\F v_n}$ converge vers $1$. 

\Propriete []  Si $u_n\sim v_n$, le signe de $u_n$ 
est ègal à celui de $v_n$ à partir d'un certain rang (par signe, on entend ici ``strictement négatif'', ``nul'' et ``strictement positif''). 
\bigskip

\Propriete []  Soient $u$ , $v$, $u'$, $v'$ et $w$ des suites complexes. 
$$
\eqalign{
&\mbox{Si $u_n\sim v_n$ et si $v_n\sim w_n$ alors $u_n\sim w_n$,}
\cr
&\mbox{Si $u_n\sim v_n$ alors $v_n\sim u_n$}
\cr
&\mbox{Si $u_n\sim u_n'$ et si $v_n\sim v_n'$ alors $u_nv_n\sim u_n'v_n'$,}
\cr
&\mbox{Si $u_n\sim v_n$ alors, pour chaque entier $n\in\ob N^*$, on a $(u_n)^k\sim(v_n)^k$,}
\cr
&\mbox{Si $u_n\sim v_n$ et si $\forall n\in\ob N, \quad u_n\neq0$, alors on a $\ds{1\F u_n}\sim{1\F v_n}$.}
}
$$

 Attention...on n'a pas le droit d'ajouter des équivalents en général, c'est illégal. Pour 
$$
u_n:=1, \quad u_n':=1,\quad v_n:=-1\quad \mbox{et}\quad v_n':=-1+{1\F n}, 
$$
par exemple, on a $u_n\sim u_n'$, on a $v_n\sim v_n'$ et pourtant on a $u_n+v_n=0\not\sim\ds{1\F n}=u_n'+v_n'$. 
\bigskip

\Propriete []  Soit $u\in\ob C^{\ob N}$ et $\ell\in\ob K^*$. Alors, on a
$$
\eqalign{ 
\lim_{n\to+\infty}u_n=\ell\quad&\Longleftrightarrow\quad u_n\sim\ell,
\cr
\lim_{n\to+\infty}u_n=0\quad&\Longleftrightarrow\quad u_n=o(1)
}
$$

\Theoreme [Title=Comparaison des suites de référence; $a>1$, $\alpha> 0$ et $\beta> 0$] 
$$
\ln(n)^\beta=o(n^\alpha), \qquad n^\alpha=o(a^n)\quad\mbox{et}\quad a^n=o(n!).
$$

\Theoreme [Title=Formule de Stirling] 
$$
\ds n!\sim\Q({n\F \e}\W)^n\sqrt{2\pi n}.
$$

\Section ofLight, suites monotones et limites. 
\bigskip

\Theoreme  Une suite croissante (resp. décroissante) $u\in\ob R^{\ob N}$ converge si, 
et~seulement~si elle est majorée (resp. minorée). Et dans ce cas, on a 
$$
\lim_{n\to+\infty}u_n=\sup_{n\in\ob N}u_n\qquad\qquad\Q(\mbox{resp. }\lim_{n\to+\infty}u_n=\inf_{n\in\ob N}u_n\W).
$$ 

\Propriete Si $u\in\ob R^{\ob N}$ est une suite croissante non majorée (resp. décroissante non minorée), 
la suite $u$ diverge et on a 
$$
\lim_{n\to+\infty}u_n=\sup_{n\in\ob N}u_n=+\infty\qquad\qquad\Q(\mbox{resp. }\lim_{n\to+\infty}u_n=\inf_{n\in\ob N}u_n=-\infty\W).
$$ 


\Theoreme [Title=Théorème des suites adjacentes]
Soit $u\in\ob R^{\ob N}$ une suite croissante et $v\in\ob R^{\ob N}$ une suite décroissante telles que 
$$
\lim_{n\to+\infty}(v_n-u_n)=0.
$$
Alors, les suites $u$ et $v$ convergent vers la même limite $\ell\in\ob R$. 

\Remarque : des suites $u$ et $v$ vérifiant les hypothèses du théorème précédent sont dites ``adjacentes'' 
\bigskip

\Theoreme [Title=Théorème des segments emboités] Soit $\{[u_n,v_n]\}_{n\in\ob N}$ une famille de segments de $\ob R$ vérifiant 
$$
\forall n\in\ob N,\qquad [u_{n+1},v_{n+1}]\subset[u_n,v_n]\qquad \mbox{et}\qquad \underbrace{\mbox{longueur de}[u_n,v_n]\to0}_{\lim(v_n-u_n)=0}
$$
Alors, il existe un nombre réel $\ell$ tel que 
$$
\bigcap_{n\in\ob R}[u_n,v_n]=\{\ell\}.
$$








\hautspages{Olus Livius Bindus}{Dérivation}

\pagetitretrue


\Chapter fonc, Dérivation. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip

\Section Gfoncder, Dérivée en un point, fonction dérivée. 

\Subsection gah, Définition locale. 

\Definition []  Soit $I$ un intervalle et soit $f:I\to\ob K$ une fonction. On dit que la fonction $f$ est dérivable en $a\in I$ si, et~seulement~si, il existe un nombre $\ell\in\ob K$ tel que 
$$
\lim_{\ss x\to a\atop\ss x\neq a}{f(x)-f(a)\F x-a}=\ell. 
$$
Ce nombre $\ell$, qui est unique et qu'on note $f'(a)$, est appelé ``nombre dérivé de $f$ en $a$''. 
\bigskip

\Exemple. l'identité $x\mapsto x$ est dérivable en chaque point $a\in\ob R$. 

\Definition []  Soit $I$ un intervalle et soit $f:I\to\ob K$ une fonction. On dit que $f$ est dérivable à gauche (resp. à droite) 
en $a\in I$ si, et~seulement~si, il existe un nombre $\ell\in\ob K$ tel que 
$$
\lim_{\ss x\to a\atop\ss x<a}{f(x)-f(a)\F x-a}=\ell\qquad\qquad\Q(\mbox{resp. }\lim_{\ss x\to a\atop\ss x>a}{f(x)-f(a)\F x-a}=\ell\W). 
$$
Ce nombre $\ell$, qui est unique, est noté $f'(a^-)$ (resp. $f'(a^+)$ et est appelé ``nombre dérivé à gauche (resp. à droite) 
de la fonction $f$ en $a$''. 
\bigskip

\Remarque : Le nombre dérivée est la limite des taux d'accroissements en $a$ et on utilise aussi le changement de variable $x=a+h$. pour écrire que 
$$
f'(a)=\lim_{\ss h\to 0\atop\ss h\neq 0}{f(a+h)-f(a)\F h}, 
$$


\Propriete []  Soit $I$ un intervalle, soit $f:I\to\ob K$ une fonction et soit $a\in i$ qui n'est pas une extrémité de $I$. 
Alors, la fonction $f$ est dérivable en $a$, si~et~seulement~si $f$ est dérivable à gauche et à droite en $a$ avec $f'(a^-)=f'(a^+)$. Dans ce cas, on a alors
$$
f'(a^-)=f'(a)=f'(a^+).
$$
\bigskip

\Exemple. La valeur absolue $x\mapsto|x|$ n'est pas dérivable en $0$. 
\bigskip

\Propriete []  Soit $I$ un intervalle et soit $f:I\to\ob K$ une fonction. Si $f$ est dérivable en $a\in I$, alors $f$ est continue en $a$. 
\bigskip

\Remarque : En d'autres mots, la continuité en $a$ est une condition nécéssaire pour que $f$ soit dérivable en $A$. 
\bigskip

\Theoreme []  Soient $I$ un intervalle réel, soit $f:I\to\ob C$ une fonction complexe et soient $g:I\to\ob R$ et $h:I\to\ob R$ les fonctions (parties réelles et parties imaginaires de $f$) définies implicitement par  
$$
\forall x\in I,\qquad f(x)=g(x)+ih(x).
$$
Alors, les fonctions réelles $g$ et $h$ sont dérivables en $a\in I$ si, et~seulement~si la fonction $f$ 
est dérivable en $a$. Et dans ce cas, on a 
$$
f'(a)=g'(a)+ih'(a). 
$$

\Subsection gah2, Définition globale. 


\Definition []  Soit $E$ un ensemble et soit $f:E\to\ob C$ une application. On dit que la fonction $f$ est dérivable sur l'ensemble $D\subset E$ si, et~seulement~si la fonction $f$ est dérivable en chaque point $x\in D$. Alors, on note $f'$ et on appelle fonction dérivée de $f$ l'application définie par 
$$
\eqalign{f: D&\to\ob K\cr x&\mapsto f'(x)}.
$$ 

\Remarque { 1 }: la fonction dérivée $f'$ de l'application $f$ est parfois notée $\ds {\d f\F \d x}$ ou encore $Df$. 
\bigskip

\Remarque { 2 }: la dérivabilité en un point $a$ est une notion locale alors que la dérivabilité sur un intervalle est une notion globale. La définition précédente permettant de faire le lien entre le local et le global. 
\bigskip

\Remarque { 3 }:Il existe des fonctions continues sur $\ob R$ qui ne sont dérivables en aucun point. 

\Subsection gah2, Opérations. 

 \quad
\Theoreme []  Soit $I$ un intervalle réel, soit $a\in I$ et soient $f:I\to\ob K$ et $g:I\to\ob K$ 
\pn
deux fonctions dérivables en $a$. 
Alors, pour $\lambda\in\ob K$, les fonctions $\lambda.f$, $f+g$ et $f\times g$ sont dérivables en $a$ et 
$$
\eqalign{
(\lambda.f)'(a)&=\lambda.f'(a),\cr
(f+g)'(a)&=f'(a)+g'(a),\cr
(fg)'(a)&=f'(a)g(a)+f(a)g'(a).
}
$$
De plus, si $g(a)\neq 0$, la fonction $f/g$, qui est définie au voisinage de $a$, est dérivable en $a$ et on a 
$$
\Q({f\F g}\W)'(a)={f'(a)g(a)-f(a)g'(a)\F g(a)^2}.
$$

\Remarque : si $g(a)\neq0$ et si $g$ est dérivable en $a$, la fonction $1/g$ est dérivable en $a$ et on a 
$$
\Q({1\F g}\W)'(a)=-{g'(a)\F g(a)^2}.
$$
En particulier, on peut dériver le quotient $f/g=f\times{1\F g}$ en écrivant 
$$
\Q({f\F g}\W)'(a)=f'(a)\times {1\F g(a)}+f(a)\times{-g'(a)\F g(a)^2}.
$$

\Remarque : Les fonctions polynômes $x\mapsto P(x)=\sum_{k=0}^n a_xx^k$ sont dérivables sur $\ob R$. 
\bigskip

\Remarque  : Les fractions rationnelles $x\mapsto {P(x)\F Q(x)}$ (quotient de deux polynômes) sont dérivables sur $\ob R$ privé des points annulant le dénominateur. 
\bigskip

 
\Theoreme []  Soient $I$ et $J$ deux intervalles. Si la fonction $f:I\to\ob R$ 
est dérivable en $a\in I$, si la fonction $g:J\to\ob K$ est dérivable en $f(a)\in J$ et si on a 
$$
\forall x\in I, \qquad f(x)\in J,
$$
Alors la fonction $g\circ f$, qui est définie sur $I$, est dérivable en $a$ et on a 
$$
(g\circ f)'(a)=f'(a)\times g'\circ f(a)=f'(a)\times g'\B(f(a)\B).
$$


 
\Theoreme []  Soient $I$ et $J$ deux intervalles et soit $f:I\to J$ une bijection. 
Si $f$ adment en $a$ un nombre dérivé $f'(a)\neq0$, alors, sa bijection réciproque $f^{-1}:J\to I$ est dérivable en $b=f(a)$ et on a 
$$
{\b(f^{-1}\b)'(b)={1\F f'(a)}={1\F f'\B(f^{-1}(b)\B)}}.
$$

\Subsection gah2, Dérivées itérées. 

\Definition []  Soit $I$ un intervalle, soit $f:I\to\ob K$ une application et soit $n\ge2$ un entier. Alors, on dit que $f$ est $n$ fois dérivable sur $I$ si, et~seulement~si, la fonction $f$ est dérivable sur $I$ et si $f'$ est $n-1$ fois 
dérivable sur $I$. Dans ce cas, on pose 
$$
\forall x\in I,\qquad f^{(n)}(x):=(f')^{(n-1)}(x)
$$


\Remarque : Soit $I$ un intervalle et soit $f:I\to\ob K$ une fonction. Par convention, on note 
$$
\forall x\in I, \qquad f^{(0)}(x):=f(x).
$$
La fonction $f^{(0)}=f$ est parfois appelée dérivée d'ordre $0$ de la fonction $f$. 
\bigskip

\Remarque : Soit $n\in\ob N$, soit $I$ un intervalle et soit $f:I\to\ob K$ une application $n$ fois dérivable sur $I$. 
Alors, on a 
$$
\forall x\in I, \qquad f^{(n)}(x):=\underbrace{{\d\F \d x}{\d\F\d x}{\d\F \d x}\cdots{\d\F \d x}{\d\F \d x}}_{\ds n\mbox{ dérivations successives}}\!\!\!\!f(x)
$$

\Remarque : la dérivée $n^\ieme$ de la fonction $f$ se note $\ds f^{(n)}$, ou $\ds{\d^n\F\d x^n}f$ ou $\ds D^nf$. 
\bigskip

\Propriete []  Soit $I$ un intervalle, soit $n\ge2$ et soit $f:I\to\ob K$ une application $n$ fois dérivable sur $I$. alors, on a $$
\forall p\in\{0,\cdots, n\}, \qquad \forall x\in I, \qquad f^{(n)}(x)=\Q(f^{(p)}\W)^{(n-p)}(x).
$$

\Remarque : étant donné un intervalle non vide $I$, on note $\sc C^0(I,\ob K)$ l'espace vectoriel 
des applications $f:I\to\ob K$, continues sur $I$. 
\bigskip

\Definition []  Pour $n\in\ob N^*$, on note $\sc C^n(I,\ob K)$ l'ensemble des applications $f:I\to\ob K$ 
dérivables sur $I$, dont la dérivée $f'$ appartient à $\sc C^{n-1}(I,\ob K)$. 
\bigskip


\Propriete []  Pour $n\in\ob N$, l'ensemble $(\sc C^n(I,\ob K), +, .)$ est l'ensemble des fonctions $f:I\to\ob K$ de classe $\sc C^n$ sur $I$, c'est à dire des applications $f:I\to\ob K$ dérivables $n$ fois sur $I$ dont les dérivées $f$, $f'$, $f'', \cdots, f^{(n)}$ sont continues sur $I$. 
\bigskip

\Definition []  Soit $I$ un intervalle et soit $f:I\to\ob K$. Alors, on dit que $f$ est de classe $\sc C^\infty$ sur $I$ si, et~seulement~si $f$ est de classe $\sc C^n$ sur $I$ pour chaque entier $n\in\ob N$. 
\bigskip

\Definition []  
On note $\sc C^\infty(I,\ob K)$ l'ensemble des fonctions $f:I\to\ob K$ de classe $\sc C^\infty$ sur $I$. 
\bigskip

\Propriete []  Pour $n\in\ob N\cup\{\infty\}$, l'ensemble $(\sc C^n(I,\ob K), +, .)$ est un $\ob K$-espace vectioriel. 
\bigskip

\Propriete []  Pour chaque entier $n\ge0$, on a $\sc C^{n+1}(I,\ob K)\subset\sc C^n(I,\ob K)$. 
\bigskip

\Remarque : la suite des ensembles $\sc C^n(I,\ob K)$ (décroissante au sens de l'inclusion) est ainsi construits 
par récurence à partir de $n=1$. A noter que l'on note souvent $\sc C^n(I)$ plutôt que $\sc C^n(I,\ob K)$. 
\bigskip

\Subsection gah2, Opérations sur les dérivées itérées. 



\Theoreme []  Soit $I$ un intervalle réel, soit $n\in\ob N^*$ et soient $f:I\to\ob K$ et $g:I\to\ob K$ 
\pn
deux fonctions $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. 
Alors, pour $(\lambda,\mu)\in\ob K^2$, la fonction $\lambda f+\mu g$ est $n$ fois dérivable 
(resp. de classe $\sc C^n$) sur $I$ et on a 
$$
\forall x\in I, \qquad {\ds(\lambda f+\mu g)^{(n)}(x)=\lambda f^{(n)}(x)+\mu g^{(n)}(x)}
$$

\Theoreme [Title=Formule de Leibniz] Soit $I$ un intervalle réel, soit $n\in\ob N^*$ et soient $f:I\to\ob K$ et $g:I\to\ob K$ 
deux fonctions $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. Alors, la~fonction $f\times g$ 
est $n$ fois dérivable sur $I$ (resp. de classe $\sc C^n$) et on a 
$$
\forall x\in I, \qquad {\ds(f\times g)^{(n)}(x)=\sum_{0\le k\le n}{n\choose k}f^{(k)}(x)g^{(n-k)}(x)}.
$$


\Propriete []  Soit $I$ un intervalle réel, soit $n\in\ob N^*$ et soient $f:I\to\ob K$ et $g:I\to\ob K$ 
\pn
deux fonctions $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$ telle que 
$$
\forall x\in I, \qquad g(x)\neq 0.
$$ 
Alors, la fonction $f/g$ est $n$ fois dérivable (resp. de classe $\sc C^n$) sur $I$. 

\Remarque : Les fonctions polynômes $x\mapsto P(x)=\sum_{k=0}^n a_xx^k$ sont de classe $\sc C^\infty$ 
sur $\ob R$. 
\bigskip

\Remarque  : Les fractions rationnelles $x\mapsto {P(x)\F Q(x)}$ (quotient de deux polynômes) sont de classe $\sc C^\infty$ sur $\ob R$ privé des points annulant le dénominateur. 
\bigskip

 
\Theoreme []  Soient $I$ et $J$ deux intervalles. Si la fonction $f:I\to\ob R$ 
est dérivable en $a\in I$, si la fonction $g:J\to\ob K$ est dérivable en $f(a)\in J$ et si on a 
$$
\forall x\in I, \qquad f(x)\in J,
$$
Alors la fonction $g\circ f$, qui est définie sur $I$, est $n$ fois dérivable (resp. de classe $\sc C^n$) sur $I$. 

\Section , Etude globale des fonctions dérivables. 


\Subsection gah, Extremums locaux. 

\Definition []  Soit $I$ un intervalle et $f:I\to\ob R$ une fonction dérivable. Alors, le point $a\in I$ est un point critique de la fonction $f$ si, et~seulement~si $f'(a)=0$. 
\bigskip

\Propriete []  Soit $I$ un intervalle et $f:I\to\ob R$ une fonction dérivable et soit $a\in I$. Si $f$ admet un extremum local en $a$ et si $a$ n'est pas une extrémité de l'intervalle $I$, alors $a$ est un point critique de $f$. 
\bigskip

\Remarque : Cette propriété est fondamentale car elle permet de réduire la recherche des extremums locaux à l'étude d'un nombre fini de points. 
\bigskip

\Remarque : Pour trouver le maximum d'une fonction sur un segment $[a,b]$ par exemple, on commence par chercher les points critiques de $f$ sur l'intervalle $\Q]a,b\W[$ puis on regarde si ces points critiques sont des maxima et on les compare aux valeurs au bors du segment (c'est à dire en $a$ et en $b$).
\bigskip

\Remarque  : L'algorithme d'étude précédent est extrémement performant si on l'utilise en conjonction 
avec le théorème de compacité (une fonction continue sur un segment admet un maximum et un minimum global). 
\bigskip

Exercice :  Trouver le maximum de la fonction $x\mapsto x^n(1-x)$ sur $\Q[0,+\infty\W[$. 
\bigskip

\Subsection grr, Théorème de Rolle, accroissements finis, prolongement $\sc C^1$. 

\Theoreme [Title=Théorème de Rolle] Soit $(a,b)\in\ob R^2$ avec $a<b$ et soit $f:[a,b]\to\ob R$ une fonction continue et dérivable sur $\Q]a,b\W[$ telle que $f(a)=f(b)$. Alors, il existe $c\in\Q]a,b\W[$ tel que $f'(c)=0$. 
\bigskip

\Theoreme [Title=Egalité des accroissements finis] 
Soit $f:[a,b]\to\ob R$ continue sur $[a,b]$ et dérivable sur $\Q]a,b\W[$. 
Alors, il existe $c\in\Q]a,b\W[$ tel que $$
f(b)-f(a)=f'(c)(b-a).
$$ 

\Theoreme [Title=Inégalités des accroissements finis] Soit $f:[a,b]\to\ob R$ une fonction dérivable et soit $(m,M)\in\ob R^2$ tels que 
$$
\forall x\in[a,b], \qquad m\le f'(x)\le M.
$$
alors, on a 
$$
m(b-a)\le f(b)-f(a)\le M(b-a)
$$
\bigskip

\Theoreme [Title=Inégalités des accroissements finis II]
Soit $f:[a,b]\to\ob K$ une fonction dérivable et soit $M\in\ob R$ tels que 
$$
\forall x\in[a,b], \qquad \b|f'(x)\b|\le M.
$$
alors, on a 
$$
\b|f(b)-f(a)\b|\le M|b-a|
$$

\Remarque : si la dérivée $f'$ d'une application $f:I\to\ob C$ est majorée par $M$ sur $I$, alors, l'application $f$ est $M$-lipschitzienne sur $I$. 
\bigskip 

Exercice :  Etudier la limite de la suite définie par récurence par $u_0\in\ob R$ et $u_{n+1}=\arctan(u_n)$ 
pour $n\ge1$. 
\bigskip

\Theoreme [Title=Théorème de prolongement $\sc C^1$]
Soit $f:[a,b]\to\ob C$ une fonction continue sur $[a,b]$, de classe $\sc C^1$ sur $\Q]a,b\W]$ (resp. sur $\Q[a,b\W[$) telle $f'$ admette une limite finie en $a$ (resp. en $b$). Alors, la fonction $f$ est de classe $\sc C^1$ sur $[a,b]$. 
\bigskip

\Subsection gah, Fonctions monotones et constantes. 

\Theoreme []  Soit $I$ un intervalle et $f:I\to\ob K$ une fonction dérivable sur $I$. Alors
$$
\underbrace{ \exists c\in\ob K: \quad\forall x\in I,\quad f(x)=c}_{\mbox{$f$ est constante sur $I$}}\qquad
\Longleftrightarrow\qquad \underbrace{\forall x\in I,\quad f'(x)=0}_{\mbox{$f'$ est identiquement nulle sur $I$}}.
$$


\Propriete []  Soit $I$ un intervalle et $f:I\to\ob R$ une fonction dérivable sur $I$. Alors : \pn
Si $f'(x)\ge0$ (resp. $f'(x)>0$) pour chaque $x\in I$, alors la fonction $f$ est (resp. strictement) croissante sur $I$. \pn
Si $f'(x)\le0$ (resp. $f'(x)< 0$) pour chaque $x\in I$, alors la fonction $f$ est (resp. strictement) décroissante sur $I$. \pn
\bigskip

\Remarque : La conclusion de cette propriété est encore vraie si l'on suppose seulement que 
$f$ est dérivable et vérifie les relations précédentes sur l'intervalle $I$ privé de ses extrémités. 
\bigskip

\Exemple. prouver que $x\mapsto arcsin(x)$ est strictement croissante sur $[-1,1]$. 
\bigskip

\Propriete []  Soit $I$ un intervalle et $f:I\to\ob R$ une fonction dérivable sur $I$. Alors, 
si $f$ est croissante sur $I$ (resp. décroissante), on a $f'(x)\ge 0$ (resp. $f'(x)\le 0$) pour $x$ dans $I$. 
\bigskip

\Remarque : il existe des fonctions strictement croissantes dont la dérivée s'annule (par exemple $x\mapsto x^3$ sur $\ob R$). 
\bigskip

\Subsection gah, Fonctions convexes. 

\Definition []  Soit $I$ un intervalle. Une application $f:I\to\ob R$ est dite convexe (resp. concave) sur l'intervalle $I$, si~et~seulement~si pour chaque couple $(x,y)\in I^2$ et chaque couple $(a,b)\in[0,1]^2$ de nombres vérifiants $a+b=1$, on a 
$$
f(ax+by)\ge af(x)+bf(y)\qquad\qquad\B(\mbox{resp. }f(ax+by)\le af(x)+bf(y)\B).
$$

$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-1.1,-3)(4,1.2)
\psaxes*[labels=none,ticks=none]{->}(-0.5,-2)(-1,-3)(4,1.1)
\psplot[linecolor=blue,plotpoints=1000]{0.001}{10}{x log}
\psplot[linecolor=red,plotpoints=1000]{0.02}{3.52}{0.02 log 3.52 log 0.02 log sub x 0.02 sub 3.52 div mul add}
\parametricplot[plotstyle=curve,linecolor=magenta, linestyle=dashed]{0}{1}{1.3 0.02 log 3.52 log 0.02 log sub 1.3 0.02 sub 3.52 div mul add t mul 1.3 log 1 t sub mul add}
\rput{0}(1.3,0.5){\blue $f(ax+by)$}
\rput{0}(2.2,-1.3){\red $af(x)+bf(y)$}
\rput{0}(1.31,-0.9){\black $\times$}
\rput{0}(1.31,0.1){\black $\times$}
\endpspicture
$$
\Figure FigConv, Concavité d'une fonction. 
\medskip

\Remarque : une fonction est convexe (resp. concave) si tout sous-arc de son graphe est sous (resp. au dessus de ) sa corde.
\bigskip

\Propriete []  Soit $I$ un intervalle et $f:I\to\ob R$ une fonction convexe (sur $I$). Alors, pour chaque entier $n\ge2$ et chaque $n$-uplet $(x_1,\cdots,x_n)\in I^n$, on a 
$$
(\lambda_1,\cdots\lambda_n)\in\Q[0,+\infty\W[^n\mbox{ et }\sum_{1\le k\le n}\lambda_k=1\Longrightarrow 
f\Q(\sum_{1\le k\le n}\lambda_kx_k\W)\le \sum_{1\le k\le n}\lambda_k f(x_k).
$$

\Propriete []  Soit $f:I\to\ob R$ une fonction de classe $\sc C^1$ sur un intervalle $I$. Alors, la fonction $f$ est convexe (resp. concave) sur $I$ si, et~seulement~si, la dérivée $f'$ est croissante (resp. décroissante) sur $I$. 
\bigskip

\Remarque : Si elle est convexe (resp. concave), une courbe de classe $\sc C^1$ est située au dessus (resp. au dessous) de ses tangentes. 
\bigskip

\Remarque  : Etant donnée une courbe convexe (resp. concave), les cordes dont l'une extrémité est fixée sur la courbe et dont l'autre extrémité décrit la courbe (dans le sens positif) sont 
de pente croissante (resp. décroissante). 
\bigskip

\Propriete []  Soit $f:I\to\ob R$ une fonction de classe $\sc C^2$ sur un intervalle $I$. Alors, la fonction $f$ est convexe (resp. concave) sur $I$ si, et~seulement~si, la dérivée seconde $f''$ est positive (resp. négative) sur $I$. 
\bigskip
















\hautspages{Olus Livius Bindus}{Intégration}

\pagetitretrue


\Chapter fonc, Intégration. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip

\Section lab0, Fonctions de classe $\sc C^k$ par morceaux. 

\Subsection lab1, subdivisions et fonctions en escalier. 

\Definition []  Une subdivision d'un segment $[a,b]$ est une suite strictement croissante finie $\sigma=\{x_0,x_1,\cdots, x_n\}$ telle que $x_0=a$ et $x_n=b$. Autrement dit, on a
$$
a=\underbrace{x_0<x_1<\cdots <x_{n-1} < x_n}_{\mbox{subdivision $\sigma$}}=b. 
$$ 

\Remarque : on peut munir l'ensemble $E$ des subdivisions du segment $[a,b]$ d'une relation d'ordre : on dit qu'une subdivision $\sigma\in E$ est plus fine qu'une subdivision $\sigma'\in E$, et on note $\sigma\prec\sigma'$ si la subdivision $\sigma$ contient tous les points de $\sigma'$ (et éventuellement d'autres en plus). 
\bigskip

\Definition []  Une fonction $f:[a,b]\to\ob K$ est dite en escalier sue le segment $[a,b]$ si, et~seulement~s'il existe une subdivision $\sigma=\{x_0,\cdots,x_n\}$ de $[a,b]$ et des nombres $c_1,\cdots,c_n$ de $\ob K$ tels que 
$$
\forall i\in\{1,\cdots, n\}, \quad \forall x\in\Q]x_{i-1},x_i\W[,\qquad f(x)=c_i
$$ 
Autrement dit, la fonction $f$ est constante sur chaque intervalle ouvert $\Q]x_{i-1},x_i\W[$ pour $1\le i\le n$. 
\bigskip

\Remarque : une telle subdivision $\sigma$ est alors dite adaptée (ou subordonnée) à $f$. 
\bigskip

\Remarque  : toute subdivision plus fine que $\sigma$ est alors adaptée à la fonction $f$. 
\bigskip

\Exemple.  La partie entière est une fonction continue par morceaux sur tout segment réel. 
\bigskip

\Propriete []  L'ensemble des fonctions en escalier sur le segment $[a,b]$ est strable par addition, par multiplication externe et par produit. En particulier, c'est un sous-espace vectoriel de $\sc F\b([a,b],\ob K\b)$. 
\bigskip

\Subsection lab2, Fonctions de classe $\sc C^k$ par morceaux. 

\Definition []  Etant donné un segment $[a,b]$ et $k\in\ob N$, on dit qu'une fonction $f:[a,b]\to\ob K$ est 
de classe $\sc C^k$ par morceaux sur $[a,b]$ s'il existe une subdivision $\sigma=\{x_0,\cdots,x_n\}$ du segment $[a,b]$ telle que, pour $1\le i\le n$, la restriction à l'intervalle $\Q]x_{i-1},x_i\W[$ de $f$ soit prolongeable en une fonction de classe $\sc C^k$ sur le segment $[x_{i-1},x_i]$. 
\bigskip

\Remarque : autrement dit, pour $1\le i\le n$, la fonction $f$ doit être de classe $\sc C^k$ sur l'intervalle ouvert $\Q]x_{i-1},x_i\W[$ et les dérivées $f, f',\cdots, f^{(k)}$ doivent admetre une limite à droite en $x_{i-1}$ et une limite à gauche en $x_i$. 
\bigskip

\Remarque : une telle subdivision $\sigma$ est alors dite adaptée (ou subordonnée) à~$f$. 
\bigskip

\Remarque : on se moque totalement des valeurs de $f$ aux points $x_0, x_1, \cdots, x_n$. 
\bigskip


\Propriete []  L'ensemble des fonctions de classe $\sc C^k$ sur le segment $[a,b]$ est stable par addition, par multiplication externe et par produit. En particulier, c'est un sous-espace vectoriel de $\sc F\b([a,b],\ob K\b)$. 
\bigskip

\Remarque : la composée $g\circ f$ d'une fonction $g$ de classe $\sc C^k$ avec une fonction $f$ de classe $\sc C^k$ par morceaux est de classe $\sc C^k$ par morceaux. 
\bigskip

\Theoreme [$f:{[a,b]}\to\ob R$ fonction continue par morceaux sur ${[a,b]}$] 
Pour chaque $\epsilon>0$, il existe deux fonction réelles $\varphi$ et $\psi$ en escalier sur $[a,b]$ telles que 
$$
\varphi\le f\le \psi\qquad \mbox{et}\qquad \psi-\varphi\le \epsilon.
$$

\Section lab4, Intégrale d'une fonction continue par morceaux. 

\Subsection lab5, Intégrale d'une fonction en escalier. 

\Definition []  Soit $f:[a,b]\to\ob C$ une fonction en escalier sur $[a,b]$, avec $a<b$. Alors, pour chaque subdivision $\sigma=\{x_0,\cdots,x_n\}$ de $[a,b]$ adaptée à la fonction $f$, la somme 
$$
\sum_{1\le i\le n}(x_i-x_{i-1})f(y_i)
$$
est indépendant du choix de la subdivision $\sigma$ et des nombres $y_i\in\Q]x_{i-1},x_i\W[\ \,(1\le i\le n)$ utilisés pour la calculer et appartient au corps $\ob K$. On l'appelle intégrale de la fonction en escalier $f$ sur le segment $[a,b]$ et on la note 
$$
\int_{[a,b]}f= \int_a^bf:=\sum_{1\le i\le n}(x_i-x_{i-1})f\underbrace{\Q({x_{i-1}+x_i\F 2}\W)}_{\mbox{un choix de $y_i$}}\qquad\mbox{(par exemple)}
$$
Par convention, on pose 
$$
\int_b^af:=-\int_a^bf\qquad \mbox{et}\qquad \int_a^af=0.
$$

\Remarque : l'intégrale d'une fonction en escalier est l'aire (algèbrique) de la zone délimitée par l'axe des abscisses et le graphe de la fonction $f$ pour $a\le x\le b$. 
\bigskip

\Remarque : la notation $\int_{[a,b]}f$ n'a de sens que si $a\le b$ contrairement à la notation $\int_a^bf$. En pratique, on utilise une variable muette (qui appartient à l'intégrale et pas à vous) 
pour préciser par rapport à quelle variable on intégre. On notera ainsi
$$
\int_{[a,b]}f(x)\d x\qquad \mbox{ou}\qquad \int_a^bf(x)\d x
$$
Le ``$\d x$'' ne sert qu'à préciser le choix de la lettre utilisé pour la variable muette. Veuillez ne pas l'oublier tout de même. 
\bigskip

\Remarque  : le fait de modifier les valeurs de la fonction en un nombre fini d'endroits ne modifie pas son intégrale. 
\bigskip

\Propriete [Title=Relation de Chasles]
Soient $(a,b,c)\in\ob R^3$ et $f:[a,c]\to\ob K$ une fonction en escalier sur le segment d'extrémités $\min\{a,b,c\}$ et $\max\{a,b,c\}$. Alors, la fonction $f$ est en escalier sur les segments $[a,b]$, $[b,c]$, $[a,c]$ et on a 
$$
\int_a^cf=\int_a^bf+\int_b^cf. 
$$

\Propriete [Title=Linéarité de l'intégrale]
Soit $(a,b)\in\ob R^2$ et soient $f:[a,b]\to\ob K$ et $g:[a,b]\to\ob K$ deux fonctions en escalier sur $[a,b]$. Alors, pour $(\lambda,\mu)\in\ob K^2$, la fonction $\lambda f+\mu g$ est en escalier sur $[a,b]$ et on a 
$$
\int_a^b(\lambda f+\mu g)=\lambda \int_a^bf+\mu \int_a^bg. 
$$

\Propriete[Title=positivité de l'intégrale] Soient $a\le b$ deux nombres réels et soit $f:[a,b]\to\ob R$ une fonction en escalier sur $[a,b]$. Alors, 
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\ge0}_{f\ge0}\qquad\Longrightarrow\qquad \int_a^bf\ge0.
$$ 
\bigskip


\Propriete [Title=Croissance de l'intégrale]
Soient $a\le b$ deux nombres réels et soient $f:[a,b]\to\ob R$ et $g:[a,b]\to\ob R$ deux fonctions en escalier sur $[a,b]$. Alors, on a 
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\le g(x)}_{f\le g}\qquad\Longrightarrow\qquad \int_a^bf\le \int_a^bg.
$$ 

\Propriete[Title=module]
Soient $a\le b$ deux nombres réels et soit $f:[a,b]\to\ob R$ une fonction 
en escalier sur $[a,b]$. Alors, l'application $|f|$ est en escalier sur $[a,b]$ et on a 
$$
\Q|\int_a^bf\W|\le \int_a^b|f|.
$$ 
\bigskip

\Subsection lab6, Intégrale d'une fonction continue par morceaux. 

\Definition []  Soient $(a,b)\in\ob R^2$ avec $a< b$ et $f:[a,b]\to\ob R$ une application continue par morceaux sur $[a,b]$. 
Alors, il existe une suite croissante $(\varphi_n)_{n\in\ob N}$ et une suite décroissante $(\psi_n)_{n\in\ob N}$ de fonctions en escaliers sur $[a,b]$ telles que 
$$
\forall n\in\ob N, \qquad \varphi_n\le f\le \psi_n\qquad\mbox{et}\qquad \psi_n-\varphi_n\le {1\F n+1}.\eqdef{gah}
$$
Alors, les suite $(\int_a^b\varphi_n)_{n\in\ob N}$ et $(\int_a^b\psi_n)_{n\in\ob N}$ convergent vers une même limite $\ell$, qui ne dépend pas du choix des suites $(\varphi_n)_{n\in\ob N}$ et $(\psi_n)_{n\in\ob N}$ vérifiant \eqref{gah}. 
Cette limite $\ell$ est appelée intégrale de la fonction $f$ sur le segment $[a,b]$ et on la note
$$
\int_{[a,b]}f= \int_a^bf:=\lim_{n\to+\infty}\int_a^b\varphi_n=\lim_{n\to+\infty}\int_a^b\psi_n.
$$
Par convention, on pose 
$$
\int_b^af:=-\int_a^bf\qquad \mbox{et}\qquad \int_a^af=0.
$$

\Remarque: la première chose que l'on fait AVANT d'intégrer une fonction sur $[a,b]$, c'est de vérifier qu'elle est bien intégrable $[a,b]$, c'est-à-dire qu'elle est continue par morceaux sur $[a,b]$. 
\bigskip

\Remarque : le fait de modifier les valeurs de la fonction en un nombre fini d'endroits ne modifie pas son intégrale. 
\bigskip

\Definition []  Soient $(a,b)\in\ob R^2$, soit $f:[a,b]\to\ob C$ une application continue par morceaux sur $[a,b]$ et soient $g:[a,b]\to\ob R$ et $h:[a,b]\to\ob R$ les parties réelles et imaginaires de la fonction $f$. Autrement dit 
$$
\forall x\in[a,b], \qquad f(x)=\underbrace{g(x)}_{\re f(x)}+i\underbrace{h(x)}_{\im f(x)}
$$
Alors, l'intégrale de la fonction $f$ de $a$ à $b$ est par définition le nombre 
$$
\int_a^bf:=\int_a^bg+i\int_a^bh.
$$

\Remarque : l'intégrale d'une fonction sur $[a,b]$ d'une fonction continue par morceaux est (par définition) l'aire (algèbrique) de la zone délimitée par l'axe des abscisses et le graphe de la fonction $f$ pour $a\le x\le b$. 
\bigskip
\Remarque  : le fait de modifier les valeurs de la fonction en un nombre fini de points ne modifie pas son intégrale. 
\bigskip

\Subsection lab6, Propriétés fondamentales de l'intégrale. 
\bigskip

\Propriete [Title=Relation de Chasles]
Soient $(a,b,c)\in\ob R^3$ et $f:[a,c]\to\ob K$ une fonction continue par morceaux sur le segment d'extrémités $\min\{a,b,c\}$ et $\max\{a,b,c\}$. Alors, la fonction $f$ est continu epar morceaux sur les segments $[a,b]$, $[b,c]$, $[a,c]$ et on a 
$$
\int_a^cf=\int_a^bf+\int_b^cf. 
$$

\Propriete [Title=Linéarité de l'intégrale]
Soit $(a,b)\in\ob R^2$ et soient $f:[a,b]\to\ob K$ et $g:[a,b]\to\ob K$ deux fonctions continues par morceaux sur $[a,b]$. Alors, pour $(\lambda,\mu)\in\ob K^2$, la fonction $\lambda f+\mu g$ est continue par morceaux sur $[a,b]$ et on a 
$$
\int_a^b(\lambda f+\mu g)=\lambda \int_a^bf+\mu \int_a^bg. 
$$

\Propriete [Title=positivité de l'intégrale]
Soient $a\le b$ deux nombres réels et soit $f:[a,b]\to\ob R$ une fonction continue par morceaux sur $[a,b]$. Alors, 
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\ge0}_{f\ge0}\qquad\Longrightarrow\qquad \int_a^bf\ge0.
$$ 
\bigskip


\Propriete [Title=Croissance de l'intégrale]
Soient $a\le b$ deux nombres réels et soient $f:[a,b]\to\ob R$ et $g:[a,b]\to\ob R$ deux fonctions continues par morceaux sur $[a,b]$. Alors, on a 
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\ge g(x)}_{f\ge g}\qquad\Longrightarrow\qquad \int_a^bf\ge \int_a^bg.
$$ 

\Propriete [Title=module]
Soient $a\le b$ deux nombres réels et soit $f:[a,b]\to\ob R$ une fonction 
continue par morceaux sur $[a,b]$. Alors, l'application $|f|$ est continue par morceaux sur $[a,b]$ et on a 
$$
\Q|\int_a^bf\W|\le \int_a^b|f|.
$$ 
\bigskip

\Theoreme [$f:{[a,b]}\to\ob R^+$ fonction continue] 
$$
\underbrace{\forall x\in[a,b],\quad f(x)=0}_{f=0}\qquad\Longleftrightarrow \int_a^bf(t)\d t=0.
$$

\Subsection lab7, Moyennes. 
\bigskip

\Definition [$f:{[a,b]}\to\ob K$ fonction continue par morceaux] 
La valeur moyenne de la fonction $f$ sur le segment $[a,b]$ est le nombre
$$
m(f):={1\F b-a}\int_a^bf(x)\d x.
$$

\Remarque : si $f:[a,b]\to\ob R$ est continue par morceaux, on a 
$$
\inf_{a\le x\le b}f(x)\le m(f)\le \sup_{a\le x\le b}f(x).
$$

\Propriete{ \bf (inégalité de la moyenne)}. Soient $a\le b$ deux nombres réels et soient $f:[a,b]\to\ob K$ et $g:[a,b]\to\ob K$ deux fonctions continues par morceaux sur $[a,b]$. Alors, 
$$
\Q|\int_a^bfg\W|\le \sup_{a\le x\le b}\b|f(x)\b|\times\int_a^b|g|.
$$
En particulier, on a 
$$
\Q|\int_a^bf\W|\le (b-a)\sup_{a\le x\le b}\b|f(x)\b|.
$$


\Subsection lab8, Produit scalaire intégrale et inégalité de Cauchy-Schwartz.
\bigskip

\Propriete []  Soient $a< b$ des nombres réels. alors, l'application 
$$
\eqalign{\sc C\b([a,b],\ob R\b)^2&\to\ob R\cr(f,g)&\mapsto\langle f,g\rangle:=\int_a^bf(t)g(t)\d t}
$$
est un produit scalaire sur $\sc C\b([a,b],\ob R\b)$, c'est à dire une forme, bilinéaire, symétrique, définie, positive. 
\bigskip

\Remarque : la norme $\|\cdot\|$ associée au produit scalaire $\langle\cdot,\cdot\rangle$ est alors définie par 
$$
\forall f\in\sc C\b([a,b],\ob R\b), \qquad \|f\|:=\sqrt{\langle f,f\rangle}=\sqrt{\int_a^bf(x)^2\d x}.
$$

\Theoreme [Title=Inégalité de Cauchy-Schwarz; $a<b$] 
$$
\forall (f,g)\in\sc C\b([a,b],\ob R\b)^2, \qquad \Q|\int_a^bf(x)g(x)\d x\W|\le \sqrt{\int_a^bf(x)^2\d x}
\times\sqrt{\int_a^bg(x)^2\d x}.
$$

\Remarque : Cette inégalité peut s'écrire plus simplement sous la forme 
$$
\b|\langle f,g\rangle\b|\le \|f\|\times\|g\|.
$$

\Subsection lab9, Sommes de Riemann.

\Definition []  Soient $a<b$ deux nombres réels. Alors, le pas d'une subdivision $\sigma=\{x_0,x_1,\cdots,x_n\}$ du segment $[a,b]$ est le nombre réel défini par
$$
|\sigma|:=\max_{1\le k\le n}(x_k-x_{k-1}).
$$

\Definition []  Soient $a<b$ deux nombres réels et $f:[a,b]\to\ob K$ une fonction continue. Pour chuaqe subdivision $\sigma:=\{x_0,\cdots,x_n\}$ du segment $[a,b]$ et chaque famille $Y=\{y_1,\cdots, y_n\}$ vérifiant 
$$
\forall k\in\{1,\cdots, n\}, \qquad x_{k-1}\le y_k\le x_k, 
$$
on appelle Somme de Riemann la quantité définie par 
$$
R_{\sigma,Y}(f):=\sum_{1\le k\le n}(x_k-x_{k-1})f(y_k).
$$

\Theoreme [Title=Somme de Riemann;{$f:[a,b]\to\ob K$ fonction continue}] 
$$
\int_a^bf(x)\d x=\lim_{\mbox{$\ss\sigma$ \sevenrm subdivision de $\ss[a,b]$
}\atop\ss |\sigma|\to0}R_{\sigma,Y}(f)
$$
Autrement dit, l'intégrale de $f$ sur le segment $[a,b]$ est la limite des sommes de Riemann associées à des subdivisions de $[a,b]$ dont le pas converge vers $0$. En particulier, on a 
$$
\eqalign{
\int_a^bf(x)\d x&=\lim_{n\to+\infty}{b-a\F n}\sum_{k=0}^{n-1}f\Q(a+k{(b-a)\F n}\W)
\cr
&=\lim_{n\to+\infty}{b-a\F n}\sum_{k=1}^nf\Q(a+k{(b-a)\F n}\W)
}
$$

Exercice :  Calculer $\ds\lim_{n\to+\infty}\sum_{1\le k\le n}{n\F k^2+n^2}$. 

\Propriete []  Soient $a<b$ deux nombres réels et $f:[a,b]\to\ob K$ une fonction $K$-lipschitzienne. Alors, pour chaque subdivision $\sigma$ de $[a,b]$, on a 
$$
\int_a^bf(x)\d x=R_{\sigma,Y}(f)+O(|\sigma|).
$$

\Remarque : Methode des trapezes pour calculer des intégrales. Plutôt que de calculer l'aire en utilisant des rectangles à l'aide de la formule $(x_{i+1}-x_i)f(y_i)$, on calcule l'aire en utilisant des trapèzes à l'aide de la formule 
$$
(x_{i+1}-x_i){f(x_i)+f(x_{i+1})\F 2}, 
$$
donnant l'aire du trapeze passant par $A(x_i,0)$, $B(x_i,f(x_i))$, $C(x_{i+1},0)$ et $D(x_{i+1},f(x_{i+1}))$. 
Ainsi, on a 
$$
\int_a^bf(t)\d t=\lim_{n\to+\infty}\sum_{i=1}^n(x_{i+1}-x_i){f(x_i)+f(x_{i+1})\F 2}
$$






\hautspages{Olus Livius Bindus}{Intégration et dérivation}

\pagetitretrue


\Chapter fonc, Intégration et dérivation. 
\bigskip

\noindent
Dans tout ce chapitre, le symbole $\ob K$ désigne le corps $\ob K=\ob R$ ou le corps $\ob K=\ob C$. 
\bigskip

\Section pri0, Primitives. 

\Subsection prir, Lien entre intégrale et primitive. 


\Definition []  Une primitive sur un intervalle $I$ d'une application $f:I\to\ob K$ est une fonction $F:I\to\ob K$ dérivable (sur $I$) telle que $f$ soit la dérivée de $F$ sur $I$. Autrement dit 
$$
\forall x\in I, \qquad F'(x)=f(x). 
$$

\Propriete []  Soit $I$ un intervalle et $F:I\to\ob K$ une primitive d'une fonction $f:I\to\ob K$. 
Alors, on a 
$$
G:I\to\ob K\mbox{ est une primitive de f sur }I\quad\ \Longleftrightarrow\quad\  \exists c\in\ob K : \quad \forall x\in I, \quad G(x)=F(x)+c
$$

\Remarque : on pourra retenir que deux primitives d'une même fonction différent d'une constante. 
\bigskip

\Theoreme [Title=Théorème fondamental de l'analyse;$f:I\to\ob K$ continue sur un intervalle $I$, $a\in I$] 
L'unique primitive de $f$ qui s'annule en $a$ est l'application $F$ définie par 
$$
\eqalign{F: I&\to\ob K\cr x&\mapsto  \int_a^xf(t)\d t}
$$
Par ailleurs, la fonction $F$ est de classe $\sc C^1$ sur $I$


\Propriete []  Soit $I$ un intervalle et $a\in I$. Pour toute primitive $F$ sur $I$ d'une fonction $f:I\to\ob K$, on a 
$$
\forall x\in I, \qquad \int_a^x f(t)\d t=[F]_a^x=F(x)-F(a). 
$$
En particulier, pour toute fonction $f$ ce classe $\sc C^1$ sur $I$, on a 
$$
\int_a^xf'(t)\d t=[f]_a^x=f(x)-f(a). 
$$

\Subsection pri0, Théorèmes fondamentaux. 

\Theoreme [Index=Theoreme@Théorème!Integration par partie@Intégration par partie;Title=Intégration par partie]
Soit $(a,b)\in\ob R^2$ avec $a< b$ et soient $f$ et $g$ deux fonctions de classe $\sc C^1$ sur le segment $[a,b]$. Alors, on a 
$$
\int_a^bf(t)g'(t)\d t=[fg]_a^b-\int_a^bf'(t)g(t)\d t. 
$$

\Theoreme [Index=Theoreme@Théorème!de changement de variable;Title=Changement de variable]
Soit $(a,b)\in\ob R^2$ avec $a< b$, soit $\varphi:[a,b]\to I$ une fonction de classe $\sc C^1$ et $f:I\to\ob K$ une fonction continue. Alors, on a 
$$
\int_a^bf\b(\varphi(x)\b)\varphi'(x)\d x=\int_{\varphi(a)}^{\varphi(b)}f(t)\d t.
$$

\Theoreme [Index=Theoreme@Théorème!de changement de variable;Title=Changement de variable] 
Soit $(a,b)\in\ob R^2$ avec $a<b$ et soit $f:[a,b]\to\ob K$ une fonction continue. Soit $I$ un intervalle et soit $\varphi:I\to[a,b]$ un difféormorphisme de classe $\sc C^1$ (i.e. une bijection $\varphi:I\to[a,b]$ de classe $\sc C^1$ sur $[a,b]$ dont la bijection réciproque $\varphi^{-1}$ est de classe $\sc C^1$ sur $I$). alors, 
on a 
$$
\int_a^bf(t)\d t=\int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}f\b(\varphi(x)\b)\varphi'(x)\d x. 
$$

\Section prir1, Calcul de primitives. 

\Remarque : Une primitive $F$ quelconque d'une fonction $f:I\to\ob K$ sur un intervalle $I$ est notée
$$
F(x)=\int f(x)\d x\qquad (x\in I).
$$
C'est une notation extrémement pratique qui sera utilisée en conjonction avec intégration par partie et changment de variable pour trouver rapidements des primitives. 
\bigskip

\Remarque : les primitives d'une fonction $f$ sur un intervalle $I$ étant unique à une constante près, il est important de se rappeler qu'il y a une constante additive sous-jacente à la notation $\int f(x)\d x$. 
{\bf NE JAMAIS OUBLIER LA CONSTANTE ! }
\bigskip

\Subsection rahh, Polynômes.

\Concept [Index=Primitives!des fonctions du type@des fonctions du type $x^n$] Monômes

\Propriete []  Pour chaque nombre réel $n\in\ob N$, on a 
$$
\int x^n\d x={x^{n+1}\F n+1}+c\qquad (x\in\ob R).
$$

\Concept [[Index=Primitives!des polynomes@des polynômes] Polynômes 

\Propriete : Pour chaque polynôme $P(x)=\ds\sum_{k=0}^da_kx^k$, on a 
$$
\int P(x)\d x=\sum_{k=0}^da_k{x^{k+1}\F k+1}+c=c+\sum_{k=1}^{d+1}a_{k-1}{x^k\F k}\qquad (x\in\ob R).
$$

\Remarque : on sait intégrer facilement les polynômes et la primitive d'un polynôme de degré $d$ 
est un polynôme de degré $d+1$. 
\bigskip

\Subsection rahhhhh, Fractions rationnelles.
\bigskip

\Concept [Index=Primitives!des fonctions du type@des fonctions du type ${1\F (x-b)^n}$] Fonctions du type ${1\F (x-b)^n}$. 

\Propriete []  Soit $a\in\ob R$, soit $n\ge2$ et soit $I$ un intervalle réel ne contenant pas $a$. Alors, on a 
$$
\eqalign{
&\int{1\F x-a}\d x=\ln|x-a|+c\qquad(x\in I),
\cr
&\int{1\F(x-a)^n}\d x={1\F 1-n}\times{1\F (x-a)^{n-1}}+c\qquad (x\in I).
}
$$ 

\Concept [Index=Primitives!des fonctions du type@des fonctions du type ${x\F (x^2+1)^n}$]
Fonctions du type ${x\F (x^2+1)^n}$

\Propriete []  Soit $n\ge2$. Alors, on a 
$$
\eqalign{
&\int{x\F x^2+1}\d x={1\F 2}\ln(x^2+1)+c\qquad(x\in\ob R),
\cr
&\int{x\F(x^2+1)^n}\d x={1\F 2(1-n)}\times{1\F (x^2+1)^{n-1}}+c\qquad (x\in\ob R).
}
$$ 

\Concept [Index=Primitives!des fonctions du type@des fonctions du type ${1\F (x^2+1)^n}$] Fonctions du type ${1\F (x^2+1)^n}$

\Propriete []  Soit $e\in\ob R$. 
Pour calculer $\ds \int{1\F(x^2+1)^n}\d x$, on procède par récurence en utilisant que 
$$
\eqalign{
\int{1\F x^2+1}\d x&=\arctan(x)+c\qquad(x\in\ob R),
\cr
\int{1\F(x^2+1)^{n+1}}\d x&={2n-1\F 2n}\int{1\F(x^2+1)^n}\d x+{1\F 2n}{x\F (x^2+1)^n}\qquad (x\in I). 
}
$$

\Remarque : on trouve la seconde relation en intégrant $\int{\d x\F (x^2+1)^n}$ par parties en écrivant~que 
$$
{1\F (x^2+1)^n}={x^2+1\F (x^2+1)^n+1}={1\F (x^2+1)^n+1}
+\underbrace{x}_{\mbox{à dériver}}\times \underbrace{{x\F(x^2+1)^{n+1}}}_{\mbox{à intégrer}}
$$
d'où 
$$
\int{1\F (x^2+1)^n}\d x =\int{1\F (x^2+1)^{n+1}}\d x -{1\F 2n}{x\F(x^2+1)^{n+1}}+{1\F 2n}\int{1\F (x^2+1)^n}\d x
$$

\Concept [Index=Primitives!des fonctions du type@des fonctions du type ${ax+b\F (x^2+cx+d)^n}$] Fonctions du type ${ax+b\F (x^2+cx+d)^n}$

\Remarque : Pour trouver les primitives de la fonction $\ds{ax+b\F(x^2+cx+d)^n}$ avec $\delta=4d-c^2< 0$, on met d'abord sous la forme canonique 
$$
{ax+b\F(x^2+cx+d)^n}={ax+b\F \b((x+c/2)^2+\delta\b)^n}
$$
puis on procède au changement de variable $u=x+c/2$ pour ecrire que 
$$
\int{ax+b\F(x^2+cx+d)^n}\d x=\int {ax+b\F \b((x+c/2)^2+\delta\b)^n}\d x=\int {au+b-ac/2\F \b(u^2+\delta\b)^n}\d u
$$
puis au changement de variable $u=t\sqrt\delta$ pour en déduire que 
$$
\int{ax+b\F(x^2+cx+d)^n}\d x=\int {au+b-ac/2\F \b(u^2+\delta\b)^n}\d u
={\sqrt\delta\F\delta^{n/2}}\int {ta\sqrt\delta +b-ac/2\F \b(t^2+1\b)^n}\d t
$$
Et ensuite, on sait calculer....
\bigskip


\Concept [Index=Fractionsrationnelles@Fractions rationnelles!definition@définition] Fractions rationnelles

\Definition []  Une fraction rationnelle à coefficients dans $\ob K$ est le quotient $F={P\F Q}$ de deux polynômes à coefficients dans $\ob K$, avec $Q\neq0$.
Le degré d'une fraction rationnelle $F$ non nulle (i.e. pour laquelle $P\neq0$) est le nombre 
$$
\deg(F)=\deg(P)-\deg(Q).
$$
Les pôles de la fraction ratrionnelle $F$ sont les racines du polynôme $Q$. La valuation d'un pole $b$ est le plus grand entier $n$ tel que 
$$
0=Q(b)=Q'(b)=\cdots=Q^{(n)}(b)
$$
\bigskip


\Propriete []  Pour chaque fraction rationnelle $F$, il existe une unique décomposition en éléments simples 
sur le corps $\ob C$ de la forme
$$
F(x)={P(x)\F Q(x)}=E(x)+{R(x)\F Q(x)}=E(x)+{R(x)\F \alpha\prod_{1\le i\le p}(x-b_i)^n_i}
=E(x)+\sum_{1\le i\le p}\sum_{1\le j\le n_i}{a_{i,j}\F (x-b_i)^j}
$$
où $E$ est un polynôme, où $a_{i,j}$ est un nombre complexe, où $b_i$ est un pôle de la fraction
rationnelle $F$ et ou $n_i$ est la multiplicité du pole $b_i$. 
\bigskip

\Remarque : pour trouver cette décomposition. Effectuer d'abord la division euclidienne du polynôme $P$ par le polynôme $Q$ pour trouver la partie entière $E$ du quotient et le reste~$R$ de la division euclidienne. 
\bigskip

\Remarque : puis on ecrit le développement sous la forme 
$$
\tilde F(x):={R(x)\F Q(x)}=\sum_{1\le i\le p}\ \sum_{1\le j\le n_i}{a_{i,j}\F (x-b_i)^j}
$$
Et on détermine les coefficients $a_{i,j}$ en considérant la parité de $R$ et $Q$, en calculant des valeurs en certains points $x$ de la fraction rationnelle $\tilde F$, des limites du type $\tilde F(x)x^k$ en $\pm\infty$ ou encore en utilisant la formule (bourrine mais qui marche toujours) 
$$
a_{i,j}={1\F (n_i-j)!}\Q(\tilde F(x)(x-b)^{n_i}\W)^{(n_i-j)}(b).
$$

\Propriete []  Pour chaque fraction rationnelle $F$ réelle, il existe une unique décomposition en éléments simples 
sur le corps $\ob R$ de la forme
$$
F(x)=E(x)+\sum_{1\le i\le p}\sum_{1\le j\le n_i}\ {a_{i,j}\F (x-b_i)^j}
+\sum_{\le i\le p'}\ \sum_{1\le j\le m_i}{\alpha_{i,j}x+\beta_{i,j}\F (x-z_i)(x-\overline z_i)}
$$
où $E$ est un polynôme réel, où $a_{i,j}$ est un nombre réel, où $b_i$ est un pôle réel de la fraction
rationnelle $F$ et ou $n_i$ est la multiplicité du pole $b_i$, où $\alpha_{i,j}$ et $\beta_{i,j}$ sont deux nombres réels, où $z_i$ est un pôle complexe non réel de la fraction
rationnelle $F$ et ou $m_i$ est la multiplicité des poles conjugués $z_i$ et $\overline{z_i}$.
\bigskip

\Remarque : pour trouver cette décomposition. On effectue la décomposition en éléments simples sur $\ob C$ puis on rassemble les termes complexes conjugués pour obtenir la décomposition 
en éléments simples sur $\ob R$. 
\bigskip

\Concept [Index=Primitives!des fractions rationnelles] Intégration des fractions rationnelles

Pour intégrer une fraction rationnelle, on la décompose en éléments simples puis on intégre chaque élément simple.
\medskip

\Remarque : C'est la décomposition en éléments simples sur $\ob R$ qui permet d'intégrer les fractions rationnelles réelles, donc il faut savoir la faire. 
\bigskip

\Subsection lol, Fractions rationnelles de fonctions élémentaires. 

\Concept [Index=Primitives!des fonctions du type@des fonctions du type $F(\cos x,\sin x)$] Fraction rationnelle $F(\cos x,\sin x)$. 

\Propriete []  Soit $F$ une fraction rationnelle. Pour calculer une primitive, de la fonction $G:x\mapsto F(\cos x,\sin x)$, on peut utiliser plusieurs méthodes : \pn
1) (méthode générale) On procède au changement de variable $x=2\arctan t$, \pn c'est à dire $t=\tan{x\F2}$, pour obtenir que 
$$
\int F(\cos x,\sin x)\d x=\int F\Q({1-t^2\F1+t^2},{2t\F 1+t^2}\W)\times{2\F1+t^2}\d t
$$
et se ramener au calcul d'une primitive d'une fraction rationnelle. 
\medskip
\noindent
2) (méthode partielle) on pose $x=\arcsin t$ , c'est à dire $t=\sin x$ sur un intervalle ou $\sin'(x)=\cos(x)>0$ par exemple) pour obtenir que 
$$
\int F(\cos x,\sin x)\d x=\int F\Q(t,\sqrt{1-t^2}\W){1\F\sqrt{1-t^2}}\d t
$$
Si la fonction $G$ est impaire, cela marche bien en général. 
\medskip
\noindent
3) (méthode partielle) on pose $x=\arccos t$ , c'est à dire $t=\cos x$ sur un intervalle ou $\cos'(x)=-\sin(x)> 0$ par exemple) pour obtenir que 
$$
\int F(\cos x,\sin x)\d x=-\int F\Q(t,-\sqrt{1-t^2}\W){1\F\sqrt{1-t^2}}\d t
$$


\Concept [Index=Primitives!des fonctions du type@des fonctions du type $F(\ch x,\sh x)$] Fraction rationnelle $F(\ch x,\sh x)$. 

\Propriete []  Soit $F$ une fraction rationnelle. Pour calculer une primitive, de la fonction $G:x\mapsto F(\ch x,\sh x)$, on peut utiliser plusieurs méthodes : \pn
1) (méthode générale) On procède au changement de variable $x=2\argth t$, c'est à dire $t=\th{x\F2}$, pour obtenir que 
$$
\int F(\ch x,\sh x)\d x=\int F\Q({1+t^2\F1-t^2},{2t\F 1-t^2}\W)\times{2\F1-t^2}\d t
$$
et se ramener au calcul d'une primitive d'une fraction rationnelle. 
\medskip
\noindent
2) (méthode partielle) on pose $x=\argsh t$ , c'est à dire $t=\sh x$, pour obtenir que 
$$
\int F(\ch x,\sh x)\d x=\int F\Q(\sqrt{1+t^2},t\W){1\F\sqrt{1+t^2}}\d t
$$

\medskip
\noindent
3) (méthode partielle) on pose $x=\argch t$ , c'est à dire $t=\ch x$ sur un intervalle ou $\ch'(x)=-\sh(x)> 0$ par exemple) pour obtenir que 
$$
\int F(\ch x,\sh x)\d x=-\int F\Q(t,-\sqrt{t^2-1}\W){1\F\sqrt{t^2-1}}\d t
$$
Si la fonction $G$ est impaire, cela marche bien en général. 



\Concept [Index=Primitives!des fonctions du type@des fonctions du type $F\Q(x,\root n\of{ax+b \F cx+d}\W)$] Fraction rationnelle $F\Q(x,\root n\of{ax+b \F cx+d}\W)$ avec $ad-bc\neq0$. 

\Propriete []  Soit $F$ une fraction rationnelle et soient $(a,b,c,d)\in\ob R^4$ tels que $ad-bc\neq0$. 
Pour calculer une primitive, de la fonction $x\mapsto F(x, \root n\of{ax+b \F cx+d})$, on procède au changement de variable 
$t=\root n\of{ax+b \F cx+d}$ pour obtenir que 
$$
\int F\Q(x, \root n\of{ax+b \F cx+d}\W)\d x=\int F\Q(-{dt^n-b\F ct^n-a}, t\W)\times n{ad-bc\F(ct^n-a)^2}t^{n-1}\d x
$$

peut utiliser plusieurs méthodes : \pn
1) (méthode générale) On procède au changement de variable $x=2\argth t$, c'est à dire $t=\th{x\F2}$, pour obtenir que 
$$
\int F(\ch x,\sh x)\d x=\int F\Q({1+t^2\F1-t^2},{2t\F 1-t^2}\W)\times{2\F1-t^2}\d t
$$
et se ramener au calcul d'une primitive d'une fraction rationnelle. 



\Concept [Index=Primitives!des fonctions du type@des fonctions du type $F(x, \sqrt{ax^2+bx+c})$]  Fraction rationnelle $F(x, \sqrt{ax^2+bx+c})$ avec $a\neq0$. 

\Propriete []  Soit $F$ une fraction rationnelle et soient $(a,b,c)\in\ob R^3$ tels que $a\neq0$. 
Pour calculer une primitive, de la fonction $x\mapsto F(x, \sqrt{ax^2+bx+c})$, on commence par metttre $ax^2+bx+c$ sous la forme canonique pour obtenir que 
$$
\int F\Q(x,\sqrt{ax^2+bx+c}\W)\d x=\int F\Q(x, \sqrt{a\Q[\Q(x-{b\F2a}\W)^2-\Delta\W]}\W)\d x
$$
Selon le signe du discriminant, on procède alors aux changements de variables qu'il faut pour faire apparaitre : \pn
la fonction $\sqrt{x^2-1}$ auquel cas, on pose ensuite $x=\ch(u)$. \pn
la fonction $\sqrt{x^2+1}$ auquel cas, on pose ensuite $x=\sh(u)$. \pn
la fonction $\sqrt{1-x^2}$ auquel cas, on pose ensuite $x=\cos(u)$ ou $x=\sin(u)$. 
\bigskip

\Section mdr, Formules de Taylor. 


\Theoreme [Title=Formule de Taylor]
Soit $n\in\ob N$ et $f:I\to\ob K$ une fonction de classe $\sc C^n$ sur $I$. Alors, on a 
$$
\forall(a,b)\in I^2, \qquad f(b)=\sum_{k=0}^{n-1}{f^{(k)}(a)\F k!}(b-a)^k+\underbrace{\int_a^b{(b-t)^{n-1}\F(n-1)!}f^{(n)}(t)\d t}_{R_n(b)}
$$

\Remarque : si $a$ et fixé, la relation précédente pour $x\in I$ s'écrit 
$$
\forall x\in I, \qquad f(x)=\underbrace{\sum_{k=0}^{n-1}{f^{(k)}(a)\F k!}(x-a)^k}_{\mbox{\sevenrm Polynôme }T_n(x)}+\underbrace{\int_a^x{(x-t)^{n-1}\F(n-1)!}f^{(n)}(t)\d t}_{R_n(x)}
$$
si de plus $a=0$, on déduit du changement de variable $t=ux$ que 
$$
\forall x\in I, \qquad f(x)=\sum_{k=0}^{n-1}{f^{(k)}(0)\F k!}x^k+x^n\int_0^1{(1-t)^{n-1}\F(n-1)!}f^{(n)}(tx)\d t.
$$
\bigskip


\Theoreme [Title=Inégalité de Taylor-Lagrange] 
Soit $n\in\ob N$ et $f:I\to\ob K$ une fonction de classe $\sc C^n$ sur $I$. Alors, on a 
$$
\forall(a,b)\in I^2, \qquad\Q|f(b)-\sum_{k=0}^{n-1}{f^{(k)}(a)\F k!}(b-a)^k\W|\le{(b-a)^n\F n!}\sup_{a\le x\le b}|f^{(n)}(x)|
$$

\Remarque : si $a$ et fixé et si l'intervalle $I$ est un segment , la relation précédente pour $x\in I$ se traduit par 
$$
\forall x\in I, \qquad f(x)=\sum_{k=0}^{n-1}{f^{(k)}(a)\F k!}(x-a)^k+O\b((x-a)^n\b)
$$
si de plus $a=0$, on déduit du changement de variable $t=ux$ que 
$$
\forall x\in I, \qquad f(x)=\sum_{k=0}^{n-1}{f^{(k)}(0)\F k!}x^k+O(x^n).
$$

\Section mdr, Développements limités. 

\Subsection def, Développement limité en un point. 
\bigskip

\Definition []  Soit $f:I\to\ob R$ une fonction et $a\in I$. On dit que $f$ admet le polynome $P(x)=\sum_{k=0}^n\alpha_k(x-a)^k$
 de degré inférieur ou égal à $n$ comme développement limité en~$a$ à l'ordre $n$ si, et~seulement~si 
$$
f(x)=P(x)+o_a\B((x-a)^n\B)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B).
$$
Le développement limité de $f$ en $a$ à l'ordre $n$ est unique. 
\bigskip

\Remarque : Si $P(x)=\sum_{k=0}^n\alpha_k(x-a)^k$ est le développement limité de $f$ en $a$, alors 
la troncature $Q(x)=\sum_{k=0}^m\alpha_k(x-a)^k$ à l'ordre $m$ (avec $0\le m\le n$) est le développement limité 
de $f$ en $a$ à l'ordre $m$. 
\bigskip

\Subsection def, Opérations algébriques. 

\Propriete []  Si $P(x)$ et $Q(x)$ sont les développements limités en $a$ des fonctions $f$ et $g$ à l'orde $n$, 
alors $(P+Q)(x)$ est le développement limité de $f+g$ en $a$ à l'ordre $n$. 
\bigskip


\Propriete []  Si $P(x)$ et $Q(x)$ sont les développements limités en $a$ des fonctions $f$ et $g$ à l'orde $n$, 
alors la troncature à l'ordre $n$ du polynôme $(P\times Q)(x)$ est le développement limité de $f\times g$ en $a$ à l'ordre $n$. 
\bigskip


\Propriete []  Si $P(x)$ et $Q(x)$ sont les développements limités en $a$ des fonctions $f$ et $g$ à l'orde $n$ 
et si $g(a)\neq0$, alors la division à l'ordre $n$ de $P$ par $Q$, suivant les puissances croissantes, 
en $a$ est le développement limité de $f/g$ en $a$ à l'ordre $n$. 
\bigskip

\Propriete []  Pour chaque entier $n\in\ob N$, on a 
$$
{1\F 1-u}=\sum_{k=0}^nu^k+o_0(u^n).
$$

\Remarque : on peut utiliser la propriété précédente pour calculer le développement limité d'un quotient (plutot que d'effectuer la division suivant les puissances croissantes). 
\bigskip

\Theoreme [Title=Formule de Taylor Young] 
Soit $f$ une fonction de classe $\sc C^n$ sur un intervalle $I$. 
Alors, la fonction $f$ admet un développement limité à l'ordre $n$ en tout point $a\in I$ donné par 
$$
f(x)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B)\qquad \mbox{avec}\qquad \alpha_k={f^{(k)}(a)\F k!}\quad\mbox{pour}\quad 0\le k\le n. 
$$

\Propriete []  Soit $f:I\to\ob K$ une fonction dérivable sur $I$ dont la dérivée admet en $a$ le développement limité à l'ordre $n$ suivant 
$$
f'(x)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B).
$$
Alors, la fonction $f$ admet en $a$ un développement limité à l'ordre $n+1$ donné par 
$$
f(x)=f(0)+\sum_{k=0}^{n+1}\alpha_k{(x-a)^{k+1}\F k+1}+o_a\B((x-a)^{n+1}\B)
$$

\Propriete []  Soit $f:I\to\ob K$ une fonction continue sur $I$ admettant en $a$ le développement limité à l'ordre $n$ suivant 
$$
f(x)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B).
$$
Alors, une primitive $F$ de $f$ admet en $a$ un développement limité à l'ordre $n+1$ donné par 
$$
F(x)=F(a)+\sum_{k=0}^{n+1}\alpha_k{(x-a)^{k+1}\F k+1}+o_a\B((x-a)^{n+1}\B)
$$

\Section mdr, Application aux courbes paramétrées. 

\Subsection courbespar, Comportement local. 
\bigskip

\Propriete []  Soit $I$ un intervalle, $f:I\to\ob R^2$ une courbe paramétrée de classe $\sc C^k$ et $a\in I$. S'ils existent, 
on note \smallskip\noindent
$p$ le plus petit entier $n\in\{1,\cdots, k\}$ tel que $f^{(n)}(a)\neq 0$, \smallskip\noindent
$q$ le plus petit entier $n\in\{p+1,\cdots,k\}$ tel que $\{f^{(p)}(a),f^{(n)}(t_0)\}$ 
forme une famille libre. \medskip\noindent
Alors, le comportement de l'application $t\mapsto f(a+t)$ en $0$ 
dans le repère 
$\b(O,\vec i,\vec j\b)$, où l'on a posé $ O=f(a) $, $\vec i=f^{(p)}(a)$ et $\vec j=f^{(q)}(a)$, 
est entièrement déterminer par la parité des nombres $p$ et $q$ et on a 
$$
f(a+h)= O +{h^p\F p!}\b(1+o(1)\b)\vec i+{h^q\F q!}\b(1+o(1)\b)\vec j\qquad(h\to0).
$$ 

\Remarque : La droite $(O,\vec i)$ est appelée droite tangente à la dourbe paramétré $(I,f)$ au point $O$. 
\medskip

\Subsection cour, Classification du comportement d'une courbe en un point. 
\bigskip

\Concept [] Point (pseudo) régulier

Si $p$ est impair et $q$ est pair (cas général), on dit que la courbe paramétrée $(I,f)$ présente un point (pseudo) régulier en $a$. Elle présente alors l'aspect suivant :

%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-1.5,-0.3)(2,2)
\rput{30}(0,0.5){
\psplot[linecolor=blue,plotpoints=1000]{-1.5}{1}{x x mul x add}
\psline[linecolor=magenta]{->}(-0.5,-0.25)(0.5,-0.25)
\psline[linecolor=red]{->}(-0.5,-0.25)(-0.7,0.75)
\rput{-30}(0.6,0){\magenta $\vec i$}
\rput{-30}(-0.8,0.95){\red $\vec j$}
\rput{-30}(-0.5,-0.5) $O$ 
\rput{70}(0.59,0.95){\blue\scalebox{1.2 1.2}{$>$}}
}
\endpspicture

\Figure FigConva, Point (pseudo) régulier.
\medskip


\Concept [] Point d'inflexion

Si $p$ est impair et $q$ est impair, on dit que la courbe paramétrée $(I,f)$ présente un point d'inflexion en $a$. Elle présente alors l'aspect suivant :

$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-1.5,-0.5)(2,2)
\rput{-20}(0,0.5){
\psplot[linecolor=blue,plotpoints=1000]{-2}{2}{x x x mul mul }
\psline[linecolor=magenta]{->}(0,0)(1,0)
\psline[linecolor=red]{->}(0,0)(0.2,0.75)
\rput{20}(1.1,0.2){\magenta $\vec i$}
\rput{20}(0.4,0.95){\red $\vec j$}
\rput{20}(0,-0.2){\blue $O$}
\rput{70}(0.98,0.95){\blue\scalebox{1.2 1.2}{$>$}}
}
\endpspicture
$$
\Figure FigConve, Point d'inflexion.
\medskip


\Concept [] Point de rebroussement de première espèce

Si $p$ est pair et $q$ est impair, on dit que la courbe paramétrée $(I,f)$ présente un point de rebroussement de première espèce en $a$. Elle présente alors l'aspect suivant :

$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-1.5,-0.5)(2,2)
\rput{30}(0,0.5){
\parametricplot[linecolor=blue,plotstyle=curve,plotpoints=100]{-2}{2}{t t mul t t t mul mul}
\psline[linecolor=magenta]{->}(0,0)(1,0)
\psline[linecolor=red]{->}(0,0)(0.3,0.8)
\rput{-30}(1.1,0.2){\magenta $\vec i$}
\rput{-30}(0.4,0.95){\red $\vec j$}
\rput{-30}(0,-0.2){\blue $O$}
\rput{120}(0.98,-0.95){\blue\scalebox{1.2 1.2}{$>$}}
}
\endpspicture
$$
\Figure FigConven, Point de rebroussement de première espèce.
\medskip


\Concept [] Point de rebroussement de seconde espèce

Si $p$ est pair et $q$ est pair, on dit que la courbe paramétrée $(I,f)$ présente un point de rebroussement de seconde espèce en $a$. Elle présente alors l'aspect suivant :

$$
%\psset{xunit=1cm, yunit=1cm}
\pspicture*[](-1.5,-0.5)(2,1.4)
\rput{30}(0,0.5){
\parametricplot[linecolor=blue,plotstyle=curve,plotpoints=100]{-2}{0}{t t mul t t t mul mul}
\parametricplot[linecolor=blue,plotstyle=curve,plotpoints=100]{0}{2}{t t mul 0 t t t t mul mul mul 3 div sub}
\psline[linecolor=magenta]{->}(0,0)(1,0)
\psline[linecolor=red]{->}(0,0)(0.3,-0.8)
\rput{-30}(1.1,0.2){\magenta $\vec i$}
\rput{-30}(0.5,-0.95){\red $\vec j$}
\rput{-30}(0,0.2){\blue $O$}
\rput{120}(0.98,-0.95){\blue\scalebox{1.2 1.2}{$>$}}
\rput{-30}(0.98,-0.35){\blue\scalebox{1.2 1.2}{$>$}}
}
\endpspicture
$$
\Figure FigConvena, Point de rebroussement de seconde espèce.
\medskip



\hautspages{Olus Livius Bindus}{Fonctions de plusieurs variables}

\pagetitretrue


\Chapter fonc, Fonctions de plusieurs variables. 
\bigskip


\Section Top, Continuité des fonctions de plusieurs variables.


\Subsection Topo, Normes et Topologie de $\ob R^n$. 

\Definition []  Soit $n\ge1$ un entier. Une norme de $\ob R^n$ est une application $N:\ob R^n\to\ob R^+$ vérifiant les propriétés suivantes : 
\Bullet L'application $N$ est bien définie et à valeurs réelles positives :
$$
\forall x\in\ob R^n,  \qquad N(x)\ge0 .
$$
\Bullet L'application $N$ est dite ``définie'' :
$$
\forall x\in\ob R^n , \qquad N(x)=0\ \Longleftrightarrow\ x=0 .
$$
\Bullet L'application $N$ est positivement homogène :
$$
\forall x\in\ob R^n , \quad  \forall \lambda\in\ob R ,\qquad N(\lambda x)=|\lambda|N(x).
$$
\Bullet L'application $N$ satisfait l'inégalité triangulaire :
$$
\forall x\in\ob R^n , \quad  \forall y\in\ob R^n , \qquad N(x+y)\le N(x)+N(y).\eqdef{eqtriangulaire}
$$

 Ne pas confondre cette propriété ``définie'' avec la propriété standard ``défini=existe". 
\medskip 


\Propriete []  L'application définie par 
$$
\forall x=(x,1,\cdots, x_n)\in\ob R^n, \qquad \b|\!\b|x\b|\!\b|:=\sqrt{x_1^2+x_2^2+\cdots+x_n^2}
$$
est une norme appelée norme euclidienne de $\ob R^n$. 
\bigskip


\Remarque :  Une norme $N$ satisfait nécéssairement les deux inégalités triangulaires 
$$
\forall x\in\ob R^n , \quad  \forall y\in\ob R^n , \qquad\b|N(x)-N(y)\b|\le N(x+y)\le N(x)+N(y).
$$


\Remarque :  Dans l'espace $\ob R^n$ muni de la norme euclidienne $\|\cdot\|$, la distance d'un point $x=(x_1,\cdots,x_n)$ à un point $y=(y_1,\cdots,y_n)$ 
est le nombre réel positif 
$$
\d(x,y):=\|x-y\|=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2}.
$$


\Remarque :  si $n=1$, la norme euclidienne de $\ob R=\ob R^n$ est la valeur absolue. 
\bigskip



\Definition []  La boule ouverte de centre $a\in\ob R^n$ et de rayon $r>0$ est l'ensemble 
$$
{
\sc B(a,r):=\b\{x\in\ob R^n:\|x-a\|<r\b\}}.
$$


\Definition []  Un ensemble $E\subset\ob R^n$ est ouvert si, et seulement si, pour chaque $x\in\ob E$, il existe une boule ouverte de centre $x$ et de rayon $r>0$ contenue dans $E$
$$
V\subset\ob R^n\mbox{ est ouvert}\quad\Longleftrightarrow\quad\forall x\in V, \quad \exists r>0:\quad \sc B(x,r)\subset V.
$$


\Definition []  Un ensemble $V\subset\ob R^n$ est un voisinage du point $a\in\ob R^n$ si, et~seulement~s'il contient une boule de centre $a$ et de rayon $r>0$. 
$$
V\subset\ob R^n\mbox{ est un voisinage de }a\in\ob R^n\quad\Longleftrightarrow\quad\exists r>0:\quad \sc B(a,r)\subset V.
$$


\Subsection LIM, Limites d'une fonction de plusieurs variables. 


\Definition []  Soient $n$ et $p$ deux entiers strictement positifs et soit $D\subset\ob R^n$. Alors, on dit qu'une fonction 
$f:D\to\ob R^p$ est une fonction (réelle si $p=1$ et vectorielle sinon) de $n$ variables. En particulier, on a 
$$
f:(\underbrace{x_1,\cdots, x_n}_{x\in\ob R^n})\mapsto f(x,_1,\cdots, x_n)=\underbrace{\B(f_1(x),f_2(x),\cdots,f_p(x)\B)}_{f(x)\in\ob R^p}.
$$ 
\bigskip


\Definition []  Une fonction de $n$ variables $f:D\subset\ob R^n\to \ob R^p$ admet la limite $\ell\in\ob R^p$ en un point $a\in\ob R^n$ si, et seulement si 
$$
\forall \epsilon> 0, \quad \exists \alpha>0:\qquad \forall x\in D\mbox{ vérifiant } \|x-a\|\le \alpha, \mbox{ on a }\|f(x)-\ell\|\le\epsilon 
$$
Cette limite $\ell$, si elle existe est unique. On la note $\ds\lim_{x\to a}f(x)$. 
\bigskip

\Remarque : On dit parfois la fonction $f$ converge/tends vers $\ell$ en $a$/quand $x$ tends vers~$a$ à la place de "la fonction admet la limite $\ell$ en $a$". 
\bigskip


\Remarque : Pour les fonction réelles, on peut également définir la notion de divergence vers $\pm\infty$ d'une fonction de $n$ variables. 
\bigskip

\Definition []  Une fonction réelle de $n$ variables $f:D\subset\ob R^n\to \ob R$ admet la limite $\ell=+\infty$ (resp. $\ell=-\infty$) en un point $a\in\ob R^n$ si, et seulement si 
$$
\forall M\in\ob R, \ \exists \alpha>0:\quad \forall x\in D\mbox{ vérifiant } \|x-a\|\le \alpha, \mbox{ on a } f(x)\ge M\quad(\mbox{resp. }f(x)\le M).
$$

\Remarque : Attention, lorsque l'on travaille avec des vecteurs $x\in\ob R^n$, on ne peut pas faire tendre $x$ vers l'infini (lequel ?), ni faire tendre $x$ vers $a$ par la droite ou par la gauche, cela n'a pas de sens. 
\bigskip


\Propriete []  Une fonction réelle de $n$ variables $f:D\subset\ob R^n\to \ob R^p$ admet la limite $\ell\in\ob R^p$ en un point $a\in\ob R^n$ si, et seulement si 
$$
{
\mbox{Pour $V$ voisinage de $\ell$, il existe $U$ voisinage de $a$ tel que }\forall x\in D\cap U, f(x)\in V}
$$


\Propriete []  une fonction $f:D\subset\ob R^n\to\ob R^p$ converge vers $\ell$ en $a$ si, 
et~seulement~si la fonction $f-\ell$ converge vers $0$ en $a$. 


\Propriete []  une fonction $f:D\subset\ob R^n\to\ob R^p$ converge vers $\ell$ en $a$ si, et~seulement~si la fonction $g:h\mapsto f(a+h)$ converge vers $\ell$ en $0$. 

\Remarque : les deux propriétés précédentes permet de transformer un problème de limite vers $\ell\in\ob R^p$ en $a\in\ob R^n$ 
en un problème de limite vers $0$ en $0$, qui est {\it a priori}, plus facile à résoudre (on translate le problème pour se ramener en $0$). 
\bigskip


\Subsection Ope, Opérations sur les limites. 

\Theoreme [$u:\ob R^n\to\ob R^p$ application linéaire] 
$$
\forall a\in\ob R^n, \qquad \lim_{x\to a}u(x)=u(a). 
$$

\Remarque : Soit $a=(a_1,a_2,\cdots,a_n)\in\ob R^n$. Alors, pour $1\le k\le n$, on a 
$$
\lim_{(x_1,\cdots,x_n)\to a}x_k=a_k. 
$$

\Propriete []  Soit $D$ un ouvert de $\ob R^n$ et $a\in D$. Si la fonction réelle de $n$ variables $f:D\to\ob R$ converge en $a$ vers $\ell\neq 0$, 
alors il existe un voisinage $V\subset D$ de $a$ pour lequel on a 
$$
\forall x\in V,\qquad f(x)\neq0.\qquad\qquad \mbox{($f$ ne s'annule pas au voisinage de $a$)}
$$
Si $f:D\to\ob R$ converge en $a$ vers $\ell> 0$, alors il existe un voisinage $V\subset D$ de $a$ tel que
$$
\forall x\in V,\qquad f(x)\ge{\ell\F 2}> 0. 
$$
($f$ est minorée par un nombre strictement positif au voisinage de $a$). 
\bigskip


\Theoreme [$a\in D$,  ouvert de $\ob R^n$,$\lambda\in\ob R$] Si $f:D\to\ob R^p$ et $g:D\to\ob R^p$ admettent une limite en $a$, 
alors les fonctions $\lambda.f$, $f+g$ et $f\times g$ convergent en $a$ et 
$$
\eqalign{
&\quad{\ds\lim_{x\to a}\b(\lambda.f(x)\b)=\lambda.\lim_{x\to a} f(x)},\cr
&{\ds\lim_{x\to a}\b(f(x)+g(x)\b)=\lim_{x\to a}f(x)+\lim_{x\to a}g(x)},\cr
&{\ds\lim_{x\to a}\b(f(x)\times g(x)\b)=\lim_{x\to a}f(x)\times\lim_{x\to a}g(x)}.
}
$$
De plus, {si $\ds\lim_{x\to a} g(x)\neq 0$}, la fonction $f/g$ est définie au voisinage de $a$ et on a 
$$
{\lim_{x\to a}\Q({f(x)\F g(x)}\W)={\ds\lim_{x\to a}f(x)\F\ds\lim_{x\to a}g(x)}}.
$$

\Theoreme  Une fonction polynôme de $n$ variables est continue sur $\ob R^n$. \pn 
Une fraction rationnelle de $n$ variables est continue sur son ensemble de définition. 

\Theoreme [$a\in D$ un ouvert de $\ob R^n$, $b\in D'$ un ouvert de $\ob R^p$]
Si $f:D\to\ob R^p$ converge vers $b$ en $a$, si $g:D'\to\ob R^p$ converge vers $\ell\in\ob R$ en $b$ et si 
$$
\forall x\in D, \qquad f(x)\in D',
$$
alors, l'application $g\circ f$, qui est définie sur $D$, converge en $a$ vers $\ell$. 

\Theoreme [$D\subset\ob R^n$] 
Pour $1\le k\le p$, la fonction $f_k:D\to\ob R$ converge en $a\in\ob R^n$ vers $\ell_k\in\ob R$ si, et~seulement~si l'application $f$ définie par 
$$
\forall x\in D, \qquad f(x):=\B(f_1(x),f_2(x),\cdots,f_p(x)\B)
$$
converge en $a$ vers $\ell=(l_1,l_2,\cdots,\ell_p)$. 

\Remarque : Cette propriéte permet en particulier de ramener l'étude de la convergence d'une fonction vectorielle à plusieurs études de convergence de fonctions réelles (pour l'ensemble d'arrivée, cela se passe bien). 
\bigskip

\Remarque : Par contre, on ne peut pas ramener l'étude de la convergence d'une fonction de plusieurs varaibles à plusieurs études de convergence de fonctions d'une variable (pour l'ensemble de départ, cela se passe mal).
\bigskip


\Subsection Cont, Continuité, prolongement par continuité en un point. 

\Definition []  Soit $D\subset\ob R^n$ et soit $a\in D$. Alors, l'application $f:D\to\ob R^p$ est continue en $a$ si, et~seulement~si
$$
\forall \epsilon>0,\qquad \exists \delta>0:\qquad \forall x\in D\mbox{ vérifiant }\|x-a\|\le\alpha, \mbox{ on a }\|f(x)-f(a)\|\le\epsilon.
$$
Autrement dit, on a 
$$
\mbox{$f$ est continue en $a$}\Longleftrightarrow \lim_{x\to a}f(x)=f(a). 
$$

\Definition []  Soit $D\subset\ob R^n$ et soit $a\notin D$ un point au contact de $D$. Alors, l'application $f:D\to\ob R^p$ 
est prolongeable par continuité en $a$ si, et~seulement~si $f$ converge en $a$. 
Dans ce cas, la fonction définie par 
$$
\tilde f(x):=\Q\{
\eqalign{
f(x)\mbox{ si }x\in D,\cr
\ds \lim_{x\to a}f(x)\mbox{ si }x=a
}
\W.
$$
est une application de $D':=D\cup\{a\}$ dans $\ob R^p$, continue en $a$, qui prolonge la fonction $f$. 
\bigskip

\Remarque : En général, s'il n'y a pas de contre indications, on confondra l'application $f$ avec son prolongement pas continuité $\tilde f$. 
\bigskip

Exercice :  Prolongement par continuité en $(0,0)$ pour l'application $f$ définie par 
$$
\forall (x,y)\neq(0,0), \qquad f(x,y)={xy^2\F x^2+y^2}. 
$$

\Theoreme [$D\subset\ob R^n$, $\lambda\in\ob R$]
Si $f:D\to\ob R$ et $g:D\to\ob R$ sont continues en $a\in\ob R$, 
alors les fonctions $\lambda.f$, $f+g$ et $f\times g$ sont continues en $a$. \pn 
De plus, si $g(a)\neq 0$, la fonction $f/g$ est continue en $a$. 

\Theoreme [$D\subset\ob R^n$ et $D'\subset\ob R^p$]
Si la fonction $f:D\to\ob R^p$ est continue en $a\in D$, 
si la fonction $g:D'\to\ob R^q$ est continue en $b=f(a)$ et si on a 
$$
\forall x\in D, \qquad f(x)\in D',
$$
Alors la fonction $g\circ f$, qui est définie sur $D$, est continue en $a$. 


\Theoreme [$D\subset\ob R^n$] 
Pour $1\le k\le p$, la fonction $f_k:D\to\ob R$ est continue en converge en $a\in D$ 
si, et~seulement~si l'application $f$ définie par 
$$
\forall x\in D, \qquad f(x):=\B(f_1(x),f_2(x),\cdots,f_p(x)\B)
$$
est continue en $a$. 

\Remarque : Cette propriéte permet en particulier de ramener l'étude de la convergence d'une fonction vectorielle à plusieurs études de convergence de fonctions réelles (pour l'ensemble d'arrivée, cela se passe bien). 
\bigskip

\Remarque : Par contre, on ne peut pas ramener l'étude de la convergence d'une fonction de plusieurs varaibles à plusieurs études de convergence de fonctions d'une variable (pour l'ensemble de départ, cela se passe mal).
\bigskip


\Propriete []  Soit $D\subset R^n$. Si $f:D\to\ob R^p$ converge en $a\in D$ vers une limite $\ell\in\ob R^p$, alors la fonction $\|f\|$ converge en $a$ vers $\|\ell\|$. En particulier, si l'application $f$ est continue en $a$ alors la fonction $\|f\|$ est continue en $a$. 
\bigskip


\Propriete []  Soit $D\subset\ob R^n$, soit $f:D\to\ob R^p$ une fonction convergeant en $a$ vers une limite finie et soit $g:D\to\ob R^p$ une fonction divergeant en $a$. Alors, $f+g$ diverge en $a$. 
\bigskip

\Remarque : si $f$ et $g$ sont deux fonctions n'admettant pas de limite en $a\in\overline{\ob R}$, la fonction $f+g$ peut converger ou diverger. 
\bigskip

\Propriete []  Soit $D$ un voisinage du point $a\in\ob R^n$. alors, 
l'ensemble des fonctions $f:D\to\ob R$ de limite nulle en $a$ forme un espace vectoriel pour l'addition et la multiplication externe des fonctions. 
De plus, si $f:D\to\ob R$ converge vers $0$ en $a$ et si $g:D\to\ob R$ est bornée au voisinage de $a$, alors, la fonction $f\times g$ converge vers $0$. 

 
\Propriete []  Soit $D\subset\ob R^n$ un voisinage du point $a$ et soient $f:D\to\ob R^p$ et $g:D\to\ob R$ deux fonctions telles que 
$$
\forall x\in D, \qquad \b|\!\b|f(x)\b|\!\b|\le g(x)\qquad \mbox{et}\qquad \lim_{x\to a}g(x)=0.
$$
Alors, la fonction $f$ converge en $a$ vers $0$ . 

\Propriete [Title=Principe des gendarmes]
Soient~$D$ un voisinage de $a\in\ob R^n$ et soient $f$, $g$ et $h$ trois fonctions de $D$ dans $\ob R$ telles que $f\le g\le h$. Si $f$ et $h$ convergent en $a$ vers le même nombre réel $\ell$, alors la suite $g$ converge vers en a vers $\ell$. 


\Propriete [Title=Conservation des inégalités larges par passage à la limite]
Soient~$D$ un voisinage de $a\in\ob R^n$ 
et $f$, $g$ deux fonctions de $D$ dans $\ob R$ avec $f\le g$. \pn 
Si $\ds\lim_{x\to a}f(x)=+\infty$, alors $\ds\lim_{x\to a}g(x)=+\infty$. 
\pn
Si $\ds\lim_{x\to a}g(x)=-\infty$, alors $\ds\lim_{x\to a}f(x)=-\infty$. 
 
\Theoreme [$D$ voisinage de $a\in\ob R^n$]
Une application $f:D\to\ob R^p$ est continue en $a$ si, et~seulement~si 
$$
\mbox{pour tout suite }u\in D^{\ob N} \mbox{ vérifiant }\lim_{n\to+\infty}u_n=a, \mbox{ on a }\lim_{n\to+\infty}f(u_n)=f(a). 
$$

\Remarque : Cette propriété est extrèmement utile pour deux raisons : \pn
a) Si une suite de terme général $u_n\in D$ tends vers $a$, si $f:D\to\ob K$ est continue en $a$ 
alors 
$$
\lim_{n\to+\infty}f(u_n)=f(a).
$$
b) Si une suite de terme général $u_n\in D$ converge vers $a$ et si la suite de terme général $v_n=f(u_n)$ diverge, alors, la fonction $f$ n'est pas continue en $a$. 
\bigskip


\Definition []  Soit $D$ un ensemble de $\ob R^n$. Alors, la fonction $f:D\subset\ob R^n\to\ob R^p$ est continue sur $D$ si, et seulement si elle est continue en chaque point $a$ de $D$. 
\bigskip


\Section Dif, Différentiabilité.
\bigskip

\Subsection Deiff, Différentielle $\d f$. 

\noindent
Soient $n$ et $p$ des entiers strictement positifs et $U$ un ouvert de $\ob R^n$. 
\bigskip

\Definition []  L'application $f:U\subset \ob R^n\to\ob R^p$ 
est différentiable en $a\in U$ si, et~seulement~si, il~existe 
une application linéaire $u\in\sc L(\ob R^n,\ob R^p)$ telle que 
$$
f(a+h)=f(a)+u(h)+o(\|h\|)\qquad\b(h\to(0,\cdots,0)\b), 
$$
c'est à dire telle que l'application $\epsilon$ définie par 
$$
f(a+h)=f(a)+u(h)+\|h\|\epsilon(h)\qquad(a+h\in U)
$$ 
vérifie $\ds\lim_{h\to (0,\cdots,0)}\epsilon(h)=0$. 
\bigskip

\Remarque. L'application linéaire $u$ est unique, lorsqu'elle existe. 
On la note $\d f_a$ et 
on l'appelle différentielle de $f$ en $a$. 
\bigskip

\Exemple. Pour $f:I\subset \ob R\to\ob R^p$ dérivable en $a\in I$, on a $\d f_a:h\mapsto h.f'(a)$ car 
$$
f(a+h)=f(a)+h.f'(a)+o_0(h)\qquad(h\to 0). 
$$
\smallskip

\Exemple. Pour $\eqalign{f: \sc M_n(\ob R)&\to\sc M_n(\ob R)\cr M&\mapsto  M^2}$ et $A\in\sc M_n(\ob R)$, 
on a $\d f_A:H\mapsto HA+AH$ car 
$$
f(A+H)=(A+H)\times(A+H)=\underbrace{A^2}_{f(A)}+\underbrace{HA+AH}_{u(H)}
+\underbrace{H^2}_{o(\|H\|)}\qquad\b(H\in\sc M_n(\ob R)\b)
$$

\Definition []  La différentielle $\d f$ d'une application $f:U\subset\ob R^n\to\ob R^p$ différentiable dur $U$ est l'application $\d f$ définie par 
$$
\eqalign{ \d f:U&\to{\sc L(\ob R^n,\ob R^p)}\cr  a&\mapsto  \d f_a}.
$$

\Subsection Appa, Applications partielles. 

\noindent 
Soient $n$ et $p$ des entiers strictements positifs et soit $U$ un ouvert de $\ob R^n$. 
\bigskip

\Concept [] Dérivée suivant un vecteur

\Definition []  Soit $\vec v$ un vecteur de $\ob R^n$, soit $f:U\subset \ob R^n\to\ob R^p$ une fonction de $n$ variables. Soit $a\in\ob R^n$ et soit $g$ l'application définie par 
$$
\forall t\in\ob R^n\mbox{ tel que}a+t\vec v\in U, \qquad g(t):=f(a+t\vec v).
$$
Lorsqu'elle existe, la dérivée $g'(0)$ de la fonction vectorielle $g$ est appelée dérivée de la fonction $f$ en $a$ selon le vecteur $\vec v$ et 
on la note $D_{\vec v}f(a)$. 
\bigskip 

\Concept [] Applications et dérivées partielles

\Definition []  Pour $1\le k\le n$, la $k^\ieme$ application partielle de la fonction $f:U\subset\ob R^n\to\ob R^p$ en~$(a_1,\cdots,a_n)\in U$ 
est l'application $$
f_k:t\mapsto f(a_1,\cdots,a_{k-1},a_k+t,a_{k+1},\cdots,a_n).
$$ 
Lorsqu'elle existe, sa dérivée $f_k'(0)$ en $0$ est appelée $k^\ieme$ dérivée partielle de la fonction $f:U\subset\ob R^n\to\ob R^p$ 
en $(a_1,\cdots,a_n)\in U$ et 
on la note $\ds {\partial f\F\partial x_i}(a_1,\cdots, a_n)$ ou $\partial_if(a_1,\cdots,a_n)$.  
\bigskip

\Remarque :  Les dérivées partielles sont des dérivées selon les vecteurs de la base canonique. 
\bigskip


\Remarque :  On appelle $k^\ieme$ dérivée partielle ou dérivée partielle par rapport à $x_k$ l'application 
$$
\eqalign{
	{\partial f\F\partial x_k}: U\subset\ob R^n&\to\ob R^p\cr
	(a_1,\cdots, a_n)&\mapsto {\partial f\F\partial x_k}(a_1,\cdots, a_n)
}.
$$ 


\Exemple.  Dérivées partielles du changement de variables sphériques. 
$$
\Psi:(r,\theta,\phi)\mapsto (r\cos\varphi\cos\theta,r\cos\varphi\sin\theta,r\sin\varphi).
$$ 

\Concept [] Matrice Jacobienne et Jacobien

\Definition []  Soit $f:x\mapsto\b(f_1(x),\cdots,f_p(x)\b)$ une application de $U\subset\ob R^n$ dans $\ob R^p$ 
dont toutes les dérivées partielles sont définies en $a\in U$. 
On appelle matrice jacobienne de $f$ en $a$ la matrice $J[f](a)\in\sc M_{p,n}(\ob R)$ 
définie par 
$$
J[f](a):=\Q({\partial f\F\partial x_1}(a),\ldots,{\partial f\F\partial x_n}(a)\W)
=\pmatrix{
{\partial f_1\F\partial x_1}(a)&\ldots&\ldots&{\partial f_1\F\partial x_n}(a)
\cr
{\partial f_2\F\partial x_1}(a)&\ldots&\ldots&{\partial f_2\F\partial x_n}(a)
\cr
\vdots&\ldots&{\partial f_i\F\partial x_j}(a)&\vdots
\cr
{\partial f_p\F\partial x_1}(a)&\ldots&\ldots&{\partial f_p\F\partial x_n}(a)
\cr
}
$$

\Definition []  Si $n=p$, on appelle Jacobien de $f$ au point $a$ et l'on note 
$$
\ds{\mbox{D}(f_1,\cdots,f_n)\F\mbox{D}(x_1,\cdots,x_n)}:=\det J[f](a)
$$ 
le déterminant 
de la matrice jacobienne de $f$ en $a$. 
\bigskip

\Concept [] Lien entre différentielle et dérivées partielles. 

\Remarque. Pour $1\le i\le n$, on note $\d x_i$ 
la forme linéaire $\d x_i:(x_1,\cdots,x_n)\mapsto x_i$ de~$\ob R^n$ et l'on pose 
$$
\d f_a:=\sum_{1\le i\le n}{\partial f\F\partial x_i}\d x_i
$$
L'application $\d f_a$ appartient alors à l'espace $\sc L(\ob R^n,\ob R^p)$ 
et on a $\sc Mat_{{\rm BC}}\d f_a=J[f](a)$. 
\bigskip


\Concept [] Equation aux dérivées partielles

\Propriete []  Soit $U\subset\ob R^n$ un ouvert connexe (consitué d'un seul morceau). alors, une fonction $f\in\sc C^1(U,\ob R)$ vérifie la relation 
$$ 
\forall x\in U,\qquad {\partial f\F\partial x_1}(x)=0. 
$$
si et seulement s'il existe un ouvert $V\subset\ob R^{n-1}$ et une fonction $g\in V \to\ob R$ de classe $\sc C^1$ telle que 
$$
\forall x=(x_1,\cdots,x_n)\in U, \qquad f(x_1,\cdots,x_n)=g(x_2,\cdots,x_n).
$$

\Remarque : plus simplement, si la dérivée partielle ${\partial f\F\partial x_k}$ d'une fonction $f$ selon la variable $k$, la fonction $f$ ne dépend pas de la variable $x_k$ et réciproquement . 
\bigskip


\Concept [] Gradient et lien avec la matrice Jacobienne

\Definition []  Si $g:U\subset\ob R^n\to\ob R$ admet toutes ses dérivées partielles en $a\in U$, 
on appelle gradient de $g$ en $a$ et on note $\vec{\grad}\ g(a)$ 
le vecteur $\Q(\vcenter{\mbox{${\partial g\F\partial x_1}(a)$}\mbox{$\vdots$}\mbox{${\partial g\F\partial x_n}(a)$}}\W)$. 
\bigskip

\Remarque. Soient $a\in U\subset\ob R^n$ et $f:x\mapsto\b(f_1(x),\cdots, f_p(x)\b)$ une application 
dont toutes les dérivées partielles sont définies en $a$. 
Alors $J[f](a)=\Q(\vcenter{\mbox{$\null^t\vec{\grad}\ f_1(a)$}\mbox{\qquad$\vdots$}\mbox{$\null^t\vec{\grad}\ f_p(a)$}}\W)$. 
\medskip

\Concept [] Gradient et différentielle

\Propriete []  Soit $f:U\subset \ob R^n\to\ob R$ une application différentiable en un point $a\in U$. Alors, on a 
$$
\forall \vec h\in\ob R^n, \qquad \d f_a(\vec h)=\vec{\grad}f(a).\vec h
$$

\Remarque : En particulier, le gradient en un point $a$ d'une fonction $f$ de plusieurs varaibles (par exemple $P(V,T)$) indique la direction à suivre pour que les valeurs de la fonction augmentent. Dans le sens contraire, la fonction décroit et perpendiculairement à cette direction, la fonction reste constante. 
\bigskip

\Subsection Ck, Applications de classe $\sc C^k$.

\Concept [] Caractérisation

\Definition []  Soit $k\ge1$. Une application $f:U\to\ob R^p$ est de classe $\sc C^k$ 
sur~l'ouvert $U\subset\ob R^n$ si, et seulement si, toutes ses dérivées partielles 
sont définies et de~classe $\sc C^{k-1}$ sur $U$. 
\bigskip


\Propriete []  La fonction $F:x\mapsto(f_1(x),\cdots,f_p(x))$ est de classe $\sc C^k$ sur un ouvert $U\subset\ob R^n$ si, et seulement si, 
l'application coordonnée $f_i$ est de classe $\sc C^k$ sur $U$ pour $1\le i\le p$. 
Etant donné $a\in U$, on a alors 
$$
	\d F_a(h)=\Q(\d(f_1)_a(h),\cdots,\d(f_p)_a(h)\W)\qquad(h\in\ob R^n)
$$ 

\Concept [] Opérations

\Propriete []  Pour $k\ge1$, l'ensemble des fonctions $f:U\to\ob R^p$ 
de classe $\sc C^k$ sur $U$ forme un espace vectoriel que l'on note $\sc C^k(U,\ob R^p)$. 
\bigskip

\Theoreme . Soient $f:U\subset\ob R^n\to\ob R^p$ et $g:V\subset\ob R^p\to\ob R^q$ 
deux fonctions de classe~$\sc C^1$ telles~que~$f(U)\subset V$. Alors, l'application $g\circ f:U\to\ob R^q$ 
est de classe $\sc C^1$ et on a 
$$
J[g\circ f](a)=J[g]\b(f(a)\b)\times J[f](a)\qquad(a\in U). 
$$ 
Autrement dit, pour $1\le i\le q$ et $1\le j\le n$, on a 
$$
{\partial (g\circ f)_i\F\partial x_j}(a)=\sum_{1\le k\le p}{\partial g_i\F\partial f_k}\b(f(a)\b){\partial f_k\F\partial x_j}(a)
=\sum_{1\le k\le p}\partial_k g_i\b(f(a)\b)\partial_j f_k(a)\qquad(a\in U)
$$

\Remarque. En particulier, pour $1\le j\le n$, on a l'égalité vectorielle 
$$
{\partial(g\circ f)\F\partial x_j}(a)
=\sum_{1\le k\le p}{\partial g\F\partial f_k}\b(f(a)\b){\partial f_k\F\partial x_j}(a)
=\sum_{1\le k\le p}\partial_k g\b(f(a)\b)\partial_j f_k(a)\qquad(a\in U). 
$$

\Exemple.  Soit $f\in\sc C^1(\ob R^2,\ob R)$. Calculer la dérivée 
de la fonction $g:x\mapsto f(1-x^2,\e^x)$. 
\bigskip

\Definition []  Soient $k\ge1$ et $U,V$ deux ouverts de $\ob R^n$. 
L'application $f:U\to V$ est un difféomorphisme de classe $\sc C^k$ si, 
et seulement si, $f$ est une bijection de $U$ sur $V$, 
de classe $\sc C^k$ sur $U$, 
dont la bijection réciproque $f^{-1}:V\to U$ est de classe $\sc C^k$ sur $V$. 
\bigskip

\Propriete []  Soit $f:U\subset\ob R^n\to V\subset\ob R^n$ 
un difféomorphisme de classe $\sc C^1$. Alors, $J[f](a)$ est inversible et 
$$
J[f^{-1}]\b(f(a)\b)=J[f](a)^{-1}\qquad(a\in U).
$$ 


\Concept [] Théorème de Schwarz

\Exemple.  Calcul de l'inverse de la matrice jacobienne des changements de variables : 
Polaire $(r,\theta)\mapsto(r\cos\theta,r\sin\theta)$ et Cylindrique $(r,\theta,z)\mapsto(r\cos\theta,r\sin\theta,z)$.  
\bigskip

\Theoreme [Théorème de  Schwarz]
Soit $f:U\subset\ob R^n\to\ob R^p$ une application de classe $\sc C^2$. 
Alors, pour~$(i,j)\in\{1,\cdots ,n\}$, on a 
$$
{\partial\F\partial x_i}\Q({\partial f\F\partial x_j}\W)(a)={\partial\F\partial x_j}\Q({\partial f\F\partial x_i}\W)(a)\qquad(a\in U). 
$$


\Concept [] Dérivées d'ordre supérieur

\Remarque. Si $k\ge2$, on peut calculer les dérivées partielles (d'ordre total inférieur à $k$) 
de~$f\in\sc C^k(U,\ob R^p)$ dans l'ordre que l'on veut. 
Pour $1\le i\le n$ et $0\le\ell\le k$, on pose 
$$
\partial_i^\ell f(a)={\partial^\ell f\F\partial x_i^\ell}(a)
:=\underbrace{{\partial\F\partial x_i}\cdots{\partial\F\partial x_i}}_{\ell\mbox{ fois}}f(a)\qquad(a\in U). 
$$
Etant donné $(\ell_1,\cdots,\ell_n)\in\ob N^n$ 
vérifiant $\ell_1+\cdots+\ell_n\le k$, on pose de même 
$$
\partial_1^{\ell_1}\cdots\partial_n^{\ell_n}f(a)={\partial^{\ell_1+\cdots+\ell_n}f\F\partial x_1^{\ell_1}\cdots\partial x_n^{\ell_n}}(a):=
{\partial^{\ell_1}\F\partial x_1^{\ell_1}}\cdots{\partial^{\ell_n}\F\partial x_n^{\ell_n}}f(a)\qquad(a\in U). 
$$
L'entier $\ell_i$ est l'odre partiel de dérivation par rapport à 
la $i^\ieme$ variable $x_i$ 
et l'entier~$L:=\ell_1+\cdots+\ell_n$ est l'ordre total de dérivation 
\bigskip

\Definition []  $f:U\subset\ob R^n\to\ob R^p$ est de classe $\sc C^\infty$ 
sur l'ouvert $U$ si, et seulement si, $f$ est de classe $\sc C^k$ sur $U$ 
pour chaque entier $k\in\ob N$. 
\bigskip

\Remarque. Une fonction $f:U\subset\ob R^n\to\ob R^p$ est de classe $\sc C^\infty$ 
(resp. $\sc C^k$) sur $U$ si, et seulement si, toutes ses dérivées partielles 
(resp. d'ordre total inférieur à $k$) existent et sont continues sur $U$ 
(on tient compte de l'ordre de dérivation dans cette remarque). 
\bigskip

\Exemple.  Toute fonction polynomiale $f:\ob R^n\to\ob R$ est de classe $\sc C^\infty$ 
sur $\ob R^n$. 
\bigskip


\Concept [] Formulaire pour les différentielles

\Definition []  Soit $U\subset\ob R^n$ un ouvert. 
On appelle différentielle de $f\in\sc C^1(U,\ob R^p)$ et on~note $\d f$ 
l'application $\eqalign{\d f: U&\to\sc L(\ob R^n,\ob R^p)\cr  a&\mapsto \d f_a}$.
\bigskip

\Exemple.  $\forall u\in\sc L(\ob R^n,\ob R^p)$, l'application $u$ 
est de classe $\sc C^\infty$ sur $\ob R^n$ et~vérifie~$\d u=u$. 
\bigskip 

\Propriete []  Soient $U$ un ouvert de $\ob R^n$, $k$ appartenant à l'ensemble $\ob N^*\cup\{\infty\}$ et deux fonctions $f\in\sc C^k(U,\ob R^p)$ et $g\in\sc C^q(U,\ob R^p)$. alors, 
\medskip
 
Si $p=q$ et $(\lambda,\mu)\in\ob R^2$, alors $\lambda f+\mu g\in\sc C^k(U,\ob R^p)$ 
et 
$$
\d(\lambda f+\mu g)=\lambda\d f+\mu\d g.
$$
Si $p=1$, alors $fg$ est de classe $\sc C^k$ sur $U$ et 
$$
\d(fg)_a=\d f_a g(a)+f(a)\d g_a\qquad(a\in U).
$$ 
Si $p=1$ et si $f(x)\neq 0$ pour $x\in U$, alors $g/f$ 
est de classe $\sc C^k$ sur $U$ et 
$$
\d\Q({g\F f}\W)_a={f(a)\d g_a-\d f_ag(a)\F f(a)^2}\qquad(a\in U).
$$ 

\Theoreme []  Soient $f:U\subset\ob R^n\to\ob R^p$ et $g:V\subset\ob R^p\to\ob R^q$ 
deux fonctions de classe~$\sc C^1$ telles~que~$f(U)\subset V$. Alors, l'application $g\circ f:U\to\ob R^q$ 
est de classe $\sc C^1$ et on a 
$$
\d(g\circ f)_a=\d g_{f(a)}\circ \d f_a.
$$ 

\Section Extrem, Extrema. 

\Definition []  Soient $A\subset\ob R^n$ et $a\in A$. 
L'application $f:A\subset\ob R^n\to\ob R$ admet un minimum (resp. un maximum) relatif en $a$ si, et seulement si, 
il existe $r>0$ tel que 
$$
\forall x\in B(a,r)\cap A, f(x)\ge f(a)\qquad\b(\mbox{resp. }\ f(x)\le f(a)\b).
$$
On appelle extremum relatif tout maximum ou minimum relatif. 
\bigskip

\Definition []  On dit que le point $a$ est un point critique d'une fonction $f:D\subset\ob R^n\to \ob R$, si et~seulement~si $f$ admet des dérivées partielles en $a$, 
qui sont nulles, i. e. 
$$
\forall k\in\{1,\cdots, n\}, \qquad {\partial f\F\partial x_k}(a)=0.
$$

\Remarque : Si $f$ est différentiable en un point critique $a$, on a forcément $\d f_a=0$. 
\bigskip


\Theoreme []  Soient $U$ un ouvert de $\ob R^n$ et $f\in\sc C^1(U,\ob R)$. Si $f$ admet un extremum relatif en $a\in U$, 
alors $a$ est un point critique de $f$.  
\bigskip

\Exemple. extrema de $(x,y)\mapsto \sqrt{x^2+y^2}-(x+y)/2$. 
\bigskip

\Remarque. C'est une condition nécéssaire mais pas suffisante, la réciproque est fausse. 
\bigskip

\Theoreme [Title=Formule de Taylor Young]
Soit $a\in\ob R^2$, soit $r>0$ et soit $f: B(a,r)\subset\ob R^2\to\ob R^p$ 
une application de classe $\sc C^2$. Alors, lorsque $h=(x,y)$ tends vers $(0,0)$, on a 
$$
f(a+h)=f(a)+\underbrace{{\partial f\F\partial x}(a)x+{\partial f\F\partial y}(a)y}_{\d f_a(x,y)}+
\underbrace{{\partial^2f\F\partial x^2}(a){x^2\F2}+{\partial^2f\F\partial x\partial y}(a)xy
+{\partial^2f\F\partial y^2}(a){y^2\F2}}_{q(x,y)}+o\b(\|h\|^2\b)
$$

\Remarque. Si $a\in U$ est un point critique de l'application $f\in\sc C^2(U,\ob R^p)$, on a 
$$
f(a+h)=f(a)+{rx^2+2sxy+ty^2\F2}+o\b(x^2+y^2\b)\qquad\b(h\to(0,0)\b). 
$$
où l'on a posé $h=(x,y)$, 
$$
\ds r={\partial^2f\F\partial x^2}(a),\qquad 
\ds s={\partial^2f\F\partial x\partial y}(a)\quad \mbox{ et }\quad 
\ds t={\partial^2f\F\partial y^2}(a). \leqno{(*)}
$$


\Theoreme []  Soient $U$ un ouvert de $\ob R^2$, soit $f\in\sc C^2(U,\ob R)$, 
soit $a\in U$ un point critique de $f$ et~soient $r$, $s$ et $t$ les nombres réels définis par ($*$). 
Alors, quatre cas se présentent : 
\medskip
\noindent 
1) Si $rt-s^2>0$ et $r<0$, la fonction $f$ admet un maximum relatif en $a$. \pn
2) Si $rt-s^2>0$ et $r>0$, la fonction $f$ admet un minimum relatif en $a$. \pn
3) Si $rt-s^2<0$, la fonction $f$ n'admet pas d'extremum relatif en $a$ {\it (selle de cheval).} \pn
4) Si $rt-s^2=0$, on ne peut conclure. {\it Il faudrait affiner l'estimation de $f$ en $a$.} 
\bigskip

\Exemple.  Extrema de $(x,y)\mapsto x^2+y^2-1$, $(x,y)\mapsto x^2-y^2$ et de $(x,y)\mapsto x^2-2xy+y^2+y^n$ ?

\Section Theo, Théorèmes d'inversion. 

\Theoreme []  Soient $U$ et $V$ deux ouverts de $\ob R^n$, $k\in\ob N^*\cup\{\infty\}$ et $f\in\sc C^k(U,V)$ une~bijection 
telle que 
$$
{\mbox{D}(f_1,\cdots,f_n)\F\mbox{D}(x_1,\cdots,x_n)}(a)\neq 0\qquad(a\in U),
$$ 
c'est-à-dire telle que la matrice jacobienne (resp. la différentielle $\d f_a$) de $f$ en $a$ 
soit~inversible pour $a\in U$. 
Alors $f:U\to V$ est un difféo\-mor\-phi\-sme de~classe~$\sc C^k$. 
\bigskip

\Exemple.  Les changements de variables polaires, cylindriques et sphériques 
sont des~difféo\-mor\-phi\-smes de classe $\sc C^\infty$. 
\bigskip

\Theoreme [] { \bf(des fonctions implicites).} Soient $U$ un ouvert de $\ob R^2$, 
$f\in\sc C^1(U,R)$ et~$(x_0,y_0)\in U$ tel que $\ds {\partial f\F\partial y}(x_0,y_0)\neq 0$. 
Alors, il existe des réels $a,b,c,d$ tels que 
$$
a<x_0<b,\qquad\quad c<y_0<d, \qquad\quad \Q]a,b\W[\times\Q]c,d\W[\subset U
$$ 
et il existe une unique application $\varphi\in\sc C^1\b(\Q]a,b\W[,\Q]c,d\W[\b)$ vérifiant 
$\varphi(x_0)=y_0$ et 
$$
\forall x\in\Q]a,b\W[, f\Q(x,\varphi(x)\W)=f(x_0,y_0).
$$ 
De plus, on a 
$$
\varphi'(x_0)=-{\ds{\partial f\F\partial x}(x_0,y_0)\F\ds {\partial f\F\partial y}(x_0,y_0)}. 
$$
et si $f$ est de classe $\sc C^k$ (resp. $\sc C^\infty$) sur $U$, 
alors $\varphi$ est de classe $\sc C^k$ (resp $\sc C^\infty$) sur $\Q]a,b\W[$. 
\bigskip

\Remarque : plus généralement, on peut calculer $\varphi'(x)$. 
\bigskip

\Exemple.  Paramétrisation des courbes de niveau de $f:(x,y)\mapsto x^2+y^2$. 





\hautspages{Olus Livius Bindus}{Intégrales multiples}

\pagetitretrue


\Chapter fonc, Intégrales multiples. 
\bigskip

Dans tout ce chapitre, la théorie des intégrales multiples est exposée en dimension $2$ (mais se généralise en dimension $3$ et même $n\ge2$). 
\bigskip

\Section Intmul, Définition. 

\Definition []  Soient $a< b$ et $c< d$ quatres nombres réels et soit $f:[a,b]\times[c,d]\to\ob R$ une fonction continue à valeurs réeles. Alors, étant donnée des 
subdivisions $\sigma$ et $\tau$ des segments $[a,b]$ et $[c,d]$, 
$$
\underbrace{a=a_0< a_1\cdots< a_p=b}_{\sigma}\qquad \mbox{et}\qquad \underbrace{c=c_0< c_1< \cdots< c_q=d}_{\tau},
$$
pour chuaque couple $(i,j)\in\{1,\cdots,p\}\times\{1,\cdots,q\}$, on se donne un point $M_{i,j}$ dans le rectangle $[a_{i-1},a_i]\times[c_{j-1},c_j]$ et on pose 
$$
S_{\sigma,\tau,M}:=\sum_{1\le i\le p}\sum_{1\le j\le q}f(M_{i,j})
$$
Lorsque l'on fait tendre le pas $\|\sigma\|$ et $\|\tau\|$ des subdivisions $\sigma$ et $\tau$ vers $0$, cette somme converge vers une limite, qui ne dépend ni du choix des subdivisions $\sigma$ et $\tau$, ni du choix des points $M_{i,j}$. Cette limite est appelée l'intégrale de la fonction $f$ sur le pavé $[a,b]\times[c,d]$ et on la note
$$
\int\limits_{[a,b]\times[c,d]}f=\int\limits_{[a,b]\times[c,d]}f(u)\d u=\int\int\limits_{\llap{$\ss[a,b]\times[c,d]$}}f(x,y)\d x\d y =\lim_{(\|\sigma\|,\|\tau\|)\to(0,0)}S_{\sigma,\tau,M}
$$

\Remarque{ \it 1} : Cette définition, qui utilise un quadrillage du pavé, est semblable à celle s'appuyant sur les sommes de Riemann, en dimension 1.
\bigskip

\Remarque{ \it 2} : De la même manière, on peut définir l'intégrale d'une fonction continue $f:A\to\ob R$ 
sur un ensemble $A$ constitué des points $(x,y)\in\ob R^2$ vérifiant 
des conditions simples du type 
$$
a\le x\le b\qquad \mbox{et}\qquad \varphi(x)\le y\le \psi(x)
$$
et plus généralement sur des domaines ``quarrables''. 
\bigskip

\Remarque{ \it 3} : L'intégrale d'une fonction $f:A\to\ob R$ est le volume (algèbrique) du ``cylindre'' délimité par le graphe de la fonction $f(x,y)$ pour $(x,y)\in A$
et par la surface palce $A\times\{0\}$. 
\bigskip

\Remarque : le fait de modifier les valeurs de la fonction sur un nombre fini de courbes (d'épaisseur $0$) ne modifie pas son intégrale. 
\bigskip


\Section Intmul, Propriétés. 

\Propriete [Title=Linéarité de l'intégrale]
Soit $A\subset\ob R^2$ un ensemble (quarrable) et soient $f:A to\ob R$ et $g:A\to\ob R$ deux fonctions continues. Alors, on a 
$$
\int_A\B(\lambda f(x)+\mu g(x)\B)\d x=\lambda\int_Af(x)\d x+\mu\int_Ag(x)\d x.
$$
 
\Remarque : l'égalité précédente se note également avec des intégrales doubles comme ceci : 
$$
\int\int_A\B(\lambda f(x,y)+\mu g(x,y)\B)\d x\d y=\lambda\int\int_Af(x,y)\d x\d y+\mu\int\int_Ag(x,y)\d x\d y.
$$
Cette notation étant un peu lourde, je ne l'utiliserai que lorsqu'elle sera réellement utile (pour Fubini par exemple) d'autant plus qu'elle n'est pas plus précise que la notation précédente (comme on intégre sur $A\subset\ob R^2$, on $\underline{\mbox{sait}}$ que l'on intégre selon $2$ variables)
\bigskip

\Propriete [Title=Additivité par rapport au domaine d'intégration] 
Soient $A$ et $B$ deux domaines quarrables de $\ob R^2$ et soit $f:A\cup B\to\ob R$ une fonction continue 
sur la réunion $A\cup B$. Si $A$ et $B$ sont disjoints (i.e. $A\cap B=\emptyset$), alors on a 
$$
\int\limits_{A\cup B}f(x)\d x=\int_Af(x)\d x+\int_Bf(x)\d x. 
$$

\Remarque : Cette propriété analoque à celle de la relation de Chasles pour les intégrales d'une seule variable, ne permet pas d'intégrer les fonctions de plusieurs variables sur des ensembles algèbriques : la relation $\int_a^bf=-\int_b^af$ (valable en dimension $1$) ne se généralise en dimension $n\ge2$. 
\bigskip

\Propriete[Title=positivité de l'intégrale] Soit $A$ un domaine quarrable de $\ob R^2$ et soit $f: A\to\ob R$ une fonction continue sur $A$. Alors, on a 
$$
\underbrace{
\forall x\in A,\quad f(x)\ge0}_{f\ge0}\qquad\Longrightarrow\qquad \int_Af(x)\d x\ge0.
$$ 


\Propriete[Title=Croissance de l'intégrale]Soit $A$ un domaine quarrable de $\ob R^2$ et soient $f: A\to\ob R$ et $g:A\to\ob R$ deux fonctions continues sur $A$. 
Alors, on a 
$$
\underbrace{
\forall x\in A,\quad f(x)\le g(x)}_{f\le g}\qquad\Longrightarrow\qquad \int_Af(x)\d x\le \int_Ag(x)\d x.
$$ 

\Propriete[Title=Valeur absolue]Soit $A$ un domaine quarrable de $\ob R^2$ et soit $f: A\to\ob R$ une fonction continue sur $A$. Alors, 
l'application $|f|$ est continue qur $A$ et on a 
$$
\Q|\int_Af(x)\d x\W|\le \int_A\b|f(x)\b|\d x.
$$

\Section Intmul, Théorèmes. 

\Subsection Fub, Théorème de Fubini. 

\Theoreme [Title=Théorème de Fubini]
Soient $a< b$ et $c<d$ des nombres réels et $f:[a,b]\times[c,d]\to\ob R$ une fonction continue. alors, on a 
$$
\int_a^b\Q(\int_c^df(x,y)\d y\W)\d x=\int\int\limits_{\llap{$\ss[a,b]\times[c,d]$}}f(x,y)\d x\d y=\int_c^d\Q(\int_a^bf(x,y)\d x\W)\d y.
$$

\Remarque{ \it 1} : Ce théorème est fondamental car il permet de décomposer le calcul (difficile) d'une intégrale en dimension $2$ en deux calcul d'intégrale d'une fonction d'une variable. 
\bigskip

\Remarque{ \it 2} : Lorsque la fonction à intégrer est du type $f(x,y)=g(x)h(y)$ (à variables séparables), son intégrale double 
se décompose en produit de deux intégrales simples. 
$$
\int_{[a,b]\times[c,d]}\underbrace{g(x)h(y)}_{f(x,y)}\d x\d y=\int_a^bg(x)\d x\times\int_c^dh(y)\d y
$$

\Exemple. $\ds\int\limits_{[0,R]\times[0,2\pi]}r\d r\d\theta=\int_0^R\Q(\int_0^{2\pi}r\d\theta\W)\d r=\pi R^2$. 
\bigskip


\Remarque{ \it 3} : Pour resoudre un problème (de difficulté moyenne/difficile) posé en concours sur les intégrales multiples, il suffit souvent d'appliquer le théorème de Fubini (généralisé). 
\bigskip


\Theoreme [Index=Theoreme@Théorème!de Fubini;Title=Théorème de Fubini {\it généralisé}]
Soit $A\subset \ob R^2$ et $f:A\to\ob R$ une fonction continue. S'il~existe 
des nombre $a< b$ et des fonctions continues $\varphi:[a,b]\to\ob R$ et $\phi:[a,b]\to\ob R$ vérifiant $\varphi\le \phi$ et 
$$
A=\B\{(x,y):a\le x\le b\mbox{ et }\varphi(x)\le y\le \phi(x)\B\}
$$
Alors, on a 
$$
\int\int_Af(x,y)\d x\d y=\int_a^b\Q(\int_{\varphi(x)}^{\phi(x)}f(x,y)\d y\W)\d x.
$$
S'il existe 
des nombres $c<d$ et des fonctions continues $\psi:[c,d]\to\ob R$ et $\Psi:[c,d]\to\ob R$ vérifiant $\psi\le\Psi$ et 
$$
A=\B\{(x,y):c\le y\le d\mbox{ et }\psi(y)\le x\le \Psi(y)\B\}
$$
Alors, on a 
$$
\int\int_Af(x,y)\d x\d y=\int_c^d\Q(\int_{\psi(y)}^{\Phi(y)}f(x,y)\d x\W)\d y.
$$
\bigskip

\Exemple.  Soient $D:=\{(x,y)\in\ob R^2:x^2+y^2\le 1\}$ et $f:D\to\ob R$ continue. 
Alors, $$
\int_Df(x,y)\d x\d y=\int_{-1}^1\Q(\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}f(x,y)\d y\W)\d x.
$$ 
\medskip

\Exemple.  Soient $T:=\{(x,y)\in\ob R^2:0\le x\le y\le 1\}$ et $f:T\to\ob R$ continue. Alors, 
$$
\int_Tf(x,y)\d x\d y=\int_0^1\Q(\int_x^1f(x,y)\d y\W)\d x=\int_0^1\Q(\int_0^yf(x,y)\d x\W)\d y.
$$ 
\bigskip


\Remarque{ \it 1} : En particulier, le théorème de Fubini permet d'inverser l'ordre d'intégration, car on a alors 
$$
\int_a^b\Q(\int_{\varphi_1(x)}^{\varphi_2(x)}f(x,y)\d y\W)\d x=\int\int_Af(x,y)\d x\d y=\int_c^d\Q(\int_{\psi(y)}^{\Phi(y)}f(x,y)\d x\W)\d y.
$$


\Remarque{ \it 2} : pour les intégrales multuples en dimension $n\ge3$, ce la marche pareil sauf qu'il y a plus de possibilités : 
\bigskip

\Exemple.  $\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\ds\int\limits_{[0,R]\times[0,2\pi]\times[-\pi/2,\pi/2]}\!\!\!\!\!\!\!\!\!\!\!r^2\cos\varphi\d r\d\theta\d\varphi=
\int_0^R\Q(\int_0^{2\pi}\Q(\int_{-\pi/2}^{\pi/2}r^2\cos\varphi\d\varphi\W)\d\theta\W)\d r={4\pi R^3\F3}$. 
\bigskip


\Subsection cdv, Théorème de changement de variables. 

\Theoreme [Title=Théorème de changement de variable] 
Soient $U,$ et $V$ des ouverts (quarrables) de $\ob R^n$, soit 
$f:V\to\ob R$ une fonction continue sur $V$ et son bord 
et soit $\phi:U\mapsto V$ un difféomorphisme de classe $\sc C^1$ de $U$ dans $V$. 
Alors, on a 
$$
\int_Vf(y_1,\cdots,y_n)\d y_1\cdots\d y_n
=\int_Uf\b(\underbrace{y_1,\cdots,y_n}_{\phi(x_1,\cdots,x_n)}\b)
\Q|{\mbox{D}(y_1,\cdots,y_n)\F\mbox{D}(x_1,\cdots,x_n)}\W|\d x_1\cdots\d x_n. 
$$

\Remarque : C'est un théorème fondamental, qui permet essentiellement de transformer 
une intégration sur un ensemble tordu en intégration sur un pavé. 
\bigskip

\Section AV, Aires et Volumes.

\Subsection A, Aire. 

\Concept Aire d'une surface plane 

\Definition [] Soit $S$ une surface (quarrable et bornée) de~$\ob R^2$. 
Alors, l'aire de $S$ est le nombre 
$$
\sc A(S):=\int_S\d x\d y. 
$$

\Exemple.  Pour $S:=\{(x,y)\in\ob R^2:x^2+y^2\le R^2\}$, on a $\sc A(S):=\pi R^2$. 
\bigskip 

\Concept Aire d'une surface

\Definition []Soit $U$ un ouvert (borné) de $\ob R^2$ et $\vec f:U\to\ob R^3$ 
une fonction (bornée) de classe $\sc C^1$. Alors, l'aire de la surface 
$S:=\{f(x,y):(x,y)\in U\}$ 
paramétrée par $f$ est l'intégrale 
$$
\sc A(S):=\int_U\bigg|\!\bigg|{\partial \vec f\F\partial x}(x,y)\wedge{\partial\vec f\F\partial y}(x,y)\bigg|\!\bigg|\d x\d y. 
$$
\smallskip

\Exemple.  Pour $S:=\{(x,y,z)\in\ob R^2:\sqrt{x^2+y^2+z^2}=R\}$, 
$\ds\sc A(S)=4\pi R^2$. 
\bigskip

\Subsection A, Volume.

\Definition []Soit $U$ un ensemble (quarrable et borné) de $\ob R^3$. Alors, le volume de $U$ est le nombre
$$
\sc V\mbox{ol}(U):=\int_U\d x\d y\d z. 
$$

\Exemple.  Pour $B:=\B\{(x,y,z)\in\ob R^3:\sqrt{x^2+y^2+z^2}<R\B\}$, on a $\sc V\mbox{ol}(B):=\ds{4\pi R^3\F3}$. 

\eject




\hautspages{Olus Livius Bindus}{Espaces vectoriels euclidiens et géométrie euclidienne}

\pagetitretrue


\Chapter fonc, Espaces vectoriels euclidiens et géométrie euclidienne. 
\bigskip

\Section la1, Espace pré-hilbertien (réel). 

\Subsection LA1, Produit scalaire (réel). 

\Rappel :  Soit $E$ un $\ob R$-espace vectoriel et soit $\Phi:E\times E\to\ob F$ une application. Alors, 
$$
\eqalign{
\Phi\mbox{ est une forme} \quad&\Leftrightarrow\quad F \mbox{ est le corps des scalaires}.
\cr
\Phi\mbox{ est symétrique} \quad&\Leftrightarrow\quad \forall (x,y)\in E^2,\quad \Phi(x,y)=\Phi(y,x).
\cr
\Phi\mbox{ est positive} \quad&\Leftrightarrow\quad\forall x\in E, \quad\Phi(x,x)\ge0. 
\cr
\Phi\mbox{ est définie} \quad&\Leftrightarrow\quad\forall x\in E, \quad\Phi(x,x)=0\Rightarrow x=0
\cr
\Phi\mbox{ est bilinéaire} \quad&\Leftrightarrow\quad \forall y\in E, \quad x\mapsto\Phi(x,y) \mbox{ et }x\mapsto\Phi(y,x) 
\mbox{ sont linéaires sur }E. }
$$

\Definition []  On appelle produit scalaire réel d'un $\ob R$-espace vectoriel $E$ toute forme bilinéaire, symétrique, définie, positive de $E$. Autrement dit, toute application 
$$
f:E\times E\to\ob R
$$ 
possédant les qualités précitées.
\bigskip


\Remarque : pour $(x,y)\in\ob E^2$, le produit scalaire $f(x,y)$ est usuellement noté 
$$
\langle x,y\rangle\quad\mbox{ ou }\quad\langle x|y\rangle\quad\mbox{ ou }\quad(x|y)\quad\mbox{ ou }\quad\vec x.\vec y.
$$

\Exemples.  L'espace vectoriel $\ob R^n$ est naturellement muni du produit scalaire défini 
par 
$$
\Q\{\eqalign{
&\forall x=(x_1,\cdots,x_n)\in \ob R^n\cr
&\forall y=(y_1,\cdots,y_n)\in\ob R^n
}\W.\qquad\langle x,y\rangle:=\sum_{k=1}^n x_ky_k
$$ 
De même, l'espace vectoriel $\sc M_{n,1}(\ob R)$ est naturellement muni du produit scalaire 
$$
\forall (X,Y)\in\sc M_{n,1}(\ob R)^2, \qquad \langle X,Y\rangle:=^tXY.
$$
L'espace $\sc C\b([a,b],\ob R\b)$ est naturellement muni du produit scalaire défini 
par 
$$
\forall(f,g)\in\sc C\b([a,b],\ob R\b)^2,\qquad
 \langle f,g\rangle:=\int_a^bf(t)g(t)\d t.
$$ 

\Subsection La2, Espace pré-hilbertien (réel). 

\Definition []  On appelle espace pré-Hilbertien (réel) tout $\ob R$-espace vectoriel muni d'un produit scalaire (réel). 
\bigskip

\Rappel :  Soit $E$ un $\ob R$-espace vectoriel. Une application $N:E\to\ob R$ est une norme de $E$ si, et seulement si, on a 
$$
\eqalignno{
\forall x\in E,&\qquad N(x)\ge0
\cr
\forall x\in E,&\qquad N(x)=0\Longleftrightarrow x=0
\cr
\forall (\lambda,x)\in\ob R\times E,& \qquad N(\lambda x)=|\lambda|N(x)
\cr
\forall (x,y)\in E\times E,&\qquad N(x+y)\le N(x)+N(y) 
}
$$

\Concept [] Norme euclidienne


\Propriete []  Pour chaque espace vectoriel $E$ muni d'un produit scalaire réel $\langle\cdot,\cdot\rangle$, l'application~$x\mapsto \|x\|$ définie~par 
$$
\forall x\in E, \qquad \|x\|=\sqrt{\langle x,x\rangle},
$$ 
est une norme de $E$, appelé norme euclidienne (associée au produit scalaire 
$\langle\cdot,\cdot\rangle$). 
\bigskip

\Remarque : En particulier la norme euclidienne satisfait les $2$ inégalités triangulaires :
$$
\forall(x,y)\in\ob E^2, \qquad \B|\|x\|-\|y\|\B|\le \|x+y\|\le \|x\|+\|y\|.
$$

\Concept [] Identités de polarisation


\Propriete []  Réciproquement, on peut exprimer le produit scalaire (forme polaire) en fonction de la norme. 
Ainsi, pour $(x,y)\in E^2$, on a 
$$
\eqalignno{
\qquad\qquad\qquad\qquad\qquad\qquad\langle x,y\rangle&={\|x+y\|^2-\|x\|^2-\|y\|^2\F 2}&\mbox{(Identité de polarisation \it symétrique)}
\cr
\langle x,y\rangle&={\|x+y\|^2-\|x-y\|^2\F 4}&\mbox{(Identité de polarisation \it asymétrque)}
}
$$

\Concept [] Identité du parallélogramme. 


\Propriete []  La somme des carrés des longueurs des diagonales d'un parallélogramme est égale à la somme des carrés des longueurs de ses cotés. 
$$
\forall (x,y)\in E^2, \qquad \|x+y\|^2+\|x-y\|^2=2\|x\|^2+2\|y\|^2.
$$

\Concept [] Inégalité de Cauchy-Schwarz

\Theoreme [Title=Inégalité de Cauchy-Schwarz]
Soit $E$ un $\ob R$-espace vectoriel 
muni d'un produit scalaire $\langle\cdot,\cdot\rangle$. Alors, on a 
$$
\forall (x,y)\in E^2, \qquad \b|\langle x,y\rangle\b|\le \|x\|\times\|y\|. 
$$
{\bf Cas d'égalité. }De plus, on a 
$$
\b|\langle x,y\rangle\b|=\|x\|\times\|y\|\quad\Longleftrightarrow \quad \mbox{la famille de vecteurs }\{x,y\} \mbox{ est liée}.
$$ 

\Remarque. Ce théorème permet démontrer des inégalités difficiles. 
Par~exemple, on a 
$$
\forall n\in\ob N^*, \qquad \Q|\sum_{k=1}^n{\e^{i n}\F n}\W|\le{\pi\F\sqrt6}\sqrt n. 
$$
On a également 
$$
\forall x\ge0, \qquad 0\le \int_0^x{\e^{-t}\F \sqrt{1+t^2}}\d t\le {\sqrt{\pi}\F 2}.
$$

\Concept [] Distance associée. 

\Definition []  Soit $E$ un espace pré-hilbertien. Alors, la distance (euclidienne) associé à son produit scalaire $\langle\cdot,\cdot\rangle$ est l'aplication $\d:E\times E\to\ob R^+$ définie par 
$$
\forall (x,y)\in E^2, \qquad \d(x,y):=\|x-y\|.
$$ 

\Concept [] Espaces vectoriels euclidiens

\Definition []  Une espace vectoriel euclidien est un $\ob R$-espace vectoriel de dimension finie muni d'un produit scalaire (réel). 
\bigskip

\Remarque : Un espace euclidien est donc un espace préhilbertien réel, de dimension finie. 
\bigskip

\Section ahahaha, Géométrie euclidienne.

\Subsection ohoh, Orthogonalité. 

\noindent
Dans cette section, $E$ désigne un $\ob R$-espace vectoriel muni 
d'un produit scalaire $\langle\cdot,\cdot\rangle$. 
\bigskip

\Definition []  Deux vecteurs $x$ et $y$ de $E$ sont orthogonaux (pour le produit scalaire de $E$) 
si, et seulement~si, leut produit scalaire est nul. 
$$
x\perp y\quad\Longleftrightarrow\quad \langle x,y\rangle=0.
$$

\Theoreme [Title=Théorème de Pythagore]
Deux vecteurs $x$ et $y$ de $E$ sont orthogonaux si, 
et~seulement~si
$$
\|x+y\|^2=\|x\|^2+\|y\|^2. 
$$


\Concept [] Familles orthogonales


\Definition []  Une famille $\{x_i\}_{i\in I}$ de vecteurs de $E$ est orthogonale si, et~seulement~si, 
ses vecteurs sont non nuls et orthogonaux $2$ à $2$. C'est-à-dire si, et seulement si,
$$
\forall (i,i')\in I^2, \qquad \Q\{\eqalign{\langle x_i,x_{i'}\rangle=
0&\mbox{ si }i\neq i'\cr
\underbrace{\langle x_i,x_{i'}\rangle}_{\|x_i\|^2}\neq 
0&\mbox{ si }i=i'
}\W.
$$

\Propriete []  Une famille orthogonale est libre. 
\bigskip

\Concept [] Vecteurs unitaires

\Definition []  Un vecteur $x$ de $E$ est unitaire si, et seulement s'il est de norme $1$. 
$$
x\mbox{ est un vecteur unitaire de }E\quad\Longleftrightarrow\quad\|x\|=1.
$$

\Concept [] Familles orthogonormales


\Definition []  Une famille $\{x_i\}_{i\in I}$ de vecteurs de $E$ est 
dite orthonormale si, et seulement si, c'est une famille orthogonale de vecteurs unitaires, c'est à dire si, et seulement si 
$$
\forall (i,i')\in I^2,\qquad \langle x_i,x_{i'}\rangle=\Q\{\eqalign{
0&\mbox{ si }i\neq i'\cr
1&\mbox{ si }i=i'
}\W.
$$

\Concept [] Expression du produit scalaire dans une base orthonormale/orthonormale

\Propriete []  Soit $\sc E:=\{e_1,\cdots,e_n\}$ une famille d'un espace pré-hilbertien $E$ et soient deux vecteurs $x=\sum_{k=1}^n\lambda_ke_k$ et $y=\sum_{k=1}^n\mu_ke_k$. 
Si la famille $\sc E$ est orthogonale, on a 
$$
\langle x,y\rangle=\sum_{k=1}^n\lambda_k\mu_k\|e_k\|^2. 
$$
Si la famille $\sc E$ est orthonormale, on a 
$$
\langle x,y\rangle=\sum_{k=1}^n\lambda_k\mu_k. \leqno{(*)}
$$

\Remarque. La donnée d'une famille orthonormale permet de ramener le 
produit scalaire au cas simple $(*)$. 
\bigskip

\Concept [] Orthogonalisation/orthonormalisation d'une famille libre. 


\Theoreme [Index=Procede de Gram-Schmidt@Procédé de Gram-Schmidt;Title=Procédé d'orthonormalisation de Gram-Schmidt]
Soit $E$ un $\ob R$-espace vectoriel 
muni d'un produit scalaire $\langle\cdot,\cdot\rangle$, soit $n\in\ob N^*$ et soit $\{x_k\}_{1\le k\le n}$ une famille libre~de~$E$. Alors, il existe 
une unique famille orthonormale $\{z_k\}_{1\le k\le n}$ de $E$ telle que 
$$
\forall k\in\{1,\cdots,n\}\quad\Q\{\eqalign{&\mbox{Vect}\b( z_1,\cdots,z_k\b)
=\mbox{Vect}\b(x_1,\cdots,x_n\b),\cr
&\langle z_k,x_k\rangle>0.}\W.
$$
La famille $\{z_1,\cdots,z_n\}$ est définie par récurence par 
$$
z_1={x_1\F\|x_1\|}\qquad\mbox{et}\qquad\forall k\in\{2,\cdots, n\}, \quad z_k=
{x_k-\sum_{\ell=1}^{k-1}\langle x_k,z_\ell\rangle z_\ell
\F \b|\!\b|x_k-\sum_{\ell=1}^{k-1}\langle x_k,z_\ell\rangle z_\ell \b|\!\b| }
$$

\Remarque{ \it 1} : Normaliser un vecteur non nul $x$, consiste à le diviser par sa norme $\|x\|$ pour le rendre unitaire. 
\bigskip

\Remarque{ \it 2}: Si $\{x_1,\cdots,x_n\}$ est une famille orthogonale et si $y$ est linéarement indépendant de $\{x_1,\cdots,x_n\}$, on peut fabriquer un vecteur $x_{n+1}$ de la forme 
$$
x_{n+1}=y+\sum_{k=1}^n\lambda_kx_k
$$ 
tel que la famille $\{x_1,\cdots,x_{n+1}\}$ soit orthogonale. Pour cela, il suffit de choisir $(\lambda_1,\cdots,\lambda_n)$ de telle sorte que, pour l'on ait
$$
\forall k\in\{1,\cdots,n\}, \qquad 0=\langle x_{n+1},x_k\rangle=\langle y,x_k\rangle+\lambda_k\underbrace{\|x_k\|^2}_{>0}.
$$

\Remarque{ \it 3} : Pour fabriquer une famille orthonormale $\{z_1,\cdots,z_n\}$ à partir d'une famille libre $\{x_1,\cdots,x_n\}$, 
on peut s'y prendre de deux fa\c cons différentes (pour le même résultat) :
\medskip
\noindent
\Methode[Procédé d'orthonormalisation de Gram-Schmidt]
On orthogonalise et on normalise en même temps : on~normalise~$x_1$ pour obtenir $z_1$, puis on fabrique un vecteur $y_2$ orthogonal à $z_1$ qu'on normalise pour obtenir $z_2$, puis on fabrique un vecteur $y_3$ orthogonal à $z_1$ et $z_2$ qu'on normalise pour obtenir $z_3$, etc...jusqu'à obtenir $\{z_1,\cdots,z_n\}$. 

\medskip
\Methode[Procédé d'orthonormalisation recommandé]
On orthogonalise la famille $\{x_1,\cdots,x_n\}$ pour obtenir une famille orthogonale $\{y_1,\cdots,y_3\}$, qu'on normalise ensuite pour obtenir $\{z_1,\cdots,z_n\}$. 

\bigskip
\Exercice. Dans l'espace $\sc C\b([-1,1],\ob R\b)$ muni du 
produit scalaire $\langle f,g\rangle=\int_{-1}^1f(t)g(t)\d t$, orthonormaliser 
la famille constituée des fonctions polynômes 
$$
p_0:t\mapsto1, \qquad p_1:t\mapsto t, \qquad p_2:t\mapsto t^2, \qquad p_3:t\mapsto t^3\quad\mbox{et}\quad p_4:t\mapsto t^4.
$$ 

\Propriete []  Chaque espace vectroriel euclidien admet au moins une base orthonormale (une base constituée par une famille orthonormale). 
\bigskip


\Concept [] Ensembles orthogonaux


\Definition []  Soit $E$ un espace pré-hilbertien. Un vecteur $x$ de $E$ est orthogonal à un sous-ensemble $G$ de $E$ si, 
et seulement si, $x$ est orthogonal à tous les vecteurs de $G$, 
$$
x\perp F \ssi \forall y\in G, \quad \underbrace{\langle x,y\rangle=0}_{x\perp y}. 
$$

\Definition []  Soit $E$ un espace pré-hilbertien. Un sous ensemble $F$ de $E$ est orthogonal à un sous-ensemble $G$ de $E$ si, 
et seulement si, chaque $x$ de $F$ est orthogonal à tous les vecteurs $y$ de $G$, 
$$
F\perp G \ssi \forall x\in F,\quad \underbrace{\forall y\in G, \quad \overbrace{\langle x,y\rangle=0}^{x\perp y}}_{x\perp G}. 
$$

\Remarque. Si $E$ est un espace pré-hilbertien et si $F$ et $G$ sont deux sous-espaces vectoriels de $E$, on note 
$$
F\mathop{+}\limits^\perp G\qquad (\mbox{resp. }F\mathop{\oplus}\limits^\perp G)
$$
si et seulement si l'on a $F\perp G$ (resp. si, et seulement si, l'on a $F\oplus G$ et $F\perp G$). 
\bigskip


\Concept [] Orthogonal d'un ensemble

\Definition []  Soit $E$ un espace préhilbertien. Alors, l'orthogonal d'un sous ensemble non vide $F\subset E$ est l'ensemble 
$$
F^\perp:=\{x\in E:\underbrace{\forall y\in F, \overbrace{\langle x,y\rangle=0}^{x\perp y}}_{x\perp F}\}.
$$

\Propriete []  Soit $E$ un espace préhilbertien. Alors, l'orthogonal $F^\perp$ d'un sous ensemble $F$ non vide de $E$ est un sous-espace vectoriel de $E$. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien. Alors, pour chaque sous espace vectoriel $F$ de $E$, on a 
$$
E=F\mathop{\oplus}\limits^\perp F^\perp.
$$
C'est pourquoi, l'orthogonal de $F$ est appelé le suplémentaire orthogonal de $F$ (il est unique). 
\bigskip


\Propriete []  Soit $E$ un espace pré-hilbertien et soit $F$ un sous ensemble non vide de $E$. 
Alors, on a 
$$
F\subset (F^\perp)^\perp
$$
De plus, si $E$ est un espace euclidien (si $E$ est de dimension finie), on a 
$$
\Vect(F)=(F^\perp)^\perp
$$

\Remarque : en particulier, dans un espace euclidien, on a 
$$
\mbox{l'ensemble non vide }F\mbox{ est un sous-espace vectoriel de $E$}\ssi F=(F^\perp)^\perp.
$$
\Subsection ohoh, Applications linéaires fondamentales.

\Concept [] Formes linéaires d'un espace euclidien

\Propriete []  soit $E$ un espace euclidien. Alors, pour toute forme linéaire $f:E\to\ob R$ de $E$, il existe un unique vecteur $a\in E$ tel que 
$$
\forall x\in E, \qquad u(x)=\langle x,a\rangle.
$$

\Concept [] Projection orthogonale

\Rappel :  Un projecteur de $E$ est un endomorphisme $p:E\to E$ vérifiant $p^2=p$. Un tel endomorphisme vérifie
$$
\eqalign{
E=\IM&(p)\oplus\Ker(p)
\cr
x\in\IM(p)&\ssi p(x)=x
\cr
x\in \Ker(p)&\ssi p(x)=0
}
$$
et est appelé projecteur sur $\IM(p)=\Ker(\Id-p)$ parallélement à $\Ker(p)$. \pn
\bigskip

\Definition []  Soit $E$ un espace pré-hilbertien. Un projecteur de $E$ est orthogonal si, et seulement s'il satisfait 
$$
\IM(p)\perp\Ker(p).
$$

\Remarque : En particulier, un projecteur orthogonal $p$ d'un espace préhilbertien $E$ satisfait 
$$
E=\IM(p)\mathop{\oplus}\limits^\perp\Ker(p).
$$

\Propriete []  Soient $E$ un espace pré-hilbertien et $F\subset E$ un sous-espace de dimension~finie.
Alors, pour chaque $x\in E$, il~existe un unique vecteur $p(x)\in F$ 
tel que 
$$
x-p(x)\perp F.
$$ 
L'application $p:x\mapsto p(x)$ est un projecteur de $E$ (appelé projection orthogonale sur $F$) vérifiant 
$$
F=\IM(p)\qquad\mbox{ et }\qquad \IM(p)\perp\Ker(p).
$$ 
\pn 
Enfin, pour chaque base orthonormale $\{e_1,\cdots e_n\}$ de $F$, on a 
$$
\forall x\in E, \qquad p(x)=\sum_{k=1}^n\langle x,e_k\rangle e_k.
$$

\Propriete []  Soit $E$ un espace préhilbertien. Alors, un projecteur $p$ de $E$ est orthogonal si, et~seulement~si,
$$
\forall x\in E, \qquad \|p(x)\|\le \|x\|. 
$$

\Definition []  Soient $E$ un espace pré-hilbertien, $F$ un sous espace vectoriel de $E$ et $x\in E$. 
On appelle distance du vecteur $x$ à l'espace $F$ et on note $\d(x,F)$ le nombre positif 
$$
\d(x,F):=\inf_{y\in F}\|y-x\|.
$$

\Propriete []  Soit $E$ un espace pré-hilbertien de norme associée $\|\cdot\|$, 
soit $F$ un sous espace vectoriel de dimension finie de $E$ et 
soit $p$ la projection orthogonale sur $F$. Alors, on a 
$$
\forall x\in E, \qquad \d(x,F)=\|p(x)-x\|. 
$$ 

\Remarque : en particulier, les projections orthogonales constituent l'outil adapté au calcul de la distance d'un vecteur à un espace vectoriel. 
\bigskip


\Exercice. Pour $E=\sc C\b([-1,1]\b)$ muni du produit scalaire $\langle f,g\rangle=\int_{-1}^1 f(t)g(t)\d t$, 
calculer la distance de l'application $t\mapsto \sh t$ à l'espace des fonctions polynômes de degré inférieur ou égale à $2$. 
\bigskip

\Concept [] Symétries orthogonales


\Rappel :  Une symétrie de $E$ est un endomorphisme $s:E\to E$ vérifiant $s^2=\Id_E$. Un tel endomorphisme vérifie
$$
\eqalign{
E=\Ker(s-\Id_E)\oplus\Ker(s+\Id_E)
\cr
x\in\Ker(s-\Id_E)&\ssi s(x)=x
\cr
x\in \Ker(s+\Id_E)&\ssi s(x)=-x
}
$$
et est appelé symétrie par rapport à l'espace $\Ker(s-\Id_E)$ parallélement à l'espace $\Ker(s+\Id_E)$. 
\bigskip

\Definition []  Soit $E$ un espace pré-hilbertien. Une symétrie de $E$ est orthogonale si, et seulement si elle satisfait 
$$
\Ker(s-\Id_E)\perp\Ker(s+\Id_E).
$$

\Remarque : En particulier, une symétrie orthogonale $s$ d'un espace préhilbertien $E$ satisfait 
$$
E=\Ker(s-\Id_E)\mathop{\oplus}\limits^\perp\Ker(s+\Id_E).
$$


\Propriete []  Soient $E$ un espace pré-hilbertien et $F\subset E$ un sous-espace de dimension~finie.
Alors, notant $p$ l'unique projection orthogonale sur $F$, l'application 
$$
s:x\mapsto 2p(x)-x
$$ 
est une symétrie de $E$ (appelé symétrie orthogonale par rapport à $F$) vérifiant 
$$
F=\Ker(s-\Id_E)\qquad\mbox{ et }\qquad \Ker(s-\Id_E)\perp\Ker(s+\Id_E).
$$ 
Enfin, pour chaque base orthonormale $\{e_1,\cdots e_n\}$ de $F$, on a 
$$
\forall x\in E, \qquad s(x)=2\sum_{k=1}^n\langle x,e_k\rangle e_k-x.
$$

\Remarque{ \it 1} : Si $p$ est la projection orthogonale sur $F$ et si $s$ est la symétrie orthogonale par rapport à~$F$, on retiendra que 
$$
s=2p-\Id_E.
$$
En particulier, si l'on dispose d'un renseignement sur $p$ (resp. $s$), on peut en déduire quelque chose sur $s$ (resp. $p$). 
\bigskip

\Remarque{ \it 2} : Si $s$ est la symétrie orthogonale par rapport à $F$, alors $-s$ est la symétrie orthogonale par rapport à $F^\perp$. 
\bigskip

\Exercice. Trouver une formule pour la symétrie orthogonale $s(x,y,z)$ et la projection orthogonale $p(x,y,z)$ (vectorielles) de $\ob R^3$ sur le plan d'équation $x+y+z=0$. 
\bigskip 

\Propriete []  Soit $E$ un espace préhilbertien. Alors, une symétrie $s$ de $E$ est orthogonale si, et~seulement~si,
$$
\forall x\in E, \qquad \|s(x)\|=\|x\|. 
$$

\Exercice. Pour $E=\sc C\b([-1,1]\b)$ muni du produit scalaire $$
\langle f,g\rangle=\int_{-1}^1f(t)g(t)\d t,
$$ 
calculer la distance de l'application $t\mapsto \sh t$ à l'espace des fonctions polynômes de degré inférieur ou égale à $2$. 
\bigskip



\eject


\hautspages{Olus Livius Bindus}{Automorphismes orthogonaux}

\pagetitretrue

\Chapter fonc, Automorphismes orthogonaux. 
\bigskip


Dans tout ce chapitre, $E$ désigne un espace euclidien, c'est-à-dire un espace vectoriel de dimension finie $n\ge1$, muni d'un produit scalaire $\langle\cdot,\cdot\rangle$. 
\bigskip

\Section ah, Généralités. 

\Subsection a, Le groupe $(\sc O(E),\circ)$. 


\Definition []  Un endomorphisme $u$ d'un espace euclidien $E$ est orthogonal si, et seulement si il conserve le produit scalaire, i. e. 
$$
\forall (x,y)\in E^2, \qquad \langle u(x),u(y)\rangle=\langle x,y\rangle. 
$$
L'ensemble des endomorphisme orthogonaux de $E$ est noté $\sc O(E)$. 
\bigskip

\Propriete []  Un endomorphisme $u$ d'un espace euclidien $E$ est orthogonal si, et~seulement~s'il conserve la norme, i. e. 
$$
\forall x\in E, \qquad \|u(x)\|=\|x\|. 
$$

\Propriete []  Chaque endomorphisme orthogonal $u$ de $E$ est bijectif. En particulier, $u$ est un automorphisme de $E$. 
\bigskip


\Propriete []  La composé $v\circ u$ de deux auomorphismes orthogonaux $u$ et $v$ est un automorphisme orthogonal. La bijection réciproque $u^{-1}$ d'un automorphisme orthogonal $u$ est un automorphisme orthogonal. 
$$
\eqalign{
u\in\sc O(E)\mbox{ et }v\in\sc O(E)&\quad\Longrightarrow v\circ u\in\sc O(E). 
\cr
u\in\sc O(E)&\quad\Longrightarrow\quad u^{-1}\in\sc O(E).
}
$$
En particulier, l'ensemble $(\sc O(E),\circ)$ forme un sous groupe de $(\sc Gl(E)\circ)$, appelé groupe orthogonal de $E$. 
\bigskip


 \Remarque : Le mot orthogonal prette à confusion. A part l'identité, les projections orthogonales (notion définie dans le chapitre précédent) ne sont pas des automorphismes orthogonaux (notion de ce chapitre). Par contre, les symétries orthogonales le sont. 
\bigskip

\Propriete []  Un endomorphisme orthogonal $u$ d'un espace euclidien $E$ transforme une base orthonormale $\sc B=\{e_1,\cdots,e_n\}$ de $E$ en une base orthonormale $\{u(e_1),\cdots,u(e_n)\}$ de $E$. 
$$
u\in\sc O(E)\mbox{ et }\sc B\mbox{ base orthonormée de }E\quad \Longrightarrow\quad u(\sc B)\mbox{ base orthonormée de }E.
$$
 

\Propriete []  Soit $u$ un endomorphisme d'un espace euclidien $E$. S'il existe une base orthonormée $\sc B=\{e_1,\cdots,e_n\}$ de $E$ telle que 
$u(\sc B)=\{u(e_1),\cdots,u(e_n)\}$ soit orthonormée, alors $u$ est un automorphisme orthogonal de $E$. 
$$
u\in\sc L(E), \quad \sc B\mbox{ et }u(\sc B)\mbox{ bases orthonormées de }E\quad \Longrightarrow\quad u\in\sc O(E).
$$

 \Remarque :  Le mot orthogonal est encore utilisé de manière inconsistante ici : un automorphisme {\bf orthogonal} est un endomorphisme qui transforme une (resp. toute) base {\bf orthonormale} en base {\bf orthonormale}. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien muni d'une base orthonomrée $\sc B$. Alors, on a 
$$
\forall u\in\sc O(E), \qquad \det_{\sc B}(u)\in\{-1,1\}. 
$$

\Subsection c, Le groupe $(\sc{SO}(E),\circ)$. 

\Definition []  Soit $E$ un espace euclidien orienté, muni d'une base orthonormée directe $\sc B$. 
Alors, l'ensemble des automorphismes orthogonaux de déterminant $1$ est noté 
$$
\sc{SO}(E):=\{u\in\sc O(E):\det_{\sc B}(u)=1\}
$$

\Propriete []  L'ensemble $\sc{SO}(E)$ muni de la loi $\circ$ forme un sous-groupe de $\sc O(E)$, 
appellé groupe spécial orthogonal de $E$. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien orienté. alors, un $u\in\sc{SO}(E)$ transforme 
une base orthonormée directe de $e$ en base orthonormée directe de $E$. \medskip
\noindent
Inversement, si un endomorphisme $u\in\sc L(E)$ transforme une base orthonormée directe~$B$ en base orthonomrmée directe $u(B)$, 
alors $u$ apparteint à $\sc{SO}(E)$. 
\bigskip

\Subsection b, Le groupe $(\sc O(n),\times)$. 


\Definition []  Une matrice $M\in\sc M_n(\ob R)$ est orthogonale si, et seulement si, $\phantom{\!\!\!\!\!\!M}^tMM=\mbox{I}_n$. \pn
L'ensemble des matrices orthogonales de $\sc M_n(\ob R)$ est noté $\sc O(n)$. 
\bigskip

\Propriete []  Le produit de deux matrices orthogonales est une matrice orthogonale. \pn
Une matrice orthogonale est inversible et son inverse est orthogonale. \pn
La transposée d'une matrice orthogonale est orthogonale. 
\bigskip

\Propriete []  Soit $n\in\ob N^*$. Alors, on a la caractérisation suivante : 
$$
\eqalign{
M\mbox{ matrice orthogonale }&\ssi \NULL^tMM=\mbox{I}_n
\cr
&\ssi M^tM=\mbox{I}_n
\cr
&\ssi M\mbox{ est inversible et }M^{-1}=\NULL^tM.
}
$$

\Propriete []  L'ensemble $(\sc O(n),\circ)$ forme un sous groupe de $(\sc Gl_n(\ob R),\times)$, appelé groupe orthogonal. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien muni d'une base orthonormale $\sc B$ et soit $u$ un endomorphisme de $E$. Alors, on a 
$$
u\mbox{ est orthogonal}\ssi\sc Mat_{\sc B}(u)\mbox{ est orthogonale}.
$$

\Remarque : en particulier, la matrice d'un endomorphisme orthogonal $u$ dans n'importe quelle base orthonormale $\sc B$ est orthogonale. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien muni d'une base orthonormale $\sc B$. Alors, les groupes $(\sc O(E),\circ)$ et $(\sc O(n),\times)$ sont isomorphes via l'application 
$$
\eqalign{ (\sc O(E),\circ)&\to(\sc O(n),\times)\cr  u&\mapsto\sc Mat_{\sc B}(u)}
$$

\Propriete []  Une matrice $M\in\sc M_n(\ob R)$ est orthogonale si, et seulement si ses colonnes (resp. ses lignes) forment une famille orthonormale pour le produit scalaire des colonnes 
$$
\langle X,Y\rangle=\phantom{\!\!\!\!\!\!X}^tXY \qquad \mbox{(resp. des lignes $\langle X,Y\rangle=X^tY$).} 
$$

\Propriete []  Soit $n\in\ob N^*$. Alors, on a 
$$
\forall M\in\sc O(n),\qquad \Det(M)\in\{-1,1\}.
$$



\Subsection c, Le groupe $(\sc{SO}(n),\times)$. 

\Definition []  Soit $n\in\ob N^*$. 
Alors, l'ensemble des matrices orthogonales de taille $n$ et de déterminant $1$ est noté 
$$
\sc{SO}(n):=\{M\in\sc O(n):\Det(M)=1\}
$$

\Propriete []  L'ensemble $\sc{SO}(n)$ muni de la loi $\times$ forme un sous-groupe de $\sc O(n)$, 
appellé groupe spécial orthogonal. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien orienté, muni d'une base orthonormale directe $\sc B$. Alors, les groupes $(\sc{SO}(E),\circ)$ et $(\sc{SO}(n),\times)$ sont isomorphes via l'application 
$$
\eqalign{ (\sc{SO}(E),\circ)&\to(\sc{SO}(n),\times)\cr  u&\mapsto \sc Mat_{\sc B}(u)}
$$

\Subsection e, Changement de base orthogonale.


\Propriete[Title=Changement de base orthonormale]
Soit $u$ un endomrophisme de $E$ et soient $\sc B=\{e_1,\cdots,e_n\}$ et $\sc B'=\{f_1,\cdots,f_n\}$ deux bases orthonormales de $E$. 
Alors, on a 
$$
\sc Mat_{\sc B',\sc B'}(u)=\sc Mat_{\sc B,\sc B'}(\Id_E)\times\sc Mat_{\sc B,\sc B}(u)\times\sc Mat_{\sc B',\sc B}(\Id_E).
$$
D'après la formule de projection sur $E$ appliquée aux bases orthonormales $\sc B$ et $\sc B'$, on a 
$$
\forall j\in\{1,\cdots,n\}, \qquad f_j=\sum_{i=1}^n\langle e_i,f_j\rangle e_i\quad\mbox{et}\quad e_j=\sum_{i=1}^n\langle f_i,e_j\rangle f_i
$$
En particulier, on remarque que les deux matrices de passages 
$$
\sc Mat_{\sc B',\sc B}(\Id_E)=\B(\langle e_i,f_j\rangle\B)_{\ss1\le i\le n\atop\ss1\le j\le n}\quad \mbox{et}\quad \sc Mat_{\sc B',\sc B}(\Id_E)=\B(\langle f_i,e_j\rangle\B)_{\ss1\le i\le n\atop\ss1\le j\le n}
$$
sont d'une part inverses l'une de l'autre mais aussi transposées l'une de l'autre. Ce sont donc des matrices orthogonales. En particulier, on a 
$$
\sc Mat_{\sc B',\sc B}(\Id_E)=\NULL^t\sc Mat_{\sc B,\sc B'}(\Id_E)
$$

\Remarque : après un changement de base orthonormal, la relation entre l'ancienne matrice $A$ et la nouvelle matrice $N$ est $$
N=\strut^tPAP
$$ 
où $P$ est une matrice orthogonale (de passage). 
\bigskip

\Section c, Classification des automorphismes orthogonaux. 

\Subsection a, Théorème fondamental. 

\Definition []  Les reflexions d'un espace euclidien $E$ sont les symétries orthogonales $s$ de~$E$ vérifiant 
$$
\dim\Ker(s-Id_E)=\dim(E)-1.
$$ 
Autrement dit, ce sont les symétries orthogonales par rapport aux hyperplans de $E$. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien, muni d'une base orthonormale $\sc B$. Alors, pour chaque reflexion $s$ de $E$, on a $\det_{\sc B}(s)=-1$. 
\bigskip

\Theoreme []  Soit $E$ un espace euclidien de dimension $n$. Alors, tout endomorphisme orthogonal de $E$ est la composée d'au plus $n$ reflexions. 
\bigskip

\Propriete []  Soit $E$ un espace euclidien orienté de dimension $n$. Alors, tout automorphisme de $\sc{SO}(E)$ est la composé d'un nombre pair $k\le n$ de reflexions. 
\bigskip

\Theoreme []  Soit $E$ un espace euclidien de dimension $n$. Alors, pour chaque automorphisme de $\sc O(E)$, il existe une base orthonormale $\sc B$ dans laquelle on a 
$$
\sc Mat_{\sc B}(u)=\pmatrix{\mbox{I}_p&&&&\cr&-\mbox{I}_q&&0&\cr&&R(\theta_1)&&\cr&0&&\ddots&\cr&&&&R(\theta_r)}
$$
avec $n=p+q+2r$ et $(\theta_1,\cdots,\theta_r)\in\ob R^r$ et 
$$
\forall \theta\in\ob R, \qquad R(\theta)=\pmatrix{\cos\theta&-\sin\theta\cr\sin\theta&\cos\theta}.
$$
De plus, on a 
$$
u\in\sc{SO}(E)\ssi q\mbox{ est pair}.
$$ 

\Remarque : on a alors 
$$
\det_{\sc B}(u)=(-1)^q\qquad\mbox{et}\qquad\mbox{Tr}(u)=p-q+2\sum_{1\le k\le r}\cos(\theta_k).
$$

\Subsection c, Automorphismes orthogonaux du plan. 

Dans toute cette section, $E$ désigne un plan euclidien, c'est-à-dire un espace euclidien de dimension $2$.
\bigskip

\Concept [] Reflexions

\Propriete []  Soit $E$ un plan euclidien orienté, muni d'une base orthonormée $\sc B$. 
Alors un endomorphisme $u$ de $E$ est une reflexion de $E$ si et seulement s'il existe $\theta\in\ob R$ tel que 
$$
\sc Mat_{\sc B}(u)=\pmatrix{\cos\theta&\sin\theta\cr\sin\theta&-\cos\theta}.
$$
Dans ce cas, $u$ est une reflexion par rapport à la droite vectorielle $\Ker(s-\Id)$. 
\bigskip

\Remarque. Si $u$ une reflexion d'un plan euclidien. Alors, $\mbox{Tr}(u)=\mbox{Tr}\B(\sc Mat_{\sc B}(u)\B)=0$.
\bigskip

\Concept [] Rotations

\Definition []  Soit $E$ un plan euclidien orienté, muni d'une base orthonormée directe $\sc B$. 
Alors un endomorphisme $u$ de $E$ est une rotation $r_\theta$ d'angle $\theta\in\ob R$ (modulo $2\pi$) si, et~seulement~si
$$
\sc Mat_{\sc B}(u)=\pmatrix{\cos\theta&-\sin\theta\cr\sin\theta&\cos\theta}.
$$ 

\Remarque : les rotations d'un plan euclidien $E$ appartiennent toutes à $\sc{SO}(E)$. 
\bigskip

\Remarque : la composée de deux rotations d'angle $\theta$ et $\theta'$ est une rotation d'angle $\theta+\theta'$. 
\bigskip

\Remarque  : on a $\Id_E=r_0$ et $-\Id_E=r_\pi$. 
\bigskip

\Propriete []  Soit $E$ un plan euclidien orienté. Si $u$ est la rotation de $E$ d'angle $\theta\in\ob R$, alors 
$$
\mbox{Tr}(u)=\mbox{Tr}\B(\sc Mat_{\sc B}(u)\B)=2\cos(\theta).
$$
De plus, si $a$ est un vecteur unitaire de $E$, on a 
$$
\Q\{\eqalign{\cos\theta&=\b\langle a,u(a)\b\rangle,\cr\sin\theta&=\Det\b(a,u(a)\b).}\W.
$$

\Propriete []  Un endomorphisme $u$ d'un plan euclidien est une rotation si, et seulement s'il transforme 
une base orthonomrée directe en base orthonormée directe. 
\bigskip

\Concept [] Composée de deux reflexions

\Propriete []  Soit $E$ un plan euclidien et soient deux droites vectorielles $D$ et $\Delta$. Alors, la~composée $s_\Delta\circ s_D$, 
des reflexions $s_D$ et $s_\Delta$ par rapport à $D$ et $\Delta$, est une rotation d'angle 
$$
\theta\equiv2(\widehat{D,\Delta})\quad [2\pi].\leqno{(**)}
$$
Réciproquement, étant donnée une rotation $r_\theta$ une rotationd'angle $\theta$ et une droite vectorielle $D$, 
il existe une unique droite vectorielle $\Delta$ vérifiant $(**)$ et on a 
$$
r_\theta=s_\Delta\circ s_D, 
$$
où $s_D$ et $s_\Delta$ désignent les reflexions par rapport à $D$ et à $\Delta$.
\bigskip

\Concept [] Structure de $\sc{SO}(E)$ 

\Theoreme []  Soit $E$ un plan euclidien. Alors, le groupe $(\sc{SO}(E),\circ)$ est constitué par toutes les rotations de $E$. 
En particulier, c'est un groupe commutatif.  
\bigskip

\Concept [] Classification des automorphismes orthogonaux du plan 

\Theoreme []  Soit $E$ un plan euclidien et soit $u\in\sc O(E)$. Alors, deux cas se présentent : \medskip
\noindent
Si $u\in\sc{SO}(E)$, alors $u$ est une rotation $r_\theta$ d'angle $\theta$. \pn
\medskip
\noindent
Si $u\notin\sc{SO}(E)$, alors $u$ est une reflexion par rapport à la droite vectorielle $D=\Ker(u-\Id_E)$. 
\bigskip

\Concept [] Structure de $\sc{SO}(2)$ et de $\sc O(2)$. 


\Theoreme []  Le groupe $\sc O(2)$ est constitué par toutes les matrices du type 
$$
R(\theta)=\pmatrix{\cos\theta&-\sin\theta\cr\sin\theta&\cos\theta}\qquad (\theta\in\ob R)
$$
et du type 
$$
S(\theta)=\pmatrix{\cos\theta&\sin\theta\cr\sin\theta&-\cos\theta}\qquad (\theta\in\ob R).
$$

\Theoreme []  Le groupe $\sc{SO}(2)$ est constitué par toutes les matrices du type $R(\theta)\ (\theta\in\ob R)$. 
En particulier le groupe $\b(\sc{SO}(2),\times\b)$ est commutatif. 
\bigskip


\Remarque : La matrice $R(\theta)$ est appelée matrice de rotation d'angle $\theta$ et l'application 
$$
\eqalign{ (\ob R,+)&\to{(\sc{SO}(2),\times)}\cr  \theta&\mapsto R(\theta)}
$$
est un morphisme surjectif de groupes. 
\bigskip


\Subsection c, Automorphismes orthogonaux de l'espace.

Dans toute cette section, $E$ désigne un espace euclidien de dimension $3$.
\bigskip

\Concept [] Reflexions

\Propriete []  Soit $E$ un plan euclidien orienté, muni d'une base orthonormée $\sc B$. 
Alors un endomorphisme $u$ de $E$ est une reflexion de $E$ si et seulement s'il existe une base orthonormale $\sc B$ pour laquelle 
$$
\sc Mat_{\sc B}(u)=\pmatrix{1&0&0\cr0&1&0\cr0&0&-1}.
$$
Dans ce cas, $u$ est une reflexion par rapport au plan vectoriel $\Ker(s-\Id)$. 
\bigskip

\Remarque. Si $u$ une reflexion d'un espace euclidien de dimension $3$, alors 
$$
\det_{\sc B}(u)=-1\quad\mbox{et}\quad
\mbox{Tr}(u)=\mbox{Tr}\B(\sc Mat_{\sc B}(u)\B)=1.
$$
\bigskip

\Concept [] Rotations

\Definition []  Soit $E$ un espace euclidien orienté de dimension $3$, muni d'une base orthonormée directe $\sc B$. 
Alors un endomorphisme $u$ de $E$ est une rotation $r_\theta$ d'angle $\theta\in\ob R$ (modulo $2\pi$) si, et~seulement~s'il 
existe une base orthonormée directe $\sc B$ telle que 
$$
\sc Mat_{\sc B}(u)=\pmatrix{1&0&0\cr0&\cos\theta&-\sin\theta\cr0&\sin\theta&\cos\theta}.
$$ 
Si $u\neq\Id_E$, la rotation $u$ est alors d'axe $\Ker(u-\Id_E)$. 
\bigskip

\Remarque : les rotations d'un espace euclidien $E$ de dimension $3$ sont éléments de  $\sc{SO}(E)$. 
\bigskip

\Remarque : la composée de deux rotations d'angle $\theta$ et $\theta'$ est une rotation, qui n'est pas nécéssairement d'angle $\theta+\theta'$. 
\bigskip

\Remarque  : $u$ est une rotation d'angle $\pi$ si, et seulement si $u$ est un retournement. 
\bigskip

\Propriete []  Soit $E$ un plan euclidien orienté. Si $u$ est une rotation de $E$ d'angle $\theta\in\ob R$. Alors 
$$
\mbox{Tr}(u)=\mbox{Tr}\B(\sc Mat_{\sc B}(u)\B)=1+2\cos(\theta).
$$
Soit $v$ le vecteur unitaire orientant l'axe $\Delta=\Ker(u-\Id_E)$ de rotation de $u$. Alors, pour~chaque vecteur $x$ orthogonal à l'axe $\Delta$, on a 
$$
u(x)=\cos(\theta)x+\sin(\theta)a\wedge x. 
$$
En particulier, si $x$ est unitaire, on a 
$$
\Q\{\eqalign{\cos\theta&=\b\langle x,u(x)\b\rangle,\cr\sin\theta&=\Det\b(a,x,u(x)\b).}\W.
$$

\Propriete []  Un endomorphisme $u$ d'un espace euclidien de dimension $3$ est une rotation si, et seulement s'il transforme 
une base orthonomrée directe en base orthonormée directe. 
\bigskip

\Concept [] Composée de deux reflexions

\Propriete []  Soit $E$ un plan euclidien et soient deux plans vectoriels $P\neq\sc P$. Alors, la~composée $s_P\circ s_{\sc P}$, 
des reflexions $s_P$ et $s_{\sc P}$ par rapport à $P$ et $\sc P$, est une rotation d'axe $P\cap\sc P$ et d'angle 
$$
\theta\equiv2(\widehat{P,\sc P})\quad [2\pi].\leqno{(**)}
$$
Inversement, étant donnée une rotation $r_\theta$ une rotation d'angle $\theta$ et un plan vectoriel~$P$, 
il existe un unique plan vectoriel $\sc P$ vérifiant $(**)$ et on a 
$$
r_\theta=s_{\sc P}\circ s_P, 
$$
où $s_P$ et $s_{\sc P}$ désignent les reflexions par rapport à $P$ et à $\sc P$.
\bigskip

\Concept [] Composée de trois reflexions


\Propriete []  Dans un espace euclidien $E$ de dimension $3$, la composé de trois reflexions est soit une reflexion, soit la composée commutative d'une rotation et d'une reflexion par rapport à un plan orthogonal à l'axe de rotation. 
\bigskip

\Propriete []  Un endomorphisme $u$ d' un espace euclidien $E$ de dimension $3$ est la composée commutative d'une rotation et d'une reflexion par rapport à un plan orthogonal à l'axe de rotation si, et seulement s'il existe une base orthonormée telle que 
$$
\sc Mat_{\sc B}(u)=\pmatrix{-1&0&0\cr0&\cos\theta&-\sin\theta\cr0&\sin\theta&\cos\theta}.
$$ 

\Concept [] Structure de $\sc{SO}(E)$ 

\Theoreme []  Soit $E$ un espace euclidien de dimension $3$. Alors, le groupe $(\sc{SO}(E),\circ)$ est constitué par toutes les rotations de $E$. 
\bigskip

\Concept [] Classification des automorphismes orthogonaux du plan 

\Theoreme []  Soit $E$ un plan euclidien et soit $u\in\sc O(E)$. Alors, deux cas et plusieurs sous-cas se présentent : \medskip
\noindent
$\underline{\mbox{Cas } u\in\sc{SO}(E)}$ : \medskip\noindent 
Soit $u=\Id_E$ (auquel cas $\Ker(u-\Id_E)=E$ n'est pas une droite vectorielle) \medskip
\noindent
Soit $u$ est une rotation $r_\theta$ d'angle $\theta\not\equiv0\ [2\pi]$ et d'axe $D:=\Ker(u-\Id_E)$. 
\bigskip
\noindent
$\underline{\mbox{Cas } u\in\sc{SO}(E)}$ : \medskip
\noindent
Soit $\sc P:=\Ker(u-\Id_E)$ est un plan vectoriel et alors $u$ est la reflexion par rapport à $\sc P$. \medskip
\noindent
Soit $\Ker(u-\Id_E)=\{0\}$ et  $u=-\Id_E$ (auquel cas $\Ker(u+\Id_E)=\{E\}$ n'est pas un plan vectoriel)\medskip
\noindent
Soit $\Ker(u-\Id_E)=\{0\}$ et $D=\Ker(u+\Id_E)$ est une droite vectorielle, alors $u$ est la composée commutative d'une rotation d'axe $D$ et d'une reflexion par rapport au plan vectoriel perpendiculaire à $D$. 
\bigskip





\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\EndomorphismesSymétriques\MatricesSymétriques\FormesQuadratiques\EndomorphismesOrthogonaux\MatricesOrthogonales}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}










\hautspages{Olus Livius Bindus}{Champs de vecteurs}

\pagetitretrue

\Chapter fonc, Champs de vecteurs.
\bigskip


\Section a, Formes différentielles de degré $1$.

\Subsection aa, Généralité. 

\Definition []  Soit $n\in\{2,3\}$. On appelle forme différentielle de degré $1$ sur $A\subset\ob R^n$ 
toute application définie sur $A$ du type 
$$
\eqalign{
	\omega:A&\to\sc L(\ob R^n,\ob R)\cr
	(x_1,\cdots, x_n) &\mapsto \omega_1(x_1,\cdots,x_n)\d x_1+\cdots+\omega_n(x_1,\cdots,x_n)\d x_n
}
$$
La forme différentielle $\omega$ est dite continue sur $A$ si, 
et seulement si, la fonction $\omega_i:A\to\ob R$ est continue sur~$A$ pour $1\le i\le n$. 
\bigskip

\Exemple.  $(x,y)\mapsto y\d x-x\d y$ ou encore $(x,y,z)\mapsto \d x+z\ch x\d y+\sh y\d z$.
\bigskip

\Remarque. Si $f:A\to\ob R$ est une fonction de classe $\sc C^1$, sa différentielle 
$$
\eqalign{
	\d f:  A&\to\sc L(\ob R^n,\ob R)\cr
	(x_1,\cdots, x_n)&\mapsto {\ds\partial f\F\ds\partial x_1}(x_1,\cdots,x_n)\d x_1+\cdots+{\ds\partial f\F\ds\partial x_n}(x_1,\cdots,x_n)\d x_n
}
$$
est une forme différentielle continue. 
\bigskip

\Subsection b, Intégrale curviligne.

\Rappel :  On appelle arc paramétré (ou chemin) de classe~$\sc C^k$ toute fonction 
$\varphi:[a,b]\to\ob R^n$ de classe $\sc C^k$. Le support de cet arc est alors l'ensemble $\varphi\b([a,b]\b)$. 
\bigskip

\Exemples.  $\eqalign{f_1: [1,4]&\to\ob R^2\cr t&\mapsto(t\cos 5t,t\sin 5t)}$ et $\eqalign{f_2:[0,8]&\to\ob R^3\cr t&\mapsto(2\cos 5 t,2\sin 5t, t)}$. 
\bigskip

\Definition []  Soit $n\in\{2,3\}$, soit $\varphi:[a,b]\to\ob R^n$ un arc paramétré de classe $\sc C^1$ 
et soit $\omega$ une forme différentielle de degré $1$ 
continue sur le support de cet arc. Alors, l'intégrale de la forme $\omega$ 
sur le chemin $\varphi$ est le nombre 
$$
\int_\varphi\omega:=\int_a^b\B(\omega_1\b(\varphi(t)\b)\varphi_1'(t)+\cdots+\omega_n\b(\varphi(t)\b)\varphi_n'(t)\B)\d t
$$
où l'on a posé $\varphi(t):=\b(\varphi_1(t),\cdots,\varphi_n(t)\b)$ pour $a\le t\le b$. 
\bigskip

\Remarque. Cette intégrale définit la circulation/le travail de la forme différentielle $\omega$ sur l'arc $\varphi$...
\bigskip

\Exemples.  $\ds\int_{f_1}(x\d x+y\d y)={15\F2}$ et $\ds\int_{f_2}\Q({zx\d x+zy\d y\F x^2+y^2}+\ln\sqrt{x^2+y^2}\d z\W)=8\ln 2$. 
\bigskip

\Propriete []  Soient $n\in\{2,3\}$, $\varphi:[a,b]\to\ob R^n$ un arc paramétré de classe $\sc C^1$ 
et $F$ une fonction de classe $\sc C^1$ sur un ouvert contenant l'image de $\varphi$. 
Alors, 
$$
\int_\varphi\d F=F\b(\varphi(b)\b)-F\b(\varphi(a)\b).
$$ 

\Remarque{ \it1}. L'energie interne $U$, l'enthalpie $H$ et l'entropie $S$ vérifient cette propriété, 
d'après le postulat fondamental de la thermodynamique et le ``théorème" de Carathéodory. 
\bigskip

\Remarque{ \it 2}. La propriété n'est pas vérifiée par toutes les 
formes différentielles : $$
\mbox{pour }\quad\varphi_1:\Q\{
\eqalign{
x(t)=\cos t\cr
y(t)=\sin t}\W.\quad\mbox{ avec }\quad0\le t\le 2\pi\quad\mbox{ on a }\quad
\ds \int_\varphi{x\d y-y\d x\F x^2+y^2}=2\pi\neq 0.
$$ 

\Section a, Formes différentielles de degré $2$.

\Subsection aa, Généralité. 

\Definition []  Une forme différentielle de degré $2$ est une application définie sur $A$ du type 
$$
\omega:(x,y,z)\mapsto P(x,y,z)\d x\wedge\d y+Q(x,y,z)\d x\wedge\d z+R(x,y,z)\d y\wedge\d z.
$$ 
La forme différentielle $\omega$ est dite continue sur $A$ si, 
et seulement si, les fonctions $P$, $Q$ et~$R$ sont continues sur~$A$. 
\bigskip

\Exemple.  $(x,y,z)\mapsto x\d x\wedge\d y+\ch(xyz)\d x\wedge\d z+\sh y\d y\wedge\d z$
\bigskip

\Subsection d, Intégrale de surface. 
\bigskip

\Definition []  Une surface paramétrée est une application de classe $\sc C^1$ du type 
$$
\eqalign{\vec f: D\subset\ob R^2&\to\ob R^3\cr(u,v)&\mapsto  \Q(X(u,v),Y(u,v),Z(u,v)\W)}.
$$ 


\Remarque. Si le paramétrage est ``régulier'', 
le vecteur $\ds {\partial \vec f\F\partial u}(u,v)\wedge{\partial\vec f\F\partial v}(u,v)$ ne s'annule pas, 
il est normal à la surface, qu'il oriente.
\bigskip
 
 
\Definition []  Soit $\vec f:(u,v)\mapsto \b(X_{u,v},Y_{u,v},Z_{u,v}\b)$, définie sur $D\subset\ob R^2$, 
une surface paramétrée de~classe $\sc C^1$ 
et soit 
$$
\omega:(x,y,z)\mapsto P(x,y,z)\d x\wedge\d y+Q(x,y,z)\d x\wedge\d z
+R(x,y,z)\d y\wedge\d z
$$ 
une~forme différentielle continue sur cette surface 
(sur l'image de $\vec f$). Alors, l'intégrale de la forme différentielle $\omega$ 
sur la surface paramétrée $\vec f$ est la nombre $$\eqalign{
\int_f\omega:=&
\int\limits_DP\b(X_{u,v},Y_{u,v},Z_{u,v}\b)\!\!\underbrace{\d X_{u,v}\wedge\d Y_{u,v}}
_{\ds{\mbox{D}(X,Y)\F\mbox{D}(u,v)}\d u\d v}\!
+\int\limits_DQ\b(X_{u,v},Y_{u,v},Z_{u,v}\b)\!\!\underbrace{\d X_{u,v}\wedge\d Z_{u,v}}
_{\ds{\mbox{D}(X,Z)\F\mbox{D}(u,v)}\d u\d v}\!
\cr
&+\int\limits_D
R\b(X_{u,v},Y_{u,v},Z_{u,v}\b)\!\!\underbrace{\d Y_{u,v}\wedge\d Z_{u,v}}
_{\ds{\mbox{D}(Y,Z)\F\mbox{D}(u,v)}\d u\d v}
}
$$

\Exemple.  Pour $f:(u,v)\mapsto(u\cos v, u\sin v,u)$ sur $D:=[1,2]\times[0,2\pi]$, on a $$
\int_fz\d x\wedge\d y=?
$$
\bigskip

\Remarque. Cette intégrale définit le flux du champ de vecteur $\omega$ à travers 
la surface orientée paramétrée par $\vec f$. 
\bigskip


\Subsection c, Analyse vectorielle. 
\bigskip
L'espace $E=\ob R^3$ est muni de sa structure usuelle d'espace vectoriel euclidien 
orienté, du repère orthonormé directcanonique $\{0,\vec i,\vec j,\vec k\}$. 
\bigskip

\Concept Gradient

\Definition [] Soit $f:U\to\ob R$ une fonction numérique 
de classe $\sc C^1$ sur un ouvert $U$ de $\ob R^3$. 
Alors, le~gradient de la fonction $f$ au point $(x,y,z)\in U$ est le vecteur 
$$
\vec{\grad} f(x,y,z):=
\pmatrix{\ds{\partial f\F\partial x}(x,y,z)\cr\ds{\partial f\F\partial x_n}^{\strut}(x,y,z)\cr\ds{\partial f\F\partial x_n}^{\strut}(x,y,z)}.
$$ 

\Concept Divergence

\Definition []Soit $\vec f:(x,y,z)\mapsto \b(P(x,y,z),Q(x,y,z),R(x,y,z)\b)$ 
un champ de vecteur de classe~$\sc C^1$ sur un~ouvert $U$ de $\ob R^3$. Alors, la divergence de $\vec f$ au point $(x,y,z)\in U$ est le nombre réel 
$$
\mbox{div } \vec f(x,y,z):={\partial P\F\partial x}(x,y,z)+{\partial Q\F\partial y}(x,y,z)+{\partial R\F\partial z}(x,y,z). 
$$

\Concept Rotationnel

\Definition[] Soit $\vec f:(x,y,z)\mapsto \b(P(x,y,z),Q(x,y,z),R(x,y,z)\b)$ 
un champ de vecteur de classe~$\sc C^1$ 
sur un~ouvert $U$ de $\ob R^3$. Alors, le rotationnel de $\vec f$ au point $(x,y,z)\in U$ est le nombre réel 
$$
\vec{\mbox{Rot}}\ \vec f(x,y,z):=\pmatrix{\ds
{\partial R\F\partial y}(x,y,z)-{\partial Q\F\partial z}(x,y,z)
\cr\ds
{\partial P^{\strut}\F\partial z}(x,y,z)-{\partial R\F\partial x}(x,y,z)
\cr\ds
{\partial Q^{\strut}\F\partial x}(x,y,z)-{\partial P\F\partial y}(x,y,z)
\cr}=
\pmatrix{\ds{\partial\hfill\F\partial x}\cr\ds{\partial\hfill\F\partial y}^{\strut}\cr\ds{\partial\hfill\F\partial z}^{\strut}\cr}
\Lambda\pmatrix{P(x,y,z)\cr Q(x,y,z)^{\strut}\cr R(x,y,z)^{\strut}}
$$

\Concept Laplacien

\Definition Soit $f:U\mapsto \ob R$ une fonction numérique 
(resp. $f:U\mapsto \ob R^3$ un champ de vecteur) de classe $\sc C^2$ 
sur un~ouvert $U$ de $\ob R^3$. Alors, le laplacien de $f$ en $(x,y,z)\in U$ est 
$$
\Delta f(x,y,z):={\partial^2f\F\partial x^2}(x,y,z)+{\partial^2f\F\partial y^2}(x,y,z)+{\partial^2f\F\partial z^2}(x,y,z). 
$$ 

\Remarque. On a $\Delta f(x,y,z)=\mbox{div }\b(\vec{\grad}\ f\b)(x,y,z)$. 
\bigskip

\Definition []  Un ouvert $U$ de $\ob R^n$ est étoilé si, et seulement si, il existe $a\in U$ tel que $\forall x\in U$ le segment $S(a,x)$, d'extrémités $a$ et $x$, est inclus dans $U$. 

\Exemples.  l'ouvert $\ob R^2\ssm\{(0,0)\}$ n'est pas étoilé alors que $\ob R^2\ssm\{(t,0):t\le 0\}$ l'est par rapport à $(1,0)$. 
\bigskip 

\Concept Potentiel scalaire

\Definition Soit $n\in\{2,3\}$, $U$ un ouvert étoilé 
de $\ob R^n$ et soit $\vec V$ un champ de vecteur de classe $\sc C^1$ sur $U$. Alors, les deux propositions suivantes 
sont équivalentes : \medskip\noindent
*) Il existe une fonction $f:U\to\ob R$ de classe $\sc C^2$ telle que $\vec V=\vec{\grad}\ f$ sur $U$. 
\medskip\noindent
**) $\vec{\mbox{Rot}}\ V=\vec 0$ sur $U$. \medskip\noindent
Lorsque ces propriétés sont vérifiées, 
on dit que $V$ dérive du potentiel scalaire $f$.
\bigskip

\Exemple.  Le champs de vecteur $\vec V:(x,y)\mapsto (2x,-2y)$ dérive de $f:(x,y)\mapsto x^2-y^2$
\bigskip
 
\Remarque : On a $f(x_1\cdots,x_n)=f(\vec 0)+\int_0^1\vec V(tx_1,\cdots,tx_n). 
\vec{(x_1,\cdots,x_n)}\d t$ lorsque $U$ est étoilé par rapport à $\vec 0$. 
Notant $\vec V:=(V_1,\cdots,V_n)$, on a donc 
$$
f(x_1,\cdots,x_n)=f(0,\cdots,0)+\sum_{1\le k\le n}x_k\int_0^1V_k(tx_1,\cdots,tx_n)\d t\qquad\b((x_1,\cdots,x_n)\in U\b). 
$$

\Remarque : Le théorème de Schwarz donne $(*\Rightarrow**)$ et 
le théorème de Poincaré donne $(*\Leftarrow**)$. 
\bigskip

\Concept Potentiel vecteur

\Definition []Soit $n\in\{2,3\}$, $U$ un ouvert étoilé de $\ob R^n$ et $V$ 
un champs de vecteur de classe $\sc C^1$ sur $U$. Alors, les deux propositions suivantes 
sont équivalentes : \medskip\noindent
*) Il existe un champ de vecteur $\vec W:U\to\ob R^n$ de classe $\sc C^2$ 
tel que $\vec V=\vec{\mbox{Rot}}\ \vec W$ sur $U$. \medskip\noindent
**) $\mbox{Div\ }\vec V=0$ sur $U$. \medskip\noindent
Lorsque ces propriétés sont vérifiées, on dit que $\vec V$ dérive du potentiel 
vecteur $\vec W$.
\bigskip

\Exercice. Le champs de vecteur $\vec V:(x,y,z)\mapsto (yz,-xz,x^2+xy)$ dérive d'un potentiel vecteur 
\bigskip

\Remarque. Lorsque $U$ est étoilé par rapport à $\vec 0$, on a 
$\vec W(x,y,z)=\int_0^1\vec V(tx,ty,tz)\wedge \vec{(x,y,z)}\d t$ \pn
à un champ de gradient près. 
Notant $\vec V=(V_1,V_2,V_3)$ et $\vec W=(W_1,W_2,W_3)$, on a 
$$
\eqalign{
W_1(x,y,z)&=\int_0^tt\B(zV_2(tx,ty,tz)-yV_3(tx,ty,tz)\B)\d t\cr
W_2(x,y,z)&=\int_0^tt\B(xV_3(tx,ty,tz)-zV_1(tx,ty,tz)\B)\d t\cr
W_3(x,y,z)&=\int_0^tt\B(yV_1(tx,ty,tz)-xV_2(tx,ty,tz)\B)\d t\cr
}\qquad\b((x,y,z)\in U\b).
$$ 

\Section df, Formule de Green-Riemann, de Stokes et Ostrogradski.

\Theoreme [Title=Formule de Green-Riemann] Soit $\omega=P\d x+Q\d y$ une forme différentielle 
de degré $1$ et de classe $\sc C^1$ sur un compact simple $K$ de $\ob R^2$. 
Notant $\partial K$ le bord orienté de $K$, on a  
$$
\int_{\partial K}\omega=\int_K\d \omega:=\int\!\!\int_K\Q({\partial Q\F\partial x}(x,y)-{\partial P\F\partial y}(x,y)\W)\d x\d y
$$

\Theoreme [Title=Formule de Stokes] Soit $\Sigma$ une surface (compacte simple) 
orientée de $\ob R^3$ et soit $\partial\Sigma$ son bord orienté. 
Pour toute forme différentielle $\omega$, de degré $1$ et de classe $\sc C^1$, 
on a 
$$
\int_{\partial\Sigma}\omega=\int\!\!\int_\Sigma\d\omega, 
$$
où $\d\omega$ reprèsente la différentielle exterieure de $\omega$. 
\bigskip

\Theoreme [Title=Formule d'Ostrogradski] Soit $\omega=P\d x\ \Lambda\ \d y
+Q\d x\ \Lambda\ \d z+R\d y\ \Lambda\ \d z$ une forme différentielle 
de degré $2$ et de classe $\sc C^1$ sur un compact simple $K$ de $\ob R^3$. 
Notant $\partial K$ le bord orienté de $K$, on a  
$$
\int_{\partial K}\omega=\int_K\d \omega:=
\int\!\!\int\!\!\int_K\Q({\partial P\F\partial z}(x,y)-{\partial Q\F\partial y}(x,y)
+{\partial R\F\partial x}(x,y)\W)\d x\d y\d z.
$$

\Remarque. Ces formules sont des applications de la même idée, 
à $3$ contextes légérement différents. Attention à l'orientation ! 



\eject
\hautspages{Olus Livius Bindus}{Métrique des courbes planes}

\pagetitretrue

\Chapter fonc, Métrique des courbes planes.
\bigskip


\noindent
Dans ce chapitre, le symbole $\sc P$ désigne un plan affine euclidien (muni d'un produit scalaire) 
et $(O,\vec i,\vec j)$ désigne un repère orthonormé direct de $\sc P$. 
\bigskip

\Section a, Arcs géométriques (orientés).

\Rappel : Une courbe paramétrée de $\sc P$ est le couple formé par un intervalle $I$ et par une application 
$$
\eqalign{f: I&\to\sc P\cr t&\mapsto  M(t)}
$$ 
Elle est de classe $\sc C^k$ si, et seulement si, ses fonctions coordonnées $t\mapsto x(t)$ et $t\mapsto y(t)$ définies par 
$$
\forall t\in I, \qquad \vec {OM(t)}=f(t)=x(t)\vec i+y(t)\vec j. 
$$
sont $k$ fois dérivables, de dérivées continues, sur l'intervalle $I$. 
\bigskip

\Rappel : Le support d'un arc paramétré $(I,f)$ est l'ensemble $\{f(t):t\in I\}$. 
\bigskip

\Remarque : l'arc paramétré $(I,f)$ est orienté : le sens de parcours de l'arc est déterminé par $f(t)$ pour les valeurs croissantes de $t\in I$. 
\bigskip

\Concept Arc géométriques

\Definition[] Un ensemble $A\subset\sc P$ est un arc géométrique de classe~$\sc C^k$  
s'il existe un arc paramétré $(I,f)$ de classe $\sc C^k$ dont $A$ est le support. 
\bigskip

\Remarque : Si $A$ est le support d'un arc paramétré $(I,f)$ de classe $\sc C^k$, alors $A$ est un arc géométrique (orienté) de classe $\sc C^k$. 
\bigskip


\Rappel :  Soit $(I,f)$ un arc paramétré de classe $\sc C^1$. Alors, \medskip\noindent
L'arc $(I,f)$ est singulier en un point $t\in I$ si, et seulement si $f'(t)=0$. \medskip\noindent
L'arc $(I,f)$ est régulier en un point $t\in I$ si,n et suelement si $f'(t)\neq 0$. \medskip\noindent
L'arc $(I,f)$ est régulier si, et seulement si 
l'arc $(I,f)$ est régulier en chaque point 
$$
(I,f)\mbox{ est régulier}\ssi\forall x\in I, \quad f'(t)\neq 0.
$$

\Rappel :  Soit $t\in I$ un point régulier d'un arc paramètré $(I,f)$ de classe $\sc C^1$. 
Alors, le vecteur unitaire tangent en $M=f(t)$ à l'arc $(I,f)$, orienté par le sens de parcours, est le vecteur 
$$
\vec T:={f'(t)\F\|f'(t)\|}. 
$$


\Rappel :  Soit $(I,f)$ un arc paramètré de classe $\sc C^2$. Alors, \medskip\noindent
L'arc $(I,f)$ est birégulier en $t\in I$ si, et seulement si, la famille $\{f'(t), f''(t)\}$ est libre. \medskip\noindent
L'arc $(I,f)$ est birégulier si, et seulement si l'arc $(I,f)$ est birégulier en chaque point $t\in I$. 
$$
(I,f)\mbox{ est birégulier}\ssi\forall t\in I,\mbox{ la famille }\{f'(t),f''(t)\} \mbox{ est libre}.
$$



\Section b, Abscisse curviligne. 


\Definition []  Soit $(I,f)$ un arc paramètré de classe $\sc C^1$ et $(t_0,t)\in I^2$
$$
\vec{OM(t)}=f(t)=x(t)\vec i+y(t)\vec j\qquad (t\in I)
$$
Alors, l'abscisse curviligne de $M(t)$ à partir de $M(t_0)$ est le nombre réel $s(t)$ défini par 
$$
s(t)=\int_{t_0}^t\|f'(u)\|\d u. \eqdef{Defs}
$$
On remarque que $t\mapsto s(t)$ est une fonction croissante, de classe $\sc C^1$ sur $I$ et que 
$$
{\d s\F\d t}=\|f'(t)\|\qquad(t\in I).
$$


\Propriete []  Si l'arc $(I,f)$ est régulier, l'application $t\mapsto s(t)$ est un difféomorphisme 
de classe $\sc C^1$ croissant et on a 
$$
{\d\F\d s}\vec{OM}={\d \F\d s}f(t)={\d f\F \d t}{\d t\F\d s}={\d f\F \d t}/{\d s\F\d t}={f'(t)\F\|f'(t)\|}=\vec T.
$$

\Remarque : on dit aussi que l'abscisse curviligne est un paramétrage admissible : cela signifie que l'on peut utiliser $s$ à la place de $t$ 
pour paramétrer la courbe. Ainsi, on pourra utiliser le paramétrage normal $f(s)$ pour $s(a)\le s\le s(b)$ à la place du paramétrage $f(t)$ pout $a\le t\le b$. 
\bigskip

\Definition []  La longeur d'un arc paramétré $\sc A:=([a,b],f)$ de classe $\sc C^1$ est le nombre 
$$
L(\sc A):=\int_a^b\|f'(t)\|\d t=s(b)-s(a). 
$$

\Section b, Repère de Frenet et courbure.

\Definition []  Soit $(I,f)$ un arc paramètré {\bf plan} de classe $\sc C^1$ régulier en $t\in I$, avec 
$$
\vec{OM(t)}=f(t)=x(t)\vec i+y(t)\vec j\qquad (t\in I),
$$ 
et soit~$\vec T$ le vecteur tangent en $M=f(t)$ à $(I,f)$. Alors, on appelle vecteur unitaire 
normal à $(I,f)$, orienté par le sens de parcours, 
l'unique vecteur unitaire $\vec N$ vérifiant
$$
(\vec T,\vec N)\equiv{\pi\F 2}\quad[2\pi]. \leqno{(*)}
$$ 

\Rappel :  Si le vecteur $\vec T=a\vec i+b\vec j$ est unitaire, alors le vecteur $\vec N=-b\vec i+a\vec j$ est unitaire est satisfait $(*)$. 
\bigskip

\Remarque. Le repère $(M,\vec T, \vec N)$ est appelé repère de Frenet de $(I,f)$ en $M=f(t)$. 
\bigskip

\Propriete []  Soit $(I,f)$ un arc paramètré de classe $\sc C^2$ régulier en $t\in I$ et soit~$(M,\vec T,\vec N)$ 
son repère de Frenet en $M=f(t)$. Alors, il existe un unique nombre $\gamma\in\ob R$ tel que 
$$
{\d \vec T\F\d s}={1\F\|f'(t)\|}{\d\vec T\F\d t}=\gamma\vec N.
$$
Ce nombre réel $\gamma$ est appelé courbure de l'arc $(I,f)$ en $M=f(t)$ et il vérifie également 
$$
{\d \vec N\F\d s}=-\gamma\vec T.
$$

\Remarque : les deux relations précédentes sont les relations de Frenet. 
%

\Propriete []  L'arc $(I,f)$ est biregulier en $t$ si, et seulement si $\gamma\neq0$. 
\bigskip

\Remarque{ \it 1}. Dans les cas de points d'inflection, la courbure $\gamma$ est nulle. 
\bigskip

\Remarque{ \it 2}. La courbure $\gamma$ peut être négative. $|\gamma|$ est appelé courbure géomètrique. 
\bigskip

\Definition []  Soit $(I,f)$ un arc birégulier en $t\in I$, 
de repère de Frenet $(M,\vec T,\vec N)$ en $M=f(t)$. 
Alors, le rayon de courbure de l'arc $(I,f)$ en $M$ est le nombre 
$$
r={1\F \gamma}.
$$ 
Le point $C$ défini par $\vec MC=r\vec N$ 
est appelé centre de courbure de l'arc $(I,f)$ en $M$. \medskip\noindent
Le cercle $\sc C(C,r)$ de centre $C$ et de rayon $r$ est appelé cercle de courbure 
ou cercle osculateur de l'arc $(I,f)$ en $M$. 
\bigskip

\Section c, Relévement.


\Propriete []  Soit $(I,f)$ un arc paramètré régulier de classe $\sc C^k$ pour $k\ge2$. 
Alors, il existe une fonction $\alpha$ de classe $\sc C^{k-1}$ sur $I$ telle que 
$$
\forall t\in I, \qquad \vec T(t)=\cos\alpha(t)\vec i+\sin\alpha(t)\vec j. \eqdef{zer}
$$

\Remarque : cette propriete de relévement permet d'associer un angle de classe $\sc C^{k-1}$ au vecteur tangent unitaire, 
sans utiliser $\arctan$, $\arccos$ ou $\arcsin$. 
\bigskip


\Propriete []  Soit $(I,f)$ un arc paramètré régulier de classe $\sc C^2$, avec 
$$
\vec{OM(t)}=f(t)=x(t)\vec i+y(t)\vec j\qquad (t\in I). 
$$
et soit $\alpha$ la fonction de classe $\sc C^1$ vérifiant \eqref{zer}. 
Alors, les fonctions $x$ et $y$ vérifient 
$$
{\d x\F\d s}=\cos(\alpha)\qquad \mbox{et}\qquad {\d y\F\d s}=\sin\alpha.
$$ 
La courbure satisfait
$$
\gamma={\d\alpha\F\d s}
$$
De même les vecteurs $\vec T$ et $\vec N$ du repère de Frenet vérifient
$$
{\d\vec T\F\d\alpha}=\vec N\qquad\mbox{et}\qquad{\d\vec N\F\d\alpha}=-\vec T.
$$

\Propriete []  Soit $(I,f)$ un arc paramètré birégulier de classe $\sc C^2$. Alors, 
l'angle $\alpha$ est un paramétrage admissible. 



\Section c, Vitese et acceleration dans le repère de Frenet. 

\Propriete []  Soit $(I,f)$ un arc paramètré régulier de classe $\sc C^2$, avec 
$$
\vec{OM(t)}=f(t)=x(t)\vec i+y(t)\vec j\qquad (t\in I). 
$$
Alors, le vecteur vitesse instantannée en $t\in I$ est 
$$
\vec v(t)={\d\vec{OM(t)}\F \d t}=f'(t)=x'(t)\vec i+y'(t)\vec j=\|f'(t)\|\vec T=v(t)\vec T.
$$
la vitesse instantannée en $t\in I$ étant définie par 
$$
v(t)={\d s\F\d t}=\|f'(t)\|.
$$
Le vecteur acceleration instantanné en $t\in I$ est 
$$
\vec a(t)={\d^2\vec{OM(t)}\F \d t^2}=f''(t)=x''(t)\vec i+y''(t)\vec j=v'(t)\vec T+\gamma v(t)\vec N.
$$







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%																%
%							Mathematicon			  				%
%																%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\catcode`@=11\relax



\eject
\hautspages{}{}%
\pagetitretrue
\overfullrule=0pt%
\nopagenumber
\newdimen\LD@dimena\LD@dimena=18cm\relax\advance\LD@dimena by-1cm\relax
\newdimen\LD@dimenb\LD@dimenb=28cm\relax\advance\LD@dimenb by-4cm\relax
 %
\centerline{%
	%\unless\ifHtml\font\SvgText=Papyrus\relax\fi
	\tikzpicture
	\ifHtml 
		\node[text height=2\LD@dimenb,text width=2\LD@dimenb,text only=false] (a) {\noindent\smash{\Image[Height=20\LD@dimenb,width=20\LD@dimenb]{\imageFolder/Hekathomb5gray.jpg}}}%
	\else
			\node[text width=\LD@dimena,text height=\LD@dimenb%,draw,line width=2pt,inner sep=0pt
	] (a) at (0,-2) {\noindent\smash{\Image[Width=\LD@dimena,Height=\LD@dimenb]{\imageFolder/Hekathomb5gray.jpg}}}%
	\fi
node [below=0.7cm,inner sep=1pt] (b) at (a.north) {};
\node [inner sep=1.5pt,above=2.7cm] (c) at (a.south) {};
\node [inner sep=1pt,below=1.5cm] (d) at (c) {};
\foreach\angle in {0,15,...,360}
{%
\node[color=black,scale=1.5] at (b.\angle) {\LD@Font@Gothic Olus Livius Bindus};
\node[color=black,scale=8] at (c.\angle) {\unless\ifHtml\LD@Font@Inferno\fi Mathematicon}; 
}%
\node[color=gray,scale=1.5] at (b) {\LD@Font@Gothic Olus Livius Bindus};
\node[color=gray,scale=8] at (c) {\unless\ifHtml\LD@Font@Inferno\fi Mathematicon};
\foreach\angle in {0,15,...,360}
{%
\node[color=black,scale=2] at  (d.\angle)  {\LD@Font@Sanctuary  Errare humanum est Perseverare diabolicum};
}%
\node[color=gray, scale=2] at (d) {\LD@Font@Sanctuary Errare humanum est Perseverare diabolicum};
\endtikzpicture}%
\headline={\ifpagetitre\the\hautpagetitre
\else\ifodd\pageno\the\hautpagedroite\else\the\hautpagegauche\fi\fi }
\footline={\ifpagetitre\the\baspagetitre
\else\ifodd\pageno\the\baspagedroite
\else\the\baspagegauche\fi\fi \global\pagetitrefalse}
\hautpagegauche={\ifMidFolio\the\hautpagemilieu\else\tenrm\folio\hfil\the\auteurcourant\hfil\fi}
\hautpagedroite={\ifMidFolio\the\hautpagemilieu\else\hfil\the\titrecourant\hfil\tenrm\folio\fi}
\eject
% let's create a different toc file for the second volume and let's overload the tac commands with the tac commands
\let\writetocentry\writetacentry
\hautspages{Olus Livius Bindus}{Table des matières}%
%\Book PT, {Math. Spé.}.
\centerline{\seventeenbf Table des matières}
\bigskip
\bigskip
\readcontentsfile{tac}%

%\unless\ifHtml
%	\readcontentsfile{tac}%
%\fi
\eject
\hautspages{Olus Livius Bindus}{Réduction des matrices et des endomorphismes}%

\Chapter Reduction, Réduction des matrices et des endomorphismes. 

\Section reduc§Intro, Introduction. 

\Subsection reduc§quoi, Qu'est-ce-que la réduction ?. 

Réduire une matrice carrée $A$ consiste à construire une matrice inversible $P$ réalisant la décomposition de $A$ en produit matriciel du type
\Equation [\bf Trigonalisation de $A$]
$$
A=P^{-1}T`\quad \mbox{ avec $T$ matrice triangulaire supérieure,}
$$
ou mieux encore, lorsque $A$ est dite diagonalisable, en produit du type
\Equation [\bf Diagonalisation de $A$] 
$$
A=P^{-1}DP\quad \mbox{ avec $T$ matrice diagonale.}
$$

\Subsection reduc§comment, Algorithme de réduction.

\tikzstyle{object}=[ellipse,fill=red!20, draw,inner sep=0.5em,text depth=-0.2em]
\tikzstyle{operator}=[rectangle,rounded corners,fill=blue!20, draw,text height=1em,text depth=0.2em,inner sep=0.4em]
\tikzstyle{fork}=[diamond,fill=green!20, draw,text height=1.2em,text depth=0.2em]
\tikzstyle{line}=[draw,line width=0.5ex]
\tikzstyle{thinline}=[draw,line width=0.2ex]

Pour réduire les matrices, le cours suggère l'algorithme (bourrin mais efficace) suivant : 

\noindent
\centerline{%
\font\SvgText=cmr8\relax
\tikzstyle{every node}=[inner sep=2pt]
\tikzpicture
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\node [object] (a) {Matrice $A$} ;
\node [operator,right of=a, node distance=2.7cm] (b) {\eightpts$\det(A-\lambda I_n)$} ;
\draw [line] (a)--(b) ; 
\node[object, right of=b,node distance=3cm] (c) {\eightpts Polynome $P_A$} ;
\draw[line,->] (b) -- (c) ;
\node[fork,node distance=2cm,below of=c] (m) {\eightpts$\forall i:m_i=n_i$} ;
\node[below of=m,node distance=2.2cm] (prep) {};
\path (m)-- node[midway] (mo) {oui} (prep) ;
\draw [line] (m)--(mo) ; \draw [line,->] (mo)--(prep);
\node[below of=prep,node distance=0.2cm] (p) {$=$} ;
\node[right of=p] (q) {$P$} ;
\node[above of=q,node distance=0.3cm] (abq) {};
\node[right of=q] (r) {$D$} ;
\node[above of=r,node distance=0.3cm] (abr) {};
\node[operator,node distance=1cm,font=\texttt,below of=r] (t) {\eightpts Inversion} ;
\node[right of=r] (s) {$P^{-1}$} ;
\node[left of=p,label=180:{\qquad}] (pp) {$A$} ;
\node[node distance=0.3cm,left of=pp] (ppp) {} ;
\draw[->,thinline,draw=red] (a) .. controls +(down:4cm) and +(left:2cm) .. (ppp.west) ;
\node[node distance=0.03cm,above of=q] (qq) {} ;
\node[node distance=0.02cm,above of=s] (ss) {} ;
\draw[<-,thinline,red] (ss.south east) .. controls +(315:1cm) and +(right:1cm) .. (t);
\draw[->,thinline,red] (qq.south west) .. controls +(225:1cm) and +(left:1cm) .. (t);
\node[node distance=4cm,snake=saw,thinline,left of=m, text width=2cm] (n) {\eightpts Trigonaliser $A$} ;
\path (m)-- node[midway] (mn) {\eightpts non } (n) ;
\draw [line] (m)--(mn) ; \draw [line,->] (mn)--(n);
\node[operator,node distance=2.8cm,right of=c] (d) {\eightpts Factorisation} ;
\draw [line](c)--(d) ;
\node[object,node distance=3cm,right of=d] (f) {\eightpts multiplicité $m_i$ } ;
\draw[->,line] (d) -- (f) ;
\draw[->,thinline,draw=red] (f.south) -- (abr.north east) ;
\node[object,node distance=1cm,below of=f] (g) {\eightpts Valeur propre $\lambda_i$} ;
\draw[->,line] (d) -- (g) ;
\draw[->,thinline, draw=red] (g.south) -- (abr.north east) ;
\node[object,node distance=2.8cm,right of=m] (k) {$\!\!\!$\eightpts Dimension $n_i\!\!\!\!$} ;
\node[operator,node distance=3cm,right of=k,text width=2.4cm,text height=1.8em] (h) {\eightpts\quad Résolution de\pn$(A-\lambda_iI_n)X=0$} ;
\node[object,node distance=1.2cm,below of=h] (j) {$\!\!\!$\eightpts Espace propre $E_i\!\!\!$} ;
\draw[->,line] (h) -- (j) ;
\draw [line] (g) -- (h) ;
\draw[->,thinline] (h) -- (k) ;
\draw [line] (k) -- (m) ;
\node[object,node distance=3.5cm,left of=j] (l) {\eightpts Base $B_i$} ;
\draw[->,thinline] (h) -- (l) ;
\draw[->,line] (j) -- (l) ;
\draw[->,line] (l) -- (k) ;
\draw[->,thinline,draw=red] (l.south west) -- (abq.north east) ;
\pgfonlayer{background}
	\draw [snake=saw,line] (pp.south west) rectangle (s.north east);
	\node [forbidden sign,line width=0.5ex, draw=red,fill=white] at (mn) {\eightpts \qquad\quad} ;
\endpgfonlayer
\endtikzpicture
}%

\Subsection pourquoi, {A quoi sert la réduction ?}. 

En pratique, un problème est plus simple s'il porte sur une matrice diagonale que s'il porte sur une matrice triangulaire ou sur une matrice quelconque. 
D'une part parce qu'il comporte moins d'inconnues et d'autre part parce que les calculs sont moins lourds. 
$$
%{\tikzstyle{every node}=[rectangle,draw,text height=1.1em,text depth=0.4em,text centered]
%\tikzpicture
%\matrix [matrix of nodes] (reduction) {
%|[text width=4cm]|Matrice $A$ de taille $n$
%&|[text width=2cm]|diagonale
%&|[text width=2.5cm]|triangulaire
%&|[text width=2cm]|quelconque\\
%|[text width=4cm]|Nombre de coefficients
%&|[text width=2cm]|$n$
%&|[text width=2.5cm]|$\ds{n^2+n\F2}$
%&|[text width=2cm]|$n^2$\\
%|[text width=4cm]|Multiplications pour $A^2$
%&|[text width=2cm]|$n$
%&|[text width=2.5cm]|$\ds {n^3+3n^2+2n\F6}$
%&|[text width=2cm]|$n^3$\\
%} ;
%\endtikzpicture}
$$
La réduction permet de simplifier ou de résoudre de manière élégante des problèmes complexes faisant intervenir des multiplications de matrices (ou des compositions d'endomorphismes) tels que 
\medskip
\noindent
1) Résoudre les systèmes linéaires du type $AX=B$. \pn
2) Déterminer les racines $n^\ieme$ d'une matrice fixée $A$. \pn
3) Calculer les puissances $A^n$ d'une matrice carrée $A$ fixée. \pn
4) Résoudre les systèmes différentiels du type $X'(t)=AX(t)+B(t)$. 





\Exercice{PTSIxg}%

\Exercice{PTake}%

\Exercice{PTakf}%

\Exercice{PTakg}%









\eject


\hautspages{Olus Livius Bindus}{Déterminant}%

\Chapter Determinant, Déterminant. 


\Section Determinant, Formes multi-linéaires.


\Concept [Index=Espacesvectoriels@Espaces vectoriels!Applications multilineaires@Applications multi-linéaires] Applications multi-linéaires

\Definition [$E$ et $F$ $\ob K$-EV, $n\ge1$] 
Une application $\Phi:E^n\to F$ est $n$-linéaire $\Leftrightarrow$ elle est linéaire par rapport à chacune de ses variables, les autres étant fixées. Autrement dit, pour $(\lambda,\mu)\in\ob K^2$ et $(x,y)\in\ E^2$, on a 
$$
\forall i\in\{1,\cdots, n\}, \qquad \Phi(\Red{\cdots}, \underbrace{\lambda x+\mu y}_i, \Blue{\cdots})=\lambda\Phi(\Red{\cdots}, \underbrace{x}_i, \Blue{\cdots})+\mu\Phi(\Red{\cdots}, \underbrace{y}_i, \Blue{\cdots}) 
$$

\Exemple 
\Item{ 1. } 
L'application de $\Phi:\ob R^4\to\ob R$ avec $\Phi:(a,b,c,d)\mapsto abcd$ est une forme $4$-linéaire sur $\ob R$. 
\Item{ 2. } 
Le produit vectoriel $(\vec u,\vec v)\mapsto \vec u\wedge \vec v$ est une application $2$-linéaire sur $\ob R^3$. 
\Item{ 3. } Le produit scalaire $(\vec u,\vec v)\mapsto\vec u.\vec v$ est une forme $2$-linéaire sur $\ob R^n$. 
\Item{ 4. } Le produit mixte $(\vec u,\vec v,\vec w)\mapsto(\vec u\wedge \vec v).\vec w$ est une forme $3$-linéaire sur $\ob R^3$. 
\PAR

\Remarque : utiliser linéaire, bilinéaire et trilinéaire de préférence à $1$-linéaire, $2$-linéaire et $3$-linéaire.
\medskip

\Exercice{PTalc}%

\Exercice{PTald}%

\Concept [Index=Espacesvectoriels@Espaces vectoriels!Applicationsalternees@Applications alternées] Applications alternées


\Definition [$E$ et $F$ $\ob K$-EV, $n\ge2$] 
Une application $\Phi:E^n\to F$ est alternée $\Longleftrightarrow$ $\Phi$ s'annule chaque fois que deux de ses variables sont égales. Autrement dit, 
$$
1\le i<j\le n\sbox{ et }x=y\quad\Longrightarrow\quad \Phi(\cdots, \underbrace{x}_i, \cdots, \underbrace{y}_j,\cdots)=0. 
$$

\Exemple. Le produit vectoriel $(\vec V,\vec W)\mapsto \vec V\wedge \vec W$ est une application alternée sur $\ob R^3$. 

\Exercice{PTale}%

\Exercice{PTalf}%

\Exercice{PTalg}%

\Concept [Index=Espacesvectoriels@Espaces vectoriels!applicationsalternees@applications alternées] Applications anti-symétriques

\Definition [$E$ et $F$ $\ob K$-EV, $n\ge2$] 
Une application $\Phi:E^n\to F$ est anti-symétrique $\Longleftrightarrow$ Si l'on échange deux variables de places, le résultat est multiplié par $-1$. Autrement dit, pour $(x,y)\in\ob E^2$, on a 
$$
1\le i<j\le n\quad\Longrightarrow\quad \Phi(\cdots, \underbrace{x}_i, \cdots, \underbrace{y}_j,\cdots)=-\Phi(\cdots, \underbrace{y}_i, \cdots, \underbrace{x}_j,\cdots). 
$$

\Exemple. Le produit vectoriel $(\vec V,\vec W)\mapsto \vec V\wedge \vec W$ est une application anti-symétrique sur $\ob R^3$. 

\Propriete [$\Phi$ application $n$-linéaire, $n\ge2$]
$$
\Phi \sbox{ est alternée }\quad\Longleftrightarrow\quad \Phi\sbox{ est anti-symétrique.} 
$$

\Demonstration. Supposons que $\Phi$ soit anti-symétrique. Alors, pour $1\le i<j\le n$ et $x=y$ dans $E$, on a 
$$
\Phi(\cdots, \underbrace{x}_i, \cdots, \underbrace{y}_j,\cdots)\overbrace{=}^{anti-symetrie}-\Phi(\cdots, \underbrace{y}_i, \cdots, \underbrace{x}_j,\cdots)\overbrace{=}^{x=y}-\Phi(\cdots, \underbrace{x}_i, \cdots, \underbrace{y}_j,\cdots)
$$
de sorte que $\Phi(\cdots, x, \cdots, y,\cdots)=0$ et, par suite, l'application $\Phi$ est alternée. \pn
Réciproquement, supposons que $\Phi$ soit alternée. Alors, pour $1\le i<j\le n$ et $(x,y)\in E^2$, nous avons
$$
\eqalign{
0&=\Phi(\cdots, x+y, \cdots, x+y,\cdots)\cr
&=\underbrace{\Phi(\cdots, x, \cdots, x,\cdots)}_0+\Phi(\cdots, x, \cdots, y,\cdots)+\Phi(\cdots, y, \cdots, x,\cdots)+\underbrace{\Phi(\cdots, y, \cdots, y,\cdots)}_0
}
$$
Ainsi, nous obtenons que $\Phi(\cdots, x, \cdots, y,\cdots)=-\Phi(\cdots, y, \cdots, x,\cdots)$ et donc que $\Phi$ est anti-symétrique. 
\CQFD


\definexref{Theofond}{Théorème fondamental}{}
\Theoreme [$E$ $\ob K$-EV de dimension $n\ge1$]
L'ensemble des formes $n$-linéaires alternées sur $E$ forme une droite vectorielle. 

\Demonstration. Théorème admis. \CQFD

\Remarque : ce théorème, d'aspect anodin, est très important pour le cours : il affirme que le déterminant existe et qu'il est unique à une constante multiplicative près. 
Nous verrons une belle application pratique de ce théorème plus loin (déterminant par blocs). 

\Section Determinant, Déterminant.

\Subsection Determinant, Déterminant d'une famille de vecteurs.

\Definition [$E$ $\ob K$-EV de dimension $n\ge1$, $\sc B$ base de $E$]
Le déterminant dans la base $\sc B$, que l'on note $\det_{\sc B}$, est l'unique forme $n$-linéaire alternée de $E$ vérifiant $\det_{\sc B}(\sc B)=1$. 

\Demonstration. Comme les formes $n$-linéaire alternée de $E$ forment une droite vectorielle, il en existe au moins une, noté $\Phi$, qui n'est pas identiquement nulle. \pn
Pour l'existence, montrons dans un premier temps que $\Phi(\sc B)\neq0$ puis que l'on peut prendre 
\Equation [*] formemulti
$$
\det_{\sc B}={1\F \Phi(\sc B)}\Phi. 
$$ 
Par définition de $\Phi$, il existe $n$ vecteurs $\{f_1,\cdots, f_n\}$ de $E$ vérifiant $\Phi(f_1, \cdots, f_n)\neq0$, que l'on peut décomposer sur la base $\sc B=\{e_1, \cdots, e_n)$ pour écrire que 
$$
\forall i\in\{1,\cdots, n\}, \qquad f_i=\sum\limits_{1\le k\le n}a_{i,k}e_k.
$$
En utilisant la $n$-linéarité de $\Phi$, nous obtenons alors que 
$$
0\neq\Phi(f_1, \cdots, f_n)=\sum\limits_{(k_1,\cdots, k_n)\in\{1,\cdots, n\}^n}a_{1, k_1}\cdots a_{n,k_n}\Phi(e_{k_1}, \cdots, e_{k_n}).
$$
La forme $\Phi$ étant alternée, et donc anti-symétrique, les termes $\Phi(e_{k_1}, \cdots, e_{k_n})$ de la somme précédente sont soit nuls, soit égaux à $\pm\Phi(\sc B)$. 
En factorisant dans l'inégalité précédente, nous remaquons alors que 
$$
0\neq \Q(\sum\limits_{(k_1,\cdots, k_n)\in\{1,\cdots, n\}^n}\pm a_{1, k_1}\cdots a_{n,k_n}\W)\Phi(\sc B)
$$
A fortiori, $\Phi(\sc B)\neq0$ et l'identité \eqref{formemulti} définit bien une forme $n$-linéaire alternée de $E$ vérifiant $\det_{\sc B}(\sc B)=1$. \pn
Pour l'unicité, nous remarquons qu'une forme $n$-linéaire alternée $f$ vérifiant $f(\sc B)=1$ appartient nécéssairement à la droite vectorielle engendrée par $\Phi$. De sorte qu'il existe $c\in\ob K$ tel que $f=c\Phi$. Comme $1=f(\sc B)=c\Phi(\sc B)$, nous obtenons alors que $f=\Phi(\sc B)^{-1}\Phi=\det_{\sc B}$. 
\CQFD



\Propriete [$E$ $\ob K$-EV de dimension $n$, $\sc B$ base de $E$]
$n$ vecteurs  $\{x_1, \cdots, x_n\}$ forment une base de $E \quad\Longleftrightarrow\quad \det_{\sc B}(x_1, \cdots, x_n)\neq0$

\Demonstration. Nous procédons par double implication. \smallskip
Prouvons d'abord l'implication $(1)\Rightarrow(2)$. Pour cela, supposons que la famille $\{x_1, \cdots, x_n\}$ forme une base $\sc C$ de $E$ et montrons que $\det_{\sc B}(x_1, \cdots, x_n)\neq0$. \pn
Notant $\Phi$ une forme $n$-linéaire alternée de $E$, non nulle, il résulte des raisonnements effectués dans la démonstration précédente que $\Phi(\sc B)\neq 0\neq\Phi(\sc C)$. En reportant dans \eqref{formemulti}, nous obtenons alors que 
$$
\det_{\sc B}(x_1, \cdots, x_n)=\det_{\sc B}(\sc C)={\Phi(\sc C)\F \Phi(\sc B)}\neq0.
$$

Prouvons maintenant l'implication $(1)\Leftarrow(2)$. Pour cela, supposons que $\det_{\sc B}(x_1, \cdots, x_n)\neq0$ et montrons que $\{x_1, \cdots, x_n\}$ forme une base de $E$. 
Comme $E$ est de dimension $n$ et comme la famille comporte $n$ vecteurs, il suffit d'établir que la famille est libre. Fixons donc $(\lambda_1, \cdots,\lambda_n)\in\ob R^n$ tel que 
\Equation [**]combilinnulle
$$
\sum_{k=1}^n\lambda_kx_k=0$$ 
et établissons que $\lambda_1=\cdots=\lambda_n=0$. \smallskip
Pour chaque entier $i\in\{1, \cdots, n\}$, nous remarquons que 
$$
\lambda_i\det_{\sc B}(x_1, \cdots, x_n)=\det_{\sc B}(x_1, \cdots, x_{i-1}, \lambda_ix_i, x_{i+1}, \cdots, x_n)
$$
et nous déduisons de \eqref{combilinnulle} que $\lambda_ix_i=-\sum_{k\neq i}\lambda_kx_k$. Le déterminant étant $n$-linéaire et alterné, il suit 
$$
\eqalign{
\lambda_i\det_{\sc B}(x_1, \cdots, x_n)&=\det_{\sc B}(x_1, \cdots, x_{i-1}, -\sum_{k\neq i}\lambda_kx_k, x_{i+1}, \cdots, x_n)\cr
&=\sum_{k\neq i}\lambda_k\underbrace{\det_{\sc B}(x_1, \cdots, x_{i-1}, x_k, x_{i+1}, \cdots, x_n)}_{0}=0.} 
$$
Comme $\det_{\sc B}(x_1, \cdots, x_n)\neq 0$, nous concluons alors que $\lambda_i=0$ pour $1\le i\le n$. Nous avons donc bien prouvé l'implication $(1)\Leftarrow(2)$.\smallskip
\CQFD

\Propriete [$E$ $\ob K$-EV de dimension $n$, $\sc B$ base de $E$]
Une famille de $n$ vecteurs de $E$ est liée $\Longleftrightarrow$ son déterminant dans la base $\sc B$ est nul. 

\Demonstration. C'est la contraposée de la propriété précédente. \CQFD

\Propriete [$E$ $\ob K$-EV de dimension $n$, $\sc B$ base de $E$]
$$
\forall (x_1, \cdots, x_n)\in E^n, \qquad \forall\lambda\in\ob K, \qquad \det_{\sc B}(\lambda x_1, \cdots, \lambda x_n)=\lambda^n\det_{\sc B}(x_1, \cdots, x_n). 
$$

\Demonstration. Conséquence immédiate de la multi-linéarité de $\det_{\sc B}$. \CQFD

\Propriete [$E$ $\ob K$-EV de dimension $n$, $\sc B$ base de $E$]
On ne change pas le déterminant d'une famille de vecteurs lorsque l'on ajoute à l'un de ses vecteurs une combinaison linéaires des autres vecteurs

\Demonstration. Si nous ajoutons au vecteur $x_i$ la combinaison linéaire $\sum_{k\neq i}\lambda_kx_k$ des autres vecteurs, nous remarquons que le déterminant ne change pas
$$
\eqalign{
\det_{\sc B}(x_1, \cdots, x_n)&=\det_{\sc B}(x_1, \cdots, x_n)+\sum_{k\neq i}\lambda_k\underbrace{\det_{\sc B}(x_1, \cdots, x_{i-1}, x_k, x_{i+1}, \cdots, x_n)}_{0}\cr
&=\det_{\sc B}(x_1, \cdots, x_{i-1}, x_i+\sum_{k\neq i}\lambda_kx_k, x_{i+1}, \cdots, x_n).
}
$$
\CQFD

\Exercice{PTfl}%

\Subsection Determinant, Déterminant d'un endomorphisme.


\Definition [$E$ $\ob K$-EV de dimension $n$]
Le déterminant d'un endomorphisme $u$ de $E$ est le nombre 
$$
\det u:=\det_{\sc B}\big(u(\sc B)\big)=\det_{\sc B}\big(u(e_1), \cdots, u(e_n)\big), 
$$
où $\sc B=\{e_1, \cdots, e_n\}$ désigne une base quelconque de $E$. 

\Demonstration. Prouvons que le nombre $\det u$ ne dépend pas de la base choisie pour le calculer. 
Soient $\sc B=\{e_1, \cdots, e_n\}$ et $\sc C=\{f_1, \cdots, f_n\}$ deux bases de $E$. Alors, nous remarquons que $\det_{\sc C}$, 
$$
 \qquad \eqalign{\Phi:E^n&\to\ob K\cr(x_1, \cdots, x_n)&\mapsto\det_{\sc B}\big(u(x_1), \cdots, u(x_n)\big)}\qquad \sbox{ et }\qquad \eqalign{\Psi:E^n&\to\ob K\cr(x_1, \cdots, x_n)&\mapsto\det_{\sc C}\big(u(x_1), \cdots, u(x_n)\big)}
$$
sont trois formes $n$-linéaires alternées de $E$. Comme l'ensemble des formes $n$-linéaires alternées sur $E$ forme une droite vectorielle, 
engendrée par l'un de ses éléments non nuls tels $\det_{\sc B}$ ou $\det_{\sc C}$, nous remarquons qu'il existe trois constantes $a$, $b$ et $c$ telles que $$
\det_{\sc C}=a\det_{\sc B},\qquad \Phi=b\det_{\sc B}\qquad \sbox{ et }\qquad \Psi=c\det_{\sc C}. 
$$
En injectant $\sc B$ ou $\sc C$ dans ces relations, nous obtenons que ces constantes valent 
$$
\det_{\sc C}(\sc B)=a\qquad \det_{\sc B}\big(u(\sc B)\big)=\Phi(\sc B)=b\qquad \sbox{et}\qquad \det_{\sc C}\big(u(\sc C)\big)=\Psi(\sc C)=c. 
$$
En reportant dans les égalités précédentes, nous obtenons alors que 
$$
\det_{\sc C}=\det_{\sc C}(\sc B)\det_{\sc B},\qquad \Phi= \det_{\sc B}\big(u(\sc B)\big)\det_{\sc B}\qquad \sbox{ et }\qquad \Psi=\det_{\sc C}\big(u(\sc C)\big)\det_{\sc C}. 
$$
Nous déduisons de l'identité $ \det_{\sc C}=\det_{\sc C}(\sc B)\det_{\sc B}$ que $\Psi=\det_{\sc C}(\sc B)\Phi$ puis que 
$$
\det_{\sc C}\big(u(\sc C)\big)\det_{\sc C}(\sc B)\det_{\sc B}=\det_{\sc C}\big(u(\sc C)\big)\det_{\sc C}=\Psi=\det_{\sc C}(\sc B)\Phi=\det_{\sc C}(\sc B)\det_{\sc B}\big(u(\sc B)\big)\det_{\sc B}
$$
En identifiant de chaque coté le coefficient de $\det_{\sc B}$, nous obtenons que 
$$
\det_{\sc C}\big(u(\sc C)\big)\det_{\sc C}(\sc B)=\det_{\sc C}(\sc B)\det_{\sc B}\big(u(\sc B)\big), 
$$
puis en simplifiant par $\det_{\sc C}(\sc B)\neq 0$ que 
$$
\det_{\sc C}\big(u(\sc C)\big)=\det_{\sc B}\big(u(\sc B)\big).
$$ 
\CQFD

\Propriete [$E$ $\ob K$-EV de dimension $n$]
$$
u\in\sc L(E) \sbox{ est un automorphisme de }E\ssi \det(u)\neq0. 
$$

\Demonstration. Soit $\sc B$ une base de $E$. Alors, on a 
$$
\eqalign{
u\sbox{ est un automorphisme }&\ssi u(\sc B)\sbox{ est une base de }E\ssi \det_{\sc B}\big(u(\sc B)\big)\neq0\cr
&\ssi\det(u)\neq0.
}
$$
\CQFD


\Theoreme [$E$ $\ob K$-EV de dimension $n$, $u$ et $v$ deux endomorphismes de $E$]
$$
\det(v\circ u)=\det(v)\times \det(u). 
$$

\Demonstration. Si $u$ ou $v$ n'est pas un automorphisme, il est évident que $u\circ v$ n'en est pas un non plus et par suite que 
$$
\det(v\circ u)=0=\det(v)\times\det(u).
$$
Ce cas trivial étant traité, supposons maintenant que $u$ et $v$ sont des automorphismes. Etant donnée une base $\sc B$ de $E$, nous remarquons que $\sc C:=u(\sc B)$ est également une base de $E$. 
Comme les formes $n$-linéaires $\det_{\sc B}$ et $\det_{\sc C}$ sont alternées et non-nulles et comme l'ensemble des formes linéaires alternées forme une droite vectorielle, il existe une constante $\alpha\neq0$ telle que 
$$
\det_{\sc B}=\alpha\det_{\sc C}.
$$ 
En injectant la base $\sc C=u(\sc B)$ dans cette identité, nous obtenons alors que 
$$
\det(u)=\det_{\sc B}(\sc C)=\alpha\det_{\sc C}(\sc C)=\alpha
$$
et par suite que $\det_{\sc B}=\det(u)\det_{\sc C}$. Nous concluons alors que 
$$
\det(v\circ u)=\det_{\sc B}\big(v\circ u(\sc B)\big)=\det_{\sc B}\big(v(\sc C)\big)=\det(u)\det_{\sc C}\big(v(\sc C)\big)=\det(u)\times\det (v).
$$
\CQFD

\Exercice{PTSIuv}%


\Propriete [$E$ $\ob K$-espace vectoriel de dimension finie, $u$ automorphisme de $E$.]
$$
\det(u^{-1})=\det(u)^{-1}.
$$


\Subsection Determinant, Déterminant d'une matrice carrée. 
\medskip

\Definition [$n\ge1$]
Le déterminant d'une matrice carrée $M\in\sc M_n(\ob K)$ est le déterminant de ses vecteurs colonnes dans la base canonique de $\sc M_{n,1}(\ob K)$. 

\Remarque : Il n'est pas nécessaire de disposer d'une formule pour calculer un déterminant, les propriétés précédentes suffisent en pratique. Rappelons cependant les formules 
permettant de calculer un déterminant en dimension $n\le3$ : 
$$
\eqalign{
\det\pmatrix{a}&:=a, \cr
\Q|\matrix{a&b\cr c&d}\W|&:=ad-bc,\cr
\Q|\matrix{a_1&a_2&a_3\cr b_1&b_2&b_3\cr c_1&c_2&c_3}\W|&:=a_1b_2c_3+a_2c_1b_3+a_3b_1c_2-a_3b_2c_1-a_2b_1c_3-a_2b_3c_2.
}
$$

\Exercice{PTalh}%

\Remarque : La belle expression compacte du déterminant d'une matrice en fonction de ses coefficients n'est pas au programme en PT (elle recquiert des concepts hors-programmes). \medskip
En notant $\sc C=\{c_1, \cdots, c_n\}$ la base canonique de $\sc M_{n,1}(\ob K)$ et en utilisant la $n$-linéarité du déterminant, nous établissons cependant facilement que 
\Equation [$\dag$]formulemoche
$$
 \forall A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}, \qquad \det A=\sum_{(i_1, \cdots, i_n)\in\{1, \cdots, n\}^n}a_{i_1, 1}a_{i_2, 2}\cdots a_{i_n,n}\underbrace{\det_{\sc C}(c_{i_1}, \cdots, c_{i_n})}_{\in\{-1\ ;\ 0\ ;\ 1\}}.
$$
Cette formule (moche et compliquée) permet de traiter certains exercices simplement (l'alternative est de procéder par récurence sur la taille de la matrice, après l'avoir développé selon une ligne ou une colonne). 
\bigskip

\Exercice{PTfd}%

\Exercice{PTff}%

\Propriete [$E$ $\ob K$-EV de dimension $n\ge1$, $u\in\sc L(E)$, $\sc B$ base de $E$]
$$
\det(u)=\det\Q(\sc Mat_{\sc B}u\W).
$$

\Demonstration. Rappelons tout d'abord que $\det(u):=\det_{\sc B}\Q(u(\sc B)\W)=\det{\sc B}\Q(u(e_1), \cdots, u(e_n)\W)$. \pn
Notant $(a_{i,j})_{1\le i,j\le n}$ les coefficients de la matrice $\sc Mat_{\sc B}u$ de $u$ dans la base $\sc B$, nous remarquons que 
$$
\forall j\in\{1, \cdots, n\}, \qquad u(e_j)=\sum_{1\le i\le n}a_{i, j}e_i.
$$
Reportant dans l'expression précédente, nous déduisons de la $n$-linéarité du déterminant $\det_{\sc B}$ que 
$$
\eqalign{
\det(u)&=\det_{\sc B}\Q(\sum_{1\le i_1\le n}a_{i_1, 1}e_{i_1}, \cdots, \sum_{1\le i_n\le n}a_{i_n, n}e_{i_n}\W)\cr
&=\sum_{(i_1, \cdots, i_n)\in\{1, \cdots, n\}^n}a_{i_1, 1}a_{i_2, 2}\cdots a_{i_n,n}\det_{\sc B}(e_{i_1}, \cdots, e_{i_n}).
}
$$
Notant $\sc C=\{f_i\}_{1\le i\le n}$ la base canonique de $\sc M_{n,1}(\ob K)$, nous remarquons alors d'une part que 
$$
\underbrace{\det_{\sc B}(e_{i_1}, \cdots, e_{i_n})}_{\sbox{déterminant dans }E}=\underbrace{\det_{\sc C}(f_{i_1}, \cdots, f_{i_n})}_{\sbox{déterminant dans }\sc M_{n,1}(\ob K)}
$$
et d'autre part que 
$$
\eqalign{
\det(u)&=\sum_{(i_1, \cdots, i_n)\in\{1, \cdots, n\}^n}a_{i_1, 1}a_{i_2, 2}\cdots a_{i_n,n}\det_{\sc C}(f_{i_1}, \cdots, f_{i_n})\cr
&=\det_{\sc C}\Bigg(\underbrace{\sum_{1\le i_1\le n}a_{i_1, 1}f_{i_1}}_{\mbox{colonne $1$}}, \cdots, \underbrace{\sum_{1\le i_n\le n}a_{i_n, n}f_{i_n}}_{\mbox{colonne $n$}}\Bigg)\cr
&=\det\Q(\sc Mat_{\sc B}u\W).
}
$$
\CQFD

\Theoreme [$A$ et $B$ matrices carrées de taille $n\ge1$]
$$
\det(AB)=\det(A)\det(B).
$$

\Demonstration. Soient $u$ et $v$ les endomorphismes de $\sc M_{n,1}(\ob K)$ définis par 
$$
\eqalign{v:\sc M_{n,1}(\ob K)&\to\sc M_{n,1}(\ob K)\cr X&\mapsto AX
}\qquad \sbox{et }\qquad \eqalign{u:\sc M_{n,1}(\ob K)&\to\sc M_{n,1}(\ob K)\cr X&\mapsto BX
}
$$
Notant $\sc C$ la base canonique de l'espace des colonnes $\sc M_{n,1}(\ob K)$, nous remarquons que 
$$
\sc Mat_{\sc C}(v)=A,\qquad \sc Mat_{\sc C}(u)=B \qquad \sbox{ et }\qquad \sc Mat_{\sc C}(v\circ u)=AB.
$$
Comme le déterminant d'un endomorphisme est le même que celui de sa matrice dans n'importe qu'elle base, nous obtenons alors que 
$$
\eqalign{
\det(AB)&=\det\sc Mat_{\sc C}(v\circ u)=\det(v\circ u)=\det(v)\det(u)=\det\Q(\sc Mat_{\sc C}(v)\W)\det\Q(\sc Mat_{\sc C}(u)\W)\cr&=\det(A)\det(B).}
$$
\CQFD

\Exercice{PTali}%

\Propriete [$A$ matrice inversible de taille $n\ge1$.]
$$
\det\Q(A^{-1}\W)=\det(A)^{-1}.
$$

\Demonstration. Soit $A\in\sc Gl_n(\ob K)$. Alors $A^{-1}A=\mbox I_n$. La multiplicativité du déterminant induit alors que 
$$
1=\det(I_n)=\det(A^{-1}A)=\det(A^{-1})\det(A)
$$
\CQFD
\Concept [Index=Determinant@Déterminant@par blocs] Déterminant d'une matrice triangulaire supérieure par blocs. 

\Propriete [$A\in\sc M_p(\ob K)${,} $B\in\sc M_q(\ob K)${,} $C\in\sc M_{p,q}(\ob K)$]
$$
\det\pmatrix{A&C\cr 0& B}=\det(A)\times \det(B)
$$

\Demonstration. ({\bf preuve non exigible}) Nous remarquons que l'application
$$
\eqalign{\Phi:\sc M_p(\ob K)&\to\ob K\cr X\quad &\mapsto\det \pmatrix{X&C\cr 0& B}}
$$
est une forme $p$-linéaire alternée de $\sc M_p(\ob K)$. A fortiori, il existe une constante $\alpha\in\ob K$ telle que $\Phi=\alpha\det$, le symbôle $\det$ désignant le déterminant des matrices carrées de taille $p$. 
En injectant $\mbox{I}_p$ dans cette relation, nous obtenons alors d'une part que
$$
\det\pmatrix{\mbox I_p&C\cr 0&B}=\Phi(I_p)=\alpha\det(I_p)=\alpha
$$ 
et d'autre part que 
$$
\det\pmatrix{A&C\cr 0& B}=\Phi(A)=\alpha\det(A)=\det\pmatrix{\mbox I_p&C\cr 0&B}\det(A). 
$$
Comme le déterminant ne change pas si l'on ajoute à une colonne une combinaison linéaire des autres colonnes, nous utilisons les $p$ premières colonnes pour établir que 
$$
\det\pmatrix{\mbox I_p&C\cr 0&B}=\det\pmatrix{\mbox I_p&0\cr 0&B}
$$
Nous remarquons alors que l'application
$$
\eqalign{\Psi:\sc M_q(\ob K)&\to\ob K\cr Y\quad &\mapsto\det \pmatrix{\mbox I_p&0\cr 0& Y}}
$$
est une forme $q$-linéaire alternée de $\sc M_q(\ob K)$. A fortiori, il existe une constante $\beta\in\ob K$ telle que $\Psi=\beta\det$, le symbôle $\det$ désignant cette fois-ci le déterminant des matrices carrées de taille $q$. 
En injectant $\mbox{I}_q$ dans cette relation, nous obtenons alors d'une part que
$$
1=\det\pmatrix{\mbox I_p&0\cr 0&I_q}=\Psi(I_q)=\beta\det(I_q)=\beta
$$ 
et d'autre part que 
$$
\det\pmatrix{A&C\cr 0& B}=\det(A)\det\pmatrix{\mbox I_p&C\cr 0&B}=\det(A)\det\pmatrix{\mbox I_p&0\cr 0&B}=\det(A)\Psi(B)=\det(A)\det(B). 
$$
\CQFD

\Exercice{PTfc}%

\Propriete [$A\in\sc M_p(\ob K)$]
Le déterminant d'une matrice diagonale (resp. triangulaire supérieure ou inférieure) est égal au produit des termes sur la diagonale principale. 

\Demonstration. On calcule le déterminant par blocs. \CQFD


\Concept [Index=Determinant@Déterminant!Invariance par transposition] Invariance du déterminant par transposition. 

\Propriete [$n\ge1$]
$$
\forall A\in\sc M_n(\ob R), \qquad \det(A)=\det\Q(\NULL^tA\W).
$$

\Demonstration. {\bf Preuve hors programme en PT}. \CQFD

\Concept [Index=Determinant@Déterminant!Developpement par rapport a une colonne@Développement par rapport à une colonne] Développement par rapport à une colonne

\Propriete [$A$ matrice carrée de taille $n\ge1$]
En développant la matrice $A=(a_{i,j})_{1\le i\le n\atop1\le j\le n}$ par rapport à la colonne $j\in\{1, \cdots n\}$, nous obtenons que 
$$
\det A=\sum_{1\le i\le n}(-1)^{i+j}a_{i,j}\det \tilde A_{i, j}, 
$$
le symbôle $\tilde A_{i, j}$ désignant la matrice $A$ privée de sa $i^\ieme$ ligne et de sa $j^\ieme$ colonne. 


\Demonstration. {\bf Preuve admise en PT}. Soit $\sc F=\{f_1, \cdots, f_n\}$ la base canonique de l'espace des colonnes $\sc M_{n,1}(\ob K)$ et soient $C_1, \cdots, C_n$ les colonnes de la matrice $A$. 
En utilisant la linéarité par rapport à la $j^\ieme$ colonne, nous obtenons que 
$$
\det A=\det(C_1, \cdots, C_{j-1},\sum_{1\le i\le n}a_{i,j}f_i, C_{j+1}, \cdots, C_n)=\sum_{1\le i\le n}a_{i, j}\det(C_1, \cdots, C_{j-1},f_i, C_{j+1}, \cdots, C_n). 
$$
Comme le déterminant est anti-symétrique, nous pouvons maintenant procéder à $j-1$ échanges de colonnes ($C_1\leftrightarrow C_2, \cdots, C_{j-1}\leftrightarrow C_j$) pour obtenir que 
$$
\det(C_1, \cdots, C_{j-1},f_i, C_{j+1}, \cdots, C_n)=(-1)^{j-1}\det\det(f_i, C_1, \cdots, C_{j-1}, C_{j+1}, \cdots, C_n). 
$$
Nous procédons également à $i-1$ échanges de lignes ($L_1\leftrightarrow L_2, \cdots, L_{i-1}\leftrightarrow L_i$) pour obtenir une matrice triangulaire supérieure par blocs de déterminant égal à 
$$
\det(f_i, C_1, \cdots, C_{j-1}, C_{j+1}, \cdots, C_n)=(-1)^{i-1}\det\pmatrix{1& \mbox{nombres}\cr 0& \tilde A_{i,j}}=(-1)^{i-1}\det\tilde A_{i,j}.
$$ 
Comme $(-1)^{i+j-2}=(-1)^{i+j}$, Il résulte alors des trois estimations précédentes que 
$$
\det A=\sum_{1\le i\le n}(-1)^{i+j}a_{i,j}\det \tilde A_{i, j}.
$$
\CQFD


\Concept [Index=Determinant@Déterminant!Developpement par rapport a une ligne@Développement par rapport à une ligne] Développement par rapport à une ligne

\Propriete [$A$ matrice carrée de taille $n\ge1$]
En développant la matrice $A$ par rapport à la ligne $i\in\{1, \cdots n\}$, nous obtenons que 
$$
\det A=\sum_{1\le j\le n}(-1)^{i+j}a_{i,j}\det \tilde A_{i, j}, 
$$
le symbôle $\tilde A_{i, j}$ désignant la matrice $A$ privée de sa $i^\ieme$ ligne et de sa $j^\ieme$ colonne. 


\Demonstration. Transposer la démonstration précédente. 
\CQFD


\Concept [Index=Determinant@Déterminant!Comatrice] Comatrice. 

\Definition [$n\ge2$, $A$ matrice de $\sc M_n(\ob K)$]
La comatrice de la matrice $A$ est la matrice 
$$
\mbox{Com}(A):=\Q((-1)^{i+j}\det(\tilde A_{i, j})\W)_{1\le i,j\le n},
$$ 
le symbôle $\tilde A_{i, j}$ désignant la matrice $A$ privée de sa $i^\ieme$ ligne et de sa $j^\ieme$ colonne. 

\Propriete [$A$ matrice de $\sc M_n(\ob K)$]
Si $A$ est inversible, son inverse est la transposée de la comatrice de $A$ divisée par $\det(A)$, 
$$
A^{-1}={\NULL^t\mbox{Com}(A)\F\det(A)}
$$

\Remarque : en pratique, cette formule ne sert que pour $n=2$. 
\bigskip

\Demonstration. Supposons que $A\in\sc M_n(\ob K)$ soit inversible et montrons que $A\NULL^t\mbox{Com}(A)=\det(A)I_n$. \pn
Notant $a_{i,j}$ et $c_{i,j}$ le coefficient respectif des matrices $A$ et $C:=A\NULL^t\mbox{Com}(A)$ de la ligne $i$ et de la colonne~$j$, nous remarquons que 
$$
c_{i,j}=\sum_{1\le k\le n}a_{i,k}(-1)^{k+j}\det(\tilde A_{j,k}). 
$$
Notant $B_{i,j}$ la matrice constituée par la matrice $A$ dont on a remplacé la $j^\ieme$ ligne par la $i^\ieme$ ligne de la matrice $A$, 
nous reconnaissons la formule du développement de $\det(B_{i,j})$ suivant la $j^\ieme$ ligne et nous déduisons du caractère alterné du déterminant que 
$$
\forall (i,j)\in\{1, \cdots, n\}^2, \qquad c_{i,j}=\det(B_{i,j})=\cases{
	\det(A)& si $i=j$\cr
	0&sinon\cr}
$$
Comme le nombre $c_{i,j}$ est égal au coefficient de la $i^{\ieme}$ ligne et de la $j^\ieme$ colonne de la matrice $\det(A)I_n$, nous concluons que 
$A\NULL^t\mbox{Com}(A)=\det(A)I_n$ et par suite que 
$$
A^{-1}={\NULL^t\mbox{Com}(A)\F\det(A)}.
$$
\CQFD

\Concept [Index=Determinant@Déterminant!Formules de Cramer] Formules de Cramer. 

\Propriete [$A$ matrice inversible de $\sc M_n(\ob K)$, $B$ vecteur colonne]
L'unique solution $X=(x_1,\ldots,x_n)$ du système de Cramer $AX=B$ satisfait
$$
\forall i\in\{1, \cdots, n\}, \qquad x_i={\det(C_1, \cdots, C_{i-1}, B, C_{i+1}, \cdots, C_n)\F \det A}, 
$$
où $C_1, \cdots, C_n$ désignent les colonnes de la matrice $A$. 

\Demonstration. L'identité matricielle $AX=B$ se traduit par la relation entre colonnes
$$
\sum_{1\le k\le n}x_kC_k=B.
$$
Le déterminant étant $n$-linéaire et alterné, pour $1\le i\le n$, nous en déduisons que 
$$
\eqalign{
\det(C_1, \cdots, C_{i-1}, B, C_{i+1}, \cdots, C_n)&=\det(C_1, \cdots, C_{i-1}, \sum_{1\le k\le n}x_kC_k, C_{i+1}, \cdots, C_n)\cr
&=\sum_{1\le k\le n}x_k\underbrace{\det(C_1, \cdots, C_{i-1}, C_k, C_{i+1}, \cdots, C_n)}_{=0\sbox{ si }k\neq i}\cr &=x_i\det(C_1, \cdots, C_n)=x_i\det(A).
}
$$
\CQFD

\Remarque : vous ne vous servirez jamais de ces formules, qui nécessitent trop de calculs. 
\bigskip


\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Déterminant}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}

\hautspages{Olus Livius Bindus}{Eléments propres}%

\Chapter Elements propres, Eléments propres. 
 

\Section Elements propres, Valeurs propres. 

\Concept [Index=Applications lineaires@Applications linéaires!valeurs propres] Valeurs propres

\Definition [$E$ $\ob K$-EV, $u\in\sc L(E)$, $\lambda\in\ob K$] 
\noindent$\lambda$ est une valeur propre de $u \ssi$ il existe un vecteur $x\neq0$ de $E$ tel que $u(x)=\lambda x$. 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$, $\lambda\in\ob K$] 
$\lambda$ est une valeur propre de $M \ssi$ il existe $X\neq0$ dans $\sc M_{n,1}(\ob K)$ tel que $AX=\lambda X$. 

\Exemple. $\Red 1$ et $\Red{-1}$ sont des valeurs propres de l'endomorphisme $u:(x,y)\mapsto(y,x)$ de $\ob R^2$ car 
$$
u\big(\Blue{(1,1)}\big)=(1,1)=\Red 1.\Blue{(1,1)}\qquad\mbox{et}\qquad u\big(\Blue{(1,-1)}\big)=(-1,1)=\Red{-1}.\Blue{(1,-1)}.
$$ 

\Exemple. $\Red 0$, $\Red 1$ et $\Red 2$ sont des valeurs propres de l'endomorphisme $v:P\mapsto 2P-XP'$ de $\ob R_2[X]$ car 
$$
v\Q(\Blue{X^2}\W)=0=\Red 0.\Blue{X^2},\qquad v(\Blue X)=X=\Red 1.\Blue X\qquad \mbox{et}\qquad
v(\Blue 1)=2=\Red 2.\Blue 1.
$$

\Exemple. $\Red 2$ et $\Red{-3}$ sont des valeurs propres de la matrice $\pmatrix{0&3\cr2&-1}$ car 
$$
\pmatrix{0&3\cr2&-1}\times\Blue{\pmatrix{3\cr2}}=\pmatrix{6\cr 4}=\Red 2.\Blue{\pmatrix{3\cr 2}}\qquad\mbox{et}\qquad \pmatrix{0&3\cr2&-1}\times\Blue{\pmatrix{-1\cr1}}=\pmatrix{3\cr -3}=\Red{-3}.\Blue{\pmatrix{-1\cr1}}.
$$

\Exercice{akh}%

\Propriete [$E$ $\ob K$-EV de dimension finie, $\sc B$ base de $E$,$\lambda\in\ob K$]
$$\lambda \sbox{ est une valeur propre de } u\in\sc L(E)\ssi \lambda \sbox{ est une valeur propre de } \sc Mat_{\sc B}(u).
$$

\Application : l'endomorphisme $u:(x,y)\mapsto(y,x)$ de $\ob R^2$ possède les mêmes valeurs propres que sa matrice $A:=\pmatrix{0&1\cr1&0}$ dans la base canonique. 

\Application : l'endomorphisme $v:P\mapsto 2P-XP'$ de $\ob R_2[X]$ possède les mêmes valeurs propres que sa matrice $B:=\pmatrix{2&0&0\cr0&1&0\cr0&0&0}$ dans la base canonique.  

\Exercice{aki}%

\Definition [$E$ $\ob K$-EV, $u\in\sc L(E)$]
$$
\eqalign{\lambda\sbox{ est une valeur propre de }u&\ssi \ker(u-\lambda\Id_E)\neq\{0\}
\cr
&\ssi u-\lambda\Id_E\sbox{ n'est pas injectif.}
}
$$ 

\Demonstration. Evident.\CQFD

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
$$
\eqalign{
\lambda\sbox{ est une valeur propre de }u&\ssi u-\lambda\Id_E\sbox{ n'est pas bijectif}\cr
&\ssi\mbox{rang}(u-\lambda\Id_E)\neq\dim(E)\cr
&\ssi \det(u-\lambda\Id_E)=0
}
$$ 

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$] 
$$
\eqalign{
\lambda\sbox{ est une valeur propre de }A&\ssi A-\lambda\mbox I_n\sbox{ n'est pas inversible}\cr
&\ssi\mbox{rang}(A-\lambda\mbox I_n)\neq n\cr
&\ssi \det(A-\lambda\mbox I_n)=0
}
$$ 

\Application : $3$ est valeur propre de l'endomorphisme $u:P\mapsto P'+3P$ de $\ob R_2[X]$. En effet, \Blue{$v:=u-3\mbox{Id}$ n'est pas une bijection} d'après la relation $\Red{v(1)}=u(1)-3=\Red0$. 

\Application : $2$ est valeur propre de la matrice $A:=\pmatrix{1&2&3\cr 0&2&0\cr1&3&7}$ car 
$$
\Blue{\mbox{Rang}(A-2\mbox I_3)}=\mbox{Rang}\pmatrix{-1&2&3\cr \Red0&\Red0&\Red0\cr1&3&5}\Blue{\neq3}
$$ 

\Demonstration. Si $E$ est de dimension finie, l'endomorphisme $u-\lambda\Id_E$ est injectif $\ssi$ il est bijectif. \CQFD


\Section Elements propres, Polynôme caractéristique.



\Subsection Null, Polynôme caractéristique. 



\Concept [Index=Applications lineaires@Applications linéaires!polynôme caractéristique@polynome caracteristique] Polynôme caractéristique

\Definition [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$] 
Le polynôme caractéristique de $u$ est l'unique polynôme $P\in\ob K[X]$ vérifiant 
$$
\forall \lambda\in\ob K, \qquad P(\lambda)=\det(u-\lambda\mbox{Id}_E).
$$

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$] 
Le polynôme caractéristique de la matrice $M$ est l'unique polynôme $P\in\ob K[X]$ vérifiant 
$$
\forall \lambda\in\ob K, \qquad P(\lambda)=\det(M-\lambda\mbox{I}_n).
$$

\Demonstration. voir la preuve de la propriété un peu plus loin. \CQFD

\Exemple. $\Blue{P=}\Red{X^2+X-6}$ est le polynôme caractéristique de la matrice $A=\pmatrix{0&3\cr2&-1}$ car 
$$
\forall \lambda\in\ob R, \qquad \Blue{P(\lambda)=\det(A-\lambda\mbox I_2)=}\det\pmatrix{-\lambda&3\cr2&-1-\lambda}=\Red{\lambda^2+\lambda-6}.
$$

\Exemple. $\Blue{P=}\Red{-X^3+3X+2}$ est le polynôme caractéristique de la matrice $A=\pmatrix{0&1&1\cr1&0&1\cr1&1&0}$ car 
$$
\forall \lambda\in\ob R, \qquad \Blue{P(\lambda)=\det(A-\lambda\mbox I_3)=}\det\pmatrix{-\lambda&1&1\cr1&-\lambda&1\cr1&1&-\lambda}=\Red{-\lambda^3+3\lambda+2}.
$$

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
$$
\lambda\sbox{ est une valeur propre de }u\ssi \lambda\sbox{ est une racine du polynôme caractéristique de }u
$$ 

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$] 
$$
\lambda\sbox{ est une valeur propre de }A\Longleftrightarrow\lambda\sbox{ est une racine du polynôme caractéristique de }A
$$ 

\Demonstration. Conséquence triviale de la définition précédente. \CQFD


\Application : $\Red 2$ et $\Red{-3}$ sont les valeurs propres de $\pmatrix{0&3\cr2&-1}$ car son polynôme caractéristique est
$$
\Blue{P=X^2+X-6}=(\Red{X-2})(\Red{X+3})
$$

\Application : $\Red{-1}$ et $\Red2$ sont les valeurs propres de $\pmatrix{0&1&1\cr1&0&1\cr1&1&0}$ car son polynôme caractéristique est
$$
\Blue{P=-X^3+3X+2}=(X^2+2X+1)(2-X)=-(\Red{X+1})^2(\Red{X-2}). 
$$

\Remarque : pour calculer un polynôme caractéristique, on utilise surtout les propriétés du déterminant (forme multi-linéaire, alternée, anti-symétrique, calcul par blocks, déve\-lop\-pe\-ment par rapport aux colonnes/lignes) et la formule de Sarrus en dernier recours seulement (car il est alors très difficile de factoriser la formule développée) . 
\bigskip

Pour factoriser un polynôme caractéristique, il faudra la plupart du temps faire diminuer son degré en trouvant des racines évidentes. Pour cela, vous pouvez : 
\medskip

\noindent
1) Travailler sur la matrice $A-\lambda \mbox I_n$ : trouver une valeur de $\lambda$ pour laquelle il existe une relation de dépendance linéaire entre les lignes ou les colonnes de la matrice $A-\lambda$.
\medskip

\noindent 
2) Travailler sur le déterminant $\det(A-\lambda\mbox I_n)$ : utiliser les propriétés du déterminant pour le simplifier et factoriser des termes. 
\medskip

\noindent
3) Travailler sur le polynôme $P$ : rechercher les racines rationnelles (si c'est un polynôme à coefficients rationnels), utiliser des techniques spécifiques aux polynômes...
\bigskip
En pratique, la méthode $2$ a le meilleur taux de réussite ; la méthode $1$ donne d'assez bons résultats (sur les matrices pas trop compliquées). Par contre, la méthode $3$ a un taux de réussite médiocre (parce qu'au dela du degré $3$, on ne sait pas faire grand chose). 
 \bigskip


\Propriete [$E$ $\ob K$-EV de dimension finie $n$, $u\in\sc L(E)$]
Le polynôme caractéristique de $u$ est un polynôme de degré $n$, de terme constant $\det u$, de terme dominant $(-1)^nX^n$ et son terme de degré $n-1$ vaut $(-1)^{n-1}\mbox{Tr}(u)X^{n-1}$. 
$$
P_u=(-1)^nX^n+(-1)^{n-1}\mbox{Tr}(u)X^{n-1}+\cdots+\det(u). 
$$

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$] 
Le polynôme caractéristique de $A$ est un polynôme de degré $n$, de terme constant $\det A$, de terme dominant $(-1)^nX^n$ et son terme de degré $n-1$ est $(-1)^{n-1}\mbox{Tr}(A)X^{n-1}$. 
$$
P_A=(-1)^nX^n+(-1)^{n-1}\mbox{Tr}(A)X^{n-1}+\cdots+\det(A). 
$$

\Application : Une matrice de taille $n$ et un endomorphisme $u$ d'un espace de dimension $n$ 
possèdent au plus $n$ valeurs propres distinctes deux à deux. En effet, 
$$
\lambda\in\ob K \sbox{ est une valeur propre}\ssi\lambda\sbox{ est une racine du polynôme caractéristique}
$$

\Application : Soit $A$ une matrice carrée de taille $2$. Alors, son polynôme caractéristique est 
$$
P=X^2-\Tr(A)X+\det(A).
$$ 

\Remarque : si $E$ est un espace vectoriel sur $\ob K=\ob R$, seules les racines réelles du polynôme caractéristique d'un endomorphisme $u\in\sc L(E)$ sont des valeurs propres de $u$ et possèdent des vecteurs propres dans $E$. 
\medskip

\Remarque : L'un des grands avantages des matrices réelles est que l'on peut également les considérer comme des matrices complexes. 
De ce fait, une matrice réelle aura des valeurs propres réelles (les racines réelles de son polynôme caractéristique) que l'on pourra associér à des vecteurs propres réels ainsi que des valeurs propres complexes (les racines complexes du polynôme caractéristique) que l'on pourra associer à des vecteurs propres complexes. 
 \bigskip

\Demonstration. Nous effectuons la démonstration dans le cadre des matrices. Nous notons $c_1, \cdots, c_n$ la base canonique des colonnes de $\sc M_{n,1}(\ob K)$, nous notons respectivement $(a_{i,j})_{1\le i,j\le n}$ et $(b_{i,j})_{1\le i,j\le n}$ les coefficients des matrices $A$ et $A-\lambda\mbox I_n$ et nous remarquons que 
$$
\Q\{\eqalign{
b_{i,i}=a_{i,i}-\lambda\qquad 1\le i\le n\cr
b_{i,j}=a_{i,j}\qquad i\neq j.
}\W.
$$
Alors, il résulte de la formule \eqref{formulemoche} que l'expression
$$
P(\lambda)=\det(A-\lambda\mbox I_n)=\sum_{(i_1, \cdots, i_n)\in\{1, \cdots, n\}^n}b_{i_1, 1}b_{i_2, 2}\cdots b_{i_n,n}\underbrace{\det_{\sc C}(c_{i_1}, \cdots, c_{i_n})}_{\in\{-1\ ;\ 0\ ;\ 1\}}
$$
est une fonction polynôme de l'indeterminée $\lambda$ à coefficients dans $\ob K$, en tant que somme et produits de monômes de l'indeterminée $\lambda$. 
En particulier, la contribution à la somme des termes de la diagonale principale (cas $i_1=1, i_2=2, \cdots, i_n=n$) est 
$$
b_{1,1}\cdots b_{n,n}\det_{\sc C}(\mbox I_n)=\prod_{i=1}^n(a_{i,i}-\lambda)=(-1)^n\lambda^n+(-1)^{n-1}\lambda^{n-1}\underbrace{(a_{1,1}+\cdots+a_{n,n})}_{\mbox{Tr}(A)}+R(\lambda),
$$
où $R$ désigne un polynôme de $\ob K[X]$ de degré inférieur à $n-2$. Comme les autres termes non nuls de la somme sont constitués par des produits d'au plus obtenus $n-2$ termes de la diagonale principale 
(ils doivent se trouver chacun sur une colonne et une ligne différente), ils forment un polynôme $S\in\ob K[X]$ de l'indeterminée $\lambda$ de degré au plus $n-2$. Ainsi, nous avons 
$$
P=(-1)^nX^n+(-1)^{n-1}\mbox{Tr}(A)X^{n-1}+\underbrace{R+S}_{\in\ob K_{n-2}[X]}.
$$
Enfin, le terme constant du polynôme $P$ est $P(0)=\det(A-0.\mbox I_n)=\det(A)$. 
\CQFD

\Propriete
La somme et le produit des valeurs propres, comptées avec multiplicité, d'une matrice (resp. d'un endomorphisme) sont égaux à la trace et au déterminant de cette matrice (resp. de cet endomorphisme). 

\Subsection Determinant, Matrices semblables. 

\Definition [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$]
$$
A\sbox{ et }B\sbox{ sont semblables}\ssi\exists P\in\sc Gl_n(\ob K)\sbox{ tel que }A=P^{-1}BP. 
$$

\Propriete [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$]
$A$ et $B$ sont semblables $\ssi$ $A$ et $B$ sont les matrices d'un même endomorphisme dans des bases différentes. 

\Demonstration. Prouvons d'abord $\Q.2\W)\rightarrow\Q.1\W)$ : 
supposons qu'il existe un endomorphisme $u$ et deux bases $\sc A$ et $\sc B$ telles que $A=\sc Mat_{\sc A}(u)$ et $B=\sc Mat_{\sc B}(u)$ puis montrons que $A$ et $B$ sont semblables. \smallskip
Nous posons $P:=\sc Mat_{\sc A, \sc B}(\mbox{Id})$ et nous déduisons de la relation 
$$
\mbox I_n=\sc Mat_{\sc A, \sc A}(\mbox{Id})=\underbrace{\sc Mat_{\sc B, \sc A}(\mbox{Id})}_{P^{-1}}\times \underbrace{\sc Mat_{\sc A, \sc B}(\mbox{Id})}_P
$$
que $P^{-1}=\sc Mat_{\sc B,\sc A}(\mbox{Id})$. Alors, nous déduisons que $A$ et $B$ sont semblables de la relation  
$$
A=\sc Mat_{\sc A}(u)=\sc Mat_{\sc B, \sc A}(\mbox{Id})\times \sc Mat_{\sc B}(u)\times \sc Mat_{\sc A, \sc B}(\mbox{Id})=P^{-1}BP.
$$
Etablissons maintenant l'implication $\Q.1\W)\rightarrow\Q.2\W)$ : supposons que $A$ et $B$ sont semblables et construisons un endomorphisme $u$ et deux bases $\sc A$ et $\sc B$ tels que $A=\sc Mat_{\sc A}(u)$ et $B=\sc Mat_{\sc B}(u)$. \smallskip
Comme $A$ et $B$ sont semblables, il existe une matrice inversible $P$ telle que $A=P^{-1}BP$. \pn Nous notons $\sc B$ la base canonique de l'espace $\sc M_{n,1}(\ob K)$, nous notons $\sc A$ la base constituée par les colonnes de la matrice $P$ et nous observons que 
$$
P=\sc Mat_{\sc A, \sc B}(\mbox{Id})\qquad\sbox{et}\qquad P^{-1}=\sc Mat_{\sc B, \sc A}(\mbox{Id})
$$
Nous remarquons alors d'une part que l'application 
$$
\eqalign{u:\sc M_{n,1}(\ob K)&\to\sc M_{n,1}(\ob K)\cr X\mapsto BX}
$$
est un endomorphisme de l'espace $\sc M_{n,1}(\ob K)$ et d'autre part que la définition de $u$ implique que 
$$
\sc Mat_{\sc B}(u)=B.
$$
Nous concluons alors en remarquant que 
$$
\sc Mat_{\sc A}(u)=\sc Mat_{\sc B, \sc A}(\mbox{Id})\times \sc Mat_{\sc B}(u)\times \sc Mat_{\sc A, \sc B}(\mbox{Id})=P^{-1}BP=A.
$$
\CQFD


\Propriete
Deux matrices semblables ont même déterminant, même polynôme caractéristique, mêmes valeurs propres comptées avec multiplicité et même trace.

\Demonstration. Si $A$ et $B$ sont semblables, il existe une matrice $P\in\sc Gl_n(\ob K)$ telle que $A=P^{-1}BP$ et alors nous déduisons de la multiplicativité du déterminant que 
$$
\eqalign{\det(A)&=\det(P^{-1}BP)=\det(P^{-1})\det(B)\det(P)=\det(P^{-1})\det(P)\det(B)\cr
&=\det(\underbrace{P^{-1}P}_{\mbox I_n})\det(B)=\det(B). }
$$
Notant respectivement $P_A$ et $P_B$ les polynôme caractéristiques de $A$ et de $B$, nous avons alors 
$$
\eqalign{
\forall \lambda\in\ob R, \qquad P_A(\lambda)&=\det(A-\lambda\mbox I_n)=\det(P^{-1}BP-\lambda I_n)=\det\Q(P^{-1}(A-\lambda\mbox I_n)P\W)\cr
&=\det(P^{-1})\det(B-\lambda\mbox I_n)\det(P)=\det(B-\lambda\mbox I_n)=P_B(\lambda). 
}
$$
Le polynôme $P_A-P_B$ ayant une infinité de racines (les nombres réels), nous en déduisons que $P_A=P_B$ et donc que les valeurs propres de $A$, i.e. les racines de $P_A$, sont les mêmes que les valeurs propres de $B$, i.e. les racines de $P_B$, comptées avec multiplicité. \pn
Enfin, en décomposant l'égalité $P_A=P_B$ sur la base canonique, nous obtenons que
$$
(-X)^n+\mbox{Tr}(A)(-X)^{n-1}+\cdots+\det(A)=P_A=P_B=(-X)^n+\mbox{Tr}(B)(-X)^{n-1}+\cdots+\det(B).
$$
En identifiant les coefficients du monôme $X^{n-1}$, il suit alors que $\mbox{Tr}(A)=\mbox{Tr}(B)$. 
\CQFD




\Section Elements propres, Vecteurs propres. 




\Concept [Index=Applications lineaires@Applications linéaires!vecteurs propres] Vecteurs propres

\Definition [$E$ $\ob K$-EV, $u\in\sc L(E)$, $\lambda\in\ob K$, $x\in E$] 
Le vecteur $x\neq0$ est un vecteur propre de $u$ pour la valeur propre $\lambda \ssi$ $u(x)=\lambda x$. 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$, $\lambda\in\ob K$, $X\in\sc M_{n,1}(\ob K)$]
Le vecteur $X\neq0$ est un vecteur propre de $A$ pour la valeur propre $\lambda \ssi$ $AX=\lambda X$. 

\Exemple. $\Red{X^2}$, $\Red X$ et $\Red 1$ sont des vecteurs propres de l'endomorphisme $v:P\mapsto 2P-XP'$ de $\ob R_2[X]$ pour les valeurs propres respectives $\Blue 0$, $\Blue 1$ et $\Blue 2$ car 
$$
v\Q(\Red{X^2}\W)=0=\Blue 0.\Red{X^2},\qquad v(\Red X)=X=\Blue 1.\Red X\qquad \mbox{et}\qquad
v(\Red 1)=2=\Blue 2.\Red 1.
$$

\Exemple. $\Red{(3,2)}$ et $\Red{(-1,1)}$ sont des vecteurs propres de $\pmatrix{0&3\cr2&-1}$ pour les valeurs propres 
$\Blue 2$ et $\Blue{-3}$ car 
$$
\pmatrix{0&3\cr2&-1}\times\Red{\pmatrix{3\cr2}}=\pmatrix{6\cr 4}=\Blue 2.\Red{\pmatrix{3\cr 2}}\qquad\mbox{et}\qquad \pmatrix{0&3\cr2&-1}\times\Red{\pmatrix{-1\cr1}}=\pmatrix{3\cr -3}=\Blue{-3}.\Red{\pmatrix{-1\cr1}}.
$$

\Remarque : Pour un vecteur $x\neq0$, on a les équivalences suivantes : \pn
$x$ est un vecteur propre de $u$ pour la valeur propre $1\Longleftrightarrow x$ est invariant par~$u$. \pn
$x$ est un vecteur propre de $u$ pour la valeur propre $0\Longleftrightarrow x$ appartient au noyau de $u$. \pn
$x$ est un vecteur propre de $u$ pour la valeur propre $-1\Longleftrightarrow u$ transforme $x$ en son opposé. 
\bigskip

\Remarque : La méthode générale pour trouver quelques/tous les vecteurs propres $x$ associès à une valeur propre $\lambda$ 
consiste à résoudre l'équation vectorielle linéaire 
$$
u(x)=\lambda x \ssi u(x)-\lambda x=0 \ssi (u-\lambda\mbox{Id})(x)=0
$$
ou, dans le cas matriciel, à résoudre le système d'équations linéaires 
$$
AX=\lambda X\ssi (AX-\lambda X)=0\ssi (A-\lambda I_n)X=0. 
$$
Cela revient à déterminer le noyau de l'application $u-\lambda \mbox{Id}$ ou de la matrice $A-\lambda\mbox I_n$. 
\bigskip
\Remarque : nous exposerons plus loin quelques techniques théoriques permettant, dans certains cas, 
de résoudre le problème précédent plus rapidement et plus simplement . 
\bigskip


\Propriete [$E$ $\ob K$-EV, $u\in\sc L(E)$] 
Une famille de vecteurs propres, associés à des valeurs propres distinctes $2$ à $2$, est~libre. 

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$]
Une famille de vecteurs propres, associés à des valeurs propres distinctes $2$ à $2$, est~libre. 


\Demonstration. Soient $x_1, \cdots, x_n$ des vecteurs propres associés à des valeurs propres $\lambda_1, \cdots, \lambda_n$, distinctes deux à deux. En procédant par récurrence sur $n$, nous allons prouver la proposition 
\Equation [$\sc P_n$] VPPn
$$
\mbox{ la famille }\{x_1, \cdots, x_n\} \mbox{ est libre}
$$
$\sc P_1$ est vraie car la famille $\{x_1\}$ est libre, en tant que famille constituée d'un seul vecteur non nul. \pn
Fixons un entier $n\ge2$ tel que $\sc P_{n-1}$ soit vraie et prouvons $\sc P_n$. 
Soient $\alpha_1,\cdots, \alpha_n$ des scalaires tels que 
\Equation [$\hbox{Eq}_1$] VPEQ1
$$
0=\sum\limits_{1\le k\le n}\alpha_kx_k. 
$$
Comme le vecteur propre $x_i$ est associé à $\lambda_i$, en appliquant $u$ à cette somme, nous obtenons que 
\Equation [$\hbox{Eq}_2$] VPEQ2
$$
0=u(0)=u\Q(\sum\limits_{1\le k\le n}\alpha_kx_k\W)=\sum\limits_{1\le k\le n}\alpha_ku(x_k)=\sum\limits_{1\le k\le n}\alpha_k\lambda_kx_k 
$$
En faisant $\lambda_n\mbox{Eq}_1-\mbox{Eq}_2$, nous obtenons alors une combinaison linéaire nulle des vecteurs $x_1, \cdots, x_{n-1}$, 
$$
0=\lambda_n\sum\limits_{1\le k\le n}\alpha_kx_k-\sum\limits_{1\le k\le n}\lambda_k\alpha_kx_k=\sum\limits_{1\le k\le n-1}(\lambda_n-\lambda_k)\alpha_kx_k.
$$
Comme ces vecteurs forment une famille libre d'après $\sc P_{n-1}$, les coefficients de cette combinaison linéaire sont tous nuls, autrement dit 
$$
\alpha_k(\lambda_n-\lambda_k)=0\qquad (1\le k<n)
$$
Comme les $\lambda_k$ sont distincts deux à deux, il suit $\alpha_1=\cdots=\alpha_{n-1}=0$. Reportant ce résultat dans $\mbox{Eq}_1$, nous en déduisons enfin que $\alpha_n=0$ et par suite que la famille $\{x_1,\cdots, x_n\}$ est libre. \refn{VPPn} est donc vraie. 
\CQFD

\Propriete [$n\ge1$, $B\in\sc M_n(\ob K)$, $X\in\sc M_{n,1}(\ob K)$]
Soient $C_1, \cdots, C_n$ les vecteurs colonnes constituant la matrice $B$. Alors, on a 
$$
\forall X=\pmatrix{x_1\cr\cdots\cr x_n}\qquad AX=\pmatrix{C_1|&\cdots&|C_n}\times\pmatrix{x_1\cr\cdots\cr x_n}=x_1C_1+\cdots+x_nC_n. 
$$

\Demonstration. Faire le produit matriciel.\CQFD

\Remarque : Cette propriété est particulìèrement utile car elle permet de de déduire un vecteur propre $X$ d'une matrice $A$, 
en trouvant une relation de dépendance linéaire entre les colonnes $C_1,\cdots, C_n$ de la matrice $B=A-\lambda\mbox I_n$. 
$$
\eqalign{
X=\pmatrix{x_1\cr\cdots\cr x_n}\sbox{vecteur propre de $A$ pour }\lambda&\ssi
(A-\lambda\mbox I_n)X=0\cr\ssi BX=0&\ssi x_1C_1+\cdots+x_nC_n=0}
$$

\Application : La matrice $A$ définie par 
$$
A=\pmatrix{0&1&1&1\cr1&0&1&1\cr1&1&0&1\cr1&1&1&0}
$$
admet, pour les valeurs propres respectives $\Blue3$, $\Blue{-1}$, $\Blue{-1}$ et $\Blue{-1}$, les vecteurs propres 
$$
T_{\Blue3}:=\pmatrix{\Red1\cr\Red1\cr\Red1\cr\Red1}, \qquad U_{\Blue{-1}}:=\pmatrix{\Red1\cr\Red{-1}\cr\Red0\cr\Red0}, 
\qquad V_{\Blue{-1}}:=\pmatrix{\Red0\cr\Red1\cr\Red{-1}\cr\Red0} \quad\mbox{et}\quad W_{\Blue{-1}}:=\pmatrix{\Red0\cr\Red0\cr\Red1\cr\Red{-1}}. 
$$
En effet, on trouve des relations de dépendance linéaire entre les colonnes de la matrice $B_{\Blue3}=A-\Blue3\mbox I_4$
$$
B_{\Blue3}=A-{\Blue3}\mbox I_4=\pmatrix{-3&1&1&1\cr1&-3&1&1\cr1&1&-3&1\cr1&1&1&-3}\quad\longrightarrow\quad \Red1.C1+\Red1.C_2+\Red1.C_3+\Red1.C_4=0\quad\longrightarrow\quad T_{\Blue3}
$$
et entre les colonnes de la matrice $B_{\Blue{-1}}=A+-(\Blue{-1})\mbox I_4=A+\mbox I_4$. 
$$
B_{\Blue{-1}}=A+\mbox I_4=\pmatrix{1&1&1&1\cr1&1&1&1\cr1&1&1&1\cr1&1&1&1}\quad\longrightarrow\Q\{\eqalign{
\Red1.C_1\Red{-1}.C_2+\Red0.C_3+\Red0.C_4=0\cr
\Red0.C_1+\Red1.C_2\Red{-1}.C_3+\Red0.C_4=0\cr
\Red0.C_1+\Red0.C_2+\Red1.C_3\Red{-1}.C_4=0\cr}\W.\longrightarrow\quad \Q\{\eqalign{
U_{\Blue{-1}}\cr
V_{\Blue{-1}}\cr
W_{\Blue{-1}}\cr}\W.
$$


\Section Elements propres, Espaces propres. 




\Concept [Index=Applications lineaires@Applications linéaires!Espaces propres] Espaces propres

\Definition [$E$ $\ob K$-EV, $u\in\sc L(E)$, $\lambda$ valeur propre de $u$] 
Le sous-espace propre associé à la valeur propre $\lambda$ de $u$ est l'espace vectoriel 
$$
E_\lambda:=\ker(u-\lambda\mbox{Id}_E)=\{x\in E:u(x)=\lambda x\}.
$$
constitué du vecteur nul $0$ et des vecteurs propres de $u$ pour la valeur propre $\lambda$. 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$, $\lambda\in\ob K$]
Le sous-espace propre associé à la valeur propre $\lambda$ de $A$ est l'espace vectoriel 
$$
E_\lambda:=\{X\in \sc M_{n,1}(\ob K):AX=\lambda X\}.
$$
constitué du vecteur nul $0$ et des vecteurs propres de $A$ pour la valeur propre $\lambda$. 

\Exemple. \Red{Les espaces propres} de l'endomorphisme $u:(x,y)\mapsto(y,x)$ de $\ob R^2$ associés à $\Blue1$ et $\Blue{-1}$ sont
$$
\Red{E_{\Blue1}=\{(a,a):a\in\ob R\}}\qquad\mbox{et}\qquad \Red{E_{\Blue{-1}}=\{(b,-b):b\in\ob R\}}
$$ 
car d'une part $u(x,y)=\Blue1(x,y)\Leftrightarrow x=y$ et d'autre part $u(x,y)=\Blue{-1}(x,y)\Leftrightarrow x=-y$. 

\Exemple. \Red{Les espaces propres} de la matrice $A=\pmatrix{0&1&1\cr1&0&1\cr1&1&0}$ associés aux valeurs $\Blue{-1}$ et $\Blue2$ sont 
$$
\Red{E_{\Blue{-1}}=\mbox{Vect}\Q(\pmatrix{1\cr-1\cr0},\pmatrix{0\cr1\cr-1}\W)}\qquad\mbox{et}\qquad \Red{E_{\Blue2}=\mbox{Vect}\pmatrix{1\cr1\cr1}}
$$

\Remarque : La restriction (au départ et à l'arrivée) d'un endomorphisme $u$ à son espace propre $F$ pour la valeur propre $\lambda$ est  une homothétie de rapport $\lambda$ (en fait, c'est~$\lambda\mbox{Id}_F$). 
\bigskip

\Theoreme [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
Pour chaque valeur propre $\lambda\in\ob K$ de l'endomorphisme $u$, on a 
$$
1\le \dim E_\lambda\le m_\lambda
$$
où $m_\lambda$ désigne la multiplicité de $\lambda$ en tant que racine du polynôme caractéristique de $u$. 

\Invertedtrue
\Theoreme [$n\ge1$, $A\in\sc M_n(\ob K)$]
Pour chaque valeur propre $\lambda\in\ob K$ de la matrice $A$, on a 
$$
1\le \dim E_\lambda\le m_\lambda
$$
où $m_\lambda$ désigne la multiplicité de $\lambda$ en tant que racine du polynôme caractéristique de $A$. 

\Demonstration. Comme $\lambda$ est une valeur propre de $u$, il existe un vecteur $x\neq0$ de $E$ vérifiant $u(x)=\lambda x$. Comme l'espace vectoriel $E_\lambda=\ker(u-\lambda\mbox{Id}_E)$ contient au moins deux éléments $0$ et $x$, il est forcément de dimension $\dim(E_\lambda)\ge1$. \pn
Nous admettons l'inégalité $\dim(E_\lambda)\le m_\lambda$ : la preuve nécessite des concepts hors programme. 
\CQFD

\Propriete [$E$ $\ob K$-EV, $u\in\sc L(E)$] 
Les espaces propres de $u$ pour des valeurs propres distinctes $2$ à $2$ sont en somme directe.  

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$] 
Les espaces propres de $A$ pour des valeurs propres distinctes $2$ à $2$ sont en somme directe.  \pn

\Demonstration. Soient $\lambda_1,\cdots, \lambda_n$ des valeurs propres de $u$ et soient $E_1, \cdots, E_n$ leurs espaces propres respectivements associés. Soient $(x_1, \cdots, x_n)\in E_1\times\cdots\times E_n$ tels que 
$$
x_1+x_2+\cdots+x_n=0.
$$
S'il existait des termes non-nuls dans la somme précédente, ils seraient liées par une relation de dépendance linéaire. Mais ils constitueraient également une famille de vecteurs propres, associés à des valeurs propres distinctes deux-à-deux, qui serait donc libre. 
Comme une famille ne peut être à la fois liée et libre, cette éventualité ne peut pas se produire et, par conséquent, $x_1=x_2=\cdots=x_n=0$. 
En conclusion, les espaces propres $E_1, \cdots, E_n$ forment une somme directe $E_{\lambda_1}\oplus E_{\lambda_2}\oplus\cdots E_{\lambda_n}$. 
\CQFD


\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
La somme des dimensions des sous-espaces propres de $u$ est inférieure ou égale à la dimension de $E$.

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$]
La somme des dimensions des sous-espaces propres de $A$ est inférieure ou égale à la dimension de $E$. \pn

\Demonstration. Soient $\lambda_1, \cdots, \lambda_k$ les valeurs propres de $u$. Alors, les espaces propres $E_{\lambda_1},\cdots, E_{\lambda_k}$ sont en somme directe et vérifient 
$$
E_{\lambda_1}\oplus E_{\lambda_2}\oplus\cdots\oplus E_{\lambda_k}\subset E.
$$
En calculant la dimension de ces espaces vectoriels, nous obtenons alors que 
$$
\dim(E_{\lambda_1})+\dim(E_{\lambda_2})+\cdots+\dim( E_{\lambda_k})=\dim(E_{\lambda_1}\oplus E_{\lambda_2}\oplus\cdots\oplus E_{\lambda_k})\le \dim(E)
$$\CQFD


\Propriete [$E$ $\ob K$-EV, $u$ et $v$ endomorphismes de $E$] 
Si $u$ et $v$ commuttent, alors les espaces propres de $u$ sont stables par $v$ et réciproquement.
$$
u\circ v=v\circ u\qquad \Longrightarrow\qquad \Q\{\eqalign{\forall F\sbox{ espace propre de $u$}, \quad v(F)\subset F\cr
\forall G\sbox{ espace propre de $v$}, \quad u(G)\subset G
}\W.
$$

\Invertedtrue
\Propriete [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$] 
Si $A$ et $B$ commuttent, alors les espaces propres de $A$ sont stables par $B$ et réciproquement.
$$
AB=BA\qquad \Longrightarrow\qquad \Q\{\eqalign{\forall F\sbox{ espace propre de $A$}, \quad &B.F\subset F\cr
\forall G\sbox{ espace propre de $B$}, \quad &A.G\subset G
}\W.
$$

\Demonstration. Soit $F$ un espace propre de $u$ pour la valeur $\lambda$. Pour $x\in F$, le vecteur $y=v(x)$ vérifie 
$$
u(y)=u\circ v(x)=v\circ u(x)=v(\lambda x)=\lambda v(x)=\lambda y, 
$$
et appartient de ce fait à l'espace $F$. En conclusion, $v(F)\subset F$. \CQFD

\Remarque : cette propriété permet de mieux comprendre et de mieux analyser les matrices. 
\bigskip

\Propriete [$E$ $\ob K$-EV, $u$ et $v$ endomorphismes de $E$, $P$ et $Q$ polynômes de {$\ob KX$} ] 
Si $u$ commute avec $v$, alors $P(u)$ commute avec $Q(v)$. 

\Invertedtrue
\Propriete [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$, $P$ et $Q$ polynômes de {$\ob K[X]$} ] 
Si $A$ commute avec $B$, alors $P(A)$ commute avec $Q(B)$. \pn

\Demonstration. Soient $P$ et $Q$ deux polynômes de $\ob K[X]$. 
Comme $u$ commute avec $v$, alors $u$ commute avec toute puissance de $v$ 
car l'associativité de la loi de composition nous permet d'écrire que 
$$
\eqalign{
\forall n\ge0, \qquad u\circ v^n&=u\circ \underbrace{v\circ\cdots\circ v}_n=(u\circ v)\circ \underbrace{v\circ \cdots\circ v}_{n-1}=(v\circ u)\circ \underbrace{v\circ \cdots\circ v}_{n-1}\cr
&=v\circ (u\circ v)\circ \underbrace{v\circ \cdots\circ v}_{n-2}=v\circ (v\circ u)\circ \underbrace{v\circ \cdots\circ v}_{n-2}=\cdots=\underbrace{v\circ\cdots\circ v}_n\circ u=v^n\circ u. }
$$
Posant $P=\sum_{k=0}^na_kX^k$, nous remarquons alors que $u$ commute avec $P(v)$ car  
$$
u\circ P(v)=u\circ \sum\limits_{0\le k\le n}a_kv^k=\sum\limits_{0\le k\le n}a_ku\circ v^k=\sum\limits_{0\le k\le n}a_kv^k\circ u=\Q(\sum\limits_{0\le k\le n}a_kv^k\W)\circ u=P(v)\circ u
$$
Comme $w=P(v)$ commute avec $u$, nous montrons alors de même que $w=P(v)$ commute avec $Q(u)$. 
\CQFD
%\vfill
%\eject



\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\ValeursPropres\VecteursPropres\PolynômesCaractéristiques}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}







\hautspages{Olus Livius Bindus}{Diagonalisation}%

\Chapter Diagonalisation, Diagonalisation. 


\Section Null, Endomorphismes et matrices diagonalisables.
%\sidx{Applications lineaires@Applications linéaires!endomorphisme diagonalisable}

\Definition [$n\ge1$, $A$ $B$ deux matrices de $\sc M_n(\ob K)$]
$$
\mbox{$A$ et $B$ sont semblables sur $\ob K$}\ssi\mbox{ Il existe $P\in\sc Gl_n(\ob K)$ telle que $A=PBP^{-1}$}. 
$$

\Remarque : deux matrices sont semblables si, et seulement si ce sont les matrices du même endomorphismes dans des bases différentes. 
\bigskip


\Definition [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$] 
$$
\eqalign{
u \sbox{ est diagonalisable} &\ssi \sbox{Il existe une base de vecteurs propres de $u$}\cr 
\ssi&\sbox{il existe une base $\sc B$ de $E$ dans laquelle $\sc Mat_{\sc B}(u)$ est diagonale }
}
$$ 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$]
$$
\eqalign{
A \sbox{ est diagonalisable} &\ssi \sbox{Il existe une base de vecteurs propres de $A$ }. 
\cr
&\ssi A \sbox{ est semblable à une matrice diagonale}
}
$$ 

\Exercice{alj}%

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$, $\sc B$ base de $E$] 
$$
u \sbox{ est diagonalisable} \ssi \sc Mat_{\sc B}(u)\sbox{ est diagonalisable.} 
$$ 

\Demonstration. Conséquence immédiate des définitions précédentes. \CQFD

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
Si le polynôme caractéristique de $u$ est scindé sur $\ob K$ et admet $n$ racines distinctes deux à deux, alors $u$ est diagonalisable. 

\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$]
Si le polynôme caractéristique de $A$ est scindé sur $\ob K$ et admet $n$ racines distinctes deux à deux, alors $A$ est diagonalisable. \pn

\Demonstration. Soient $(\lambda_1, \cdots,\lambda_n)$ les racines du polynôme caractéristique $P$ de l'endomorphisme $u$, qui est scindé et de degré $n$. Alors, pour chaque entier $k\in\{1,\cdots, n\}$, le nombre $\lambda_k$ est une valeur propre de $u$ de sorte qu'il existe un vecteur propre $x_k\neq0$ de $u$ associé à la valeur $\lambda_k$. Comme la famille $\{x_1, \cdots, x_n\}$ est une famille de vecteurs propres de $u$ associés à des valeurs propres distinctes deux à deux, c'est une famille libre. A fortiori, c'est une base car c'est une famille libre de $n$ vecteurs de $E$, espace vectoriel de dimension~$n$. Comme $\{x_1, \cdots, x_n\}$ est une base de $E$, constituée de vecteurs propres de $u$, l'endomorphisme $u$ est diagonalisable. \CQFD

\Concept [Index=Applicationslineaires@Applications linéaires!Caractérisation des endomorphismes diagonalisables@Caracterisation des endomorphismes diagonalisables] Première caractérisation des endomorphismes diagonalisables


\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
$u$ est diagonalisable $\Leftrightarrow$ les racines du polynôme caractéristique $P$ de $u$ sont toutes dans~$\ob K$ ($P$~est scindé sur $\ob K$) et la dimension de chaque espace propre $E_{\lambda_k}$ est égal à~$n_k$, la~mul\-ti\-pli\-ci\-té en tant que racine de $P$ de la valeur propre $\lambda_k$ correspondante. 
$$
u\sbox{ diagonalisable}\ssi\Q\{\eqalign{P(\lambda)=(-1)^n\prod_{1\le k\le p}\Big(\lambda-\overbrace{\lambda_k}^{\in\ob K}\Big)^{n_k}\cr n_k=\dim(E_{\lambda_k})\ \sbox{ pour }\ 1\le k\le p}\W.
$$

\Invertedtrue
\Propriete [$n\ge1$ $A\in\sc M_n(\ob K)$]
$A$ est diagonalisable sur $\ob K$ $\Leftrightarrow$ les racines du polynôme caractéristique $P$ de $A$ sont dans~$\ob K$ ($P$~est scindé sur $\ob K$) et la dimension de chaque espace propre $E_{\lambda_k}$ est égal à~$n_k$, la~mul\-ti\-pli\-ci\-té en tant que racine de $P$ de la valeur propre $\lambda_k$ correspondante. 
$$
A\sbox{ diagonalisable sur }\ob K\ssi\Q\{\eqalign{P(\lambda)=(-1)^n\prod_{1\le k\le p}\Big(\lambda-\overbrace{\lambda_k}^{\in\ob K}\Big)^{n_k}\cr n_k=\dim(E_{\lambda_k})\ \sbox{ pour }\ 1\le k\le p}\W.
$$

\Remarque : lorsque l'on diagonalise, il faut faire attention au corps sur lequel on travaille. Ainsi, certaines matrices réelles sont diagonalisables sur $\ob C$ mais ne le sont pas sur $\ob R$, comme par exemple les matrices de rotations : 
$$
\pmatrix{\cos\theta&-\sin\theta\cr\sin\theta&\cos\theta}\sbox{ est semblable à }\pmatrix{\e^{i\theta}&0\cr0&\e^{-i\theta}}\sbox{ dans $\sc M_2(\ob C)$ mais pas dans $\sc M_2(\ob R)$}.
$$
Le grande force des matrices réelles par rapport aux endomorphismes réels est que l'on peut les étudier à la fois sur $\ob R$ et sur $\ob C$. 
\bigskip

\Remarque : Lorsque un endomorphisme/une matrice est diagonalisable, on obtient une base de diagonalisation par réunion des bases de chacun des sous-espaces propres. 
\bigskip


\Concept [Index=Applications lineaires@Applications linéaires!Caractérisation des endomorphismes diagonalisables@Caractérisation des endomorphismes diagonalisables] Seconde caractérisation des endomorphismes diagonalisables

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$] 
$$
u \sbox{ est diagonalisable} \ssi\eqalign{&\sbox{Il existe un polynôme $Q\neq0$ vérifiant $Q(u)=0$}\cr 
&\sbox{et dont toutes les racines sont simples et dans $\ob K$}}
$$ 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$]
$$
A \sbox{ est diagonalisable} \ssi\eqalign{&\sbox{Il existe un polynôme $Q\neq0$ vérifiant $Q(A)=0$}\cr 
&\sbox{et dont toutes les racines sont simples et dans $\ob K$}}
$$ 


\Concept [Index=Applications lineaires@Applications linéaires!Diagonalisation simultanée] Diagonalisation simultanée

\Propriete [$E$ $\ob K$-EV de dimension finie, $u$ et $v$ endomorphismes de $E$]
Si $u$ et $v$ sont diagonalisables et commuttent, on peut les diagonaliser dans la même base~:\pn
Il existe une base $\sc B$, constitués de vecteurs à la fois propres pour $u$ et pour $v$ et alors 
$$
\sc Mat_{\sc B}(u)\sbox{ et }\sc Mat_{\sc B}(v)\sbox{ sont diagonales}
$$

\Propriete [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$]
Si $A$ et $B$ sont diagonalisables et commuttent, on peut les diagonaliser simultanément : \pn
Il existe des matrices $D_1$ et $D_2$ diagonales et $P\in\sc Gl_n(\ob K)$ telles que 
$$
A=PD_1P^{-1}\qquad\sbox{ et }\qquad B=PD_2P^{-1}
$$ 

\Demonstration. Se fait par récurence sur la dimension de $E$.\CQFD

\Section Null, Algorithme de diagonalisation. 

\medskip
\Concept [Title=Algorithme de diagonalisation] Pour Diagonaliser une matrice diagonalisable

\noindent
{\bf Etape 1 : }Ecrire la relation $P(\lambda)=\det(A-\lambda\mbox{I}_n)$, en dessinant la matrice. \medskip\noindent
{Au besoin, chercher des racines évidentes $\lambda$ de $P$ (méthode standard) et trouver des relations de dépendance linéaire entre les colonnes (méthode $\star$), pour ces valeurs de $\lambda$. }
\medskip
\noindent
{\bf Etape 2 : }Calculer et factoriser $P$. En déduire les valeurs propres $\lambda$ et leur multiplicité~$m_\lambda$.\medskip\noindent
{\it Méthode $\star$ : ajouter à une colonne une combinaison linéaire des autres colonnes, conformément aux relations de dépendances linéaires précédemment trouvées, pour factoriser plus rapidement. }
\medskip
\noindent
{\bf Etape 3 : }Pour chaque valeur propre $\lambda$, résoudre le système $(A-\lambda\mbox{I}_n)X=0$. En déduire une base~$\sc B_\lambda$ (comportant $m_\lambda$ vecteurs) de l'espace vectoriel des solutions. \medskip\noindent
{\it Méthode $\star$, A chaque relation de dépendance entre colonnes précédemment trouvée, correspond une solution (un vecteur propre non nul)}
\medskip
\noindent
{\bf Etape 4 : } Ecrire la relation $A=PDP^{-1}$ ainsi que la matrice diagonale $D$ dont les coefficients sont les valeurs propres trouvées. \medskip\noindent
{\it Si vous avez des valeurs propres $\lambda$, $\mu$ et $\gamma$ de multiplicité respective $3$, $1$ et $2$, votre matrice $D$ sera }
$$
D=\pmatrix{\lambda&&&&&\cr
&\lambda&&&&\cr
&&\lambda&&&\cr
&&&\mu&&\cr
&&&&\gamma&\cr
&&&&&\gamma}
$$
Ecrire la matrice $P$ dont les colonnes sont les vecteurs des bases $\sc B_ \lambda$, en respectant les contraintes : \pn
{\bf a) chaque vecteur n'apparait qu'une fois dans la matrice $P$. \pn b) Sur une même colonne de $P$ et de $D$ se trouvent un vecteur propre et sa valeur propre correspondante. }\medskip\noindent
{\it Reprenant l'exemple précédant, Si vous obtenez $\sc B_\lambda=\{u_1,u_2,u_3\}$, $\sc B_\mu=\{v\}$ et $\sc B_\gamma=\{w_1,v_2\}$, votre matrice $P$ sera la matrice dont les colonnes sont }
$$
P=\pmatrix{u_1,u_2,u_3,v,w_1,w_2}\qquad\mbox{\rm ou également par exemple }\qquad P=\pmatrix{u_3,u_1,u_2,v,w_2,w_1}
$$
{\bf Etape 5 : }Au besoin, inverser la matrice $P$ pour obtenir la matrice $P^{-1}$. \pn
{\it on évitera cette étape lorsque cela est possible car elle est lourde en calculs}. 

\noindent
\centerline{%
\font\SvgText=cmr8\relax
\tikzstyle{every node}=[inner sep=2pt]
\tikzpicture
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\node [object] (a) {Matrice $A$} ;
\node [operator,right of=a, node distance=2.7cm] (b) {\eightpts$\det(A-\lambda I_n)$} ;
\draw [line] (a)--(b) ; 
\node[object, right of=b,node distance=3cm] (c) {\eightpts Polynome $P_A$} ;
\draw[line,->] (b) -- (c) ;
\node[fork,node distance=2cm,below of=c] (m) {\eightpts$\forall i:m_i=n_i$} ;
\node[below of=m,node distance=2.2cm] (prep) {};
\path (m)-- node[midway] (mo) {oui} (prep) ;
\draw [line] (m)--(mo) ; \draw [line,->] (mo)--(prep);
\node[below of=prep,node distance=0.2cm] (p) {$=$} ;
\node[right of=p] (q) {$P$} ;
\node[above of=q,node distance=0.3cm] (abq) {};
\node[right of=q] (r) {$D$} ;
\node[above of=r,node distance=0.3cm] (abr) {};
\node[operator,node distance=1cm,font=\texttt,below of=r] (t) {\eightpts Inversion} ;
\node[right of=r] (s) {$P^{-1}$} ;
\node[left of=p,label=180:{\qquad}] (pp) {$A$} ;
\node[node distance=0.3cm,left of=pp] (ppp) {} ;
\draw[->,thinline,draw=red] (a) .. controls +(down:4cm) and +(left:2cm) .. (ppp.west) ;
\node[node distance=0.03cm,above of=q] (qq) {} ;
\node[node distance=0.02cm,above of=s] (ss) {} ;
\draw[<-,thinline,red] (ss.south east) .. controls +(315:1cm) and +(right:1cm) .. (t);
\draw[->,thinline,red] (qq.south west) .. controls +(225:1cm) and +(left:1cm) .. (t);
\node[node distance=4cm,snake=saw,thinline,left of=m, text width=2cm] (n) {\eightpts Trigonaliser $A$} ;
\path (m)-- node[midway] (mn) {\eightpts non } (n) ;
\draw [line] (m)--(mn) ; \draw [line,->] (mn)--(n);
\node[operator,node distance=2.8cm,right of=c] (d) {\eightpts Factorisation} ;
\draw [line](c)--(d) ;
\node[object,node distance=3cm,right of=d] (f) {\eightpts multiplicité $m_i$ } ;
\draw[->,line] (d) -- (f) ;
\draw[->,thinline,draw=red] (f.south) -- (abr.north east) ;
\node[object,node distance=1cm,below of=f] (g) {\eightpts Valeur propre $\lambda_i$} ;
\draw[->,line] (d) -- (g) ;
\draw[->,thinline, draw=red] (g.south) -- (abr.north east) ;
\node[object,node distance=2.8cm,right of=m] (k) {$\!\!\!$\eightpts Dimension $n_i\!\!\!\!$} ;
\node[operator,node distance=3cm,right of=k,text width=2.4cm,text height=1.8em] (h) {\eightpts\quad Résolution de\pn$(A-\lambda_iI_n)X=0$} ;
\node[object,node distance=1.2cm,below of=h] (j) {$\!\!\!$\eightpts Espace propre $E_i\!\!\!$} ;
\draw[->,line] (h) -- (j) ;
\draw [line] (g) -- (h) ;
\draw[->,thinline] (h) -- (k) ;
\draw [line] (k) -- (m) ;
\node[object,node distance=3.5cm,left of=j] (l) {\eightpts Base $B_i$} ;
\draw[->,thinline] (h) -- (l) ;
\draw[->,line] (j) -- (l) ;
\draw[->,line] (l) -- (k) ;
\draw[->,thinline,draw=red] (l.south west) -- (abq.north east) ;
\pgfonlayer{background}
	\draw [snake=saw,line] (pp.south west) rectangle (s.north east);
	\node [forbidden sign,line width=0.5ex, draw=red,fill=white] at (mn) {\eightpts \qquad\quad} ;
\endpgfonlayer
\endtikzpicture
}%
\Figure [Index=Algorithme!dediagonalisation@de diagonalisation] Algorithme de diagonalisation.

\Section Null, Homothéties, projections, symétries. 


\Concept [Index=Applications lineaires@Applications linéaires!Homothéties@Homotheties] Homothéties


\Propriete [$E$ $\ob K$-espace vectoriel de dimension finie $n\ge1$]
La matrice d'une homothétie de rapport $\lambda\in\ob K$ dans une base de $E$ est la matrice $\lambda\mbox I_n$. 
En particulier, une homothétie de rapport $\lambda$ est diagonalisable : \pn $\lambda$ est son unique valeur propre et son espace propre associé est $E_\lambda=E$. 

\Demonstration. Soit $u$ une homothétie de rapport $\lambda\in\ob K$ et soit $\sc B:=\{e_1, \cdots, e_n\}$ une base de $E$. Alors, 
$$
\forall k\in\{1, \cdots, n\}, \qquad u(x_k)=\lambda x_k.
$$
En écrivant la matrice de $u$ dans la base $\sc B$, nous obtenons alors que $\sc Mat_{\sc B}(u)=\lambda\mbox I_n$. Comme cette matrice admet $P=(\lambda-X)^n$ comme polynôme caractéristique, $\lambda$ est l'unique valeur propre de $u$. \CQFD

\Concept [Index=Applications lineaires@Applications linéaires!Projections] Projections


\Propriete [$E$ $\ob K$-espace vectoriel de dimension finie $n\ge1$]
La matrice d'une projection $u$ de rang $k$ dans une base $\sc B$ de $E$ constituée par la réunion d'une base de $\IM(u)$ et d'une base de $\ker(u)$ est 
$$
\sc Mat_{\sc B}=\pmatrix{\mbox I_k&0\cr0&0}.
$$ 
En particulier, une telle projection est diagonalisable et ses valeurs propres sont : \pn $\lambda=1$ pour la multiplicité $m_1=k$ et l'espace propre $E_1=\IM(u)$ \pn$\lambda=0$ pour la multiplicité $m_0=n-k$ et l'espace propre $E_0=\ker(u)$. 
\bigskip

\Demonstration. Soit $u$ une projection de rang $k$. Alors, $u$ satisfait la relation 
$$
E=\ker(u)\oplus \IM(u)
$$
et nous en déduisons que $\IM(u)$ et $\ker(u)$ sont respectivement de dimension $k$ et $n-k$. Etant données une base $\{e_1, \cdots, e_k\}$ de $\IM(u)$ et une base $\{e_{k+1},\cdots, e_n\}$ de $\ker(u)$, nous remarquons de plus que leur réunion $\sc B=\{e_1, \cdots, e_n\}$ constitue une base de $E$ et que 
$$
u(e_\ell)=\cases{e_k& si $1\le \ell\le k$\cr 0& sinon\cr}
$$
A fortiori, la matrice de la projection $u$ dans la base $\sc B$ est 
$$
\sc Mat_{\sc B}=\pmatrix{\mbox I_k&0\cr0&0}.
$$ 
Comme le polynôme caractéristique de cette matrice est $P=(1-X)^k(-X)^{n-k}$, les valeurs propres de $u$ sont $1$ pour la multiplicité $m_1=k$ et $0$ pour la multiplicité $m_0=n-k$. Enfin, les espaces propres associés aux valeurs propres $1$ et $0$ sont $E_1:=\ker(u-\mbox{Id}_E)=\IM(u)$ et $E_0=\ker(u)$. 
\CQFD


\Concept [Index=Applications lineaires@Applications linéaires!Symétries@Symetries] Symétries


\Propriete [$E$ $\ob K$-espace vectoriel de dimension finie $n\ge1$]
La matrice d'une symétrie $u$ par rapport à $F$ parallélement à $G$ dans une base $\sc B$ de $E$ constituée par la réunion d'une base de $F$ et d'une base de $G$ est 
$$
\sc Mat_{\sc B}=\pmatrix{\mbox I_k&0\cr0&-I_{n-k}}\qquad\sbox{où}\qquad k:=\dim F
$$ 
En particulier, une telle symétrie est diagonalisable et ses valeurs propres sont : \pn $\lambda=1$ pour la multiplicité $m_1=k$ et l'espace propre $E_1=F$ \pn$\lambda=0$ pour la multiplicité $m_0=n-k$ et l'espace propre $E_0=G$. 
\bigskip

\Demonstration. Soit $u$ une symétrie par rapport à $F$ paralllèlement à $G$. Alors, $u$ satisfait les relations 
$$
E=\ker(u-\mbox{Id}_E)\oplus\ker(u+\mbox{Id}_E), \qquad F=\ker(u-\mbox{Id}_E)\quad\mbox{et}\quad G=\ker(u+\mbox{Id}_E).
$$
 Etant données une base $\{e_1, \cdots, e_k\}$ de $F$ et une base $\{e_{k+1},\cdots, e_n\}$ de $G$, nous remarquons de plus que leur réunion $\sc B=\{e_1, \cdots, e_n\}$ constitue une base de $E$ et que 
$$
u(e_\ell)=\cases{e_k& si $1\le \ell\le k$\cr 
-e_k& sinon}
$$
A fortiori, la matrice de la symétrie $u$ dans la base $\sc B$ est 
$$
\sc Mat_{\sc B}=\pmatrix{\mbox I_k&0\cr0&-I_{n-k}}.
$$ 
Comme le polynôme caractéristique de cette matrice est $P=(1-X)^k(-1-X)^{n-k}$, les valeurs propres de $u$ sont $1$ pour la multiplicité $m_1=k$ et $-1$ pour la multiplicité $m_{-1}=n-k$. Enfin, les espaces propres associés aux valeurs propres $1$ et $0$ sont $E_1:=\ker(u-\mbox{Id}_E)=F$ et $E_{-1}=\ker(u+\mbox{Id}_E)=G$. 
\CQFD

\hautspages{Olus Livius Bindus}{Trigonalisation}%

\Chapter Trigonalisation, Trigonalisation. 



\Definition [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$] 
$$
u \sbox{ est trigonalisable} \ssi\eqalign{&\mbox{il existe une base $\sc B$ de $E$ dans laquelle}\cr&\mbox{$\sc Mat_{\sc B}(u)$ est triangulaire supérieure}}
$$ 

\Invertedtrue
\Definition [$n\ge1$, $A\in\sc M_n(\ob K)$]
$$
\eqalign{
A \sbox{ est diagonalisable} &\ssi \sbox{Il existe une base de vecteurs propres de $A$ }. 
\cr
&\ssi A \sbox{ est semblable à une matrice diagonale}
}
$$ 


\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$, $\sc B$ base de $E$] 
$$
u \sbox{ est trigonalisable} \ssi \sc Mat_{\sc B}(u)\sbox{ est trigonalisable.} 
$$ 

\Demonstration. Conséquence immédiate des définitions précédentes. \CQFD

\Concept [Index=Applications lineaires@Applications linéaires!Caractérisation des endomorphismes trigonalisables@Caractérisation des endomorphismes trigonalisables] Caractérisation des endomorphismes trigonalisables

\Propriete [$E$ $\ob K$-EV de dimension finie, $u\in\sc L(E)$]
$$
\sbox{$u$ est trigonalisable }\ssi\sbox{ le polynôme caractéristique de $u$ est scindé sur $\ob K$}
$$
 
\Invertedtrue
\Propriete [$n\ge1$, $A\in\sc M_n(\ob K)$]
$$
\sbox{$A$ est trigonalisable }\ssi\sbox{ le polynôme caractéristique de $A$ est scindé sur $\ob K$}
$$

\Demonstration. Admise (nécéssite des concepts hors programme)\CQFD

\Propriete
Toutes les matrices sont trigonalisables sur $\ob C$. \pn
Les endomorphismes d'un $\ob C$-espace vectoriel de dimension finie sont tous trigonalisables.

\Demonstration. Conséquence immédiate du théorème de D'Alembert-Gauss et de la caractérisation précédente.\CQFD 


\Propriete [$E$ $\ob K$-EV de dimension finie, $u$ et $v$ endomorphismes de $E$]
Si $u$ et $v$ sont trigonalisables et commuttent, ils sont trigonalisables dans la même base : \pn
Il existe une base $\sc B$ pour laquelle $\sc Mat_{\sc B}(u)$ et $\sc Mat_{\sc B}(v)$ sont triangulaires supérieures. 



\Propriete [$n\ge1$, $A$ et $B$ matrices de $\sc M_n(\ob K)$]
Si $A$ et $B$ sont trigonalisables et commuttent, on peut les trigonaliser simultanément  : \pn
Il existe des matrices $T_1$ et $T_2$ triangulaires supérieures et $P\in\sc Gl_n(\ob K)$ telles que 
$$
A=PT_1P^{-1}\qquad\sbox{ et }\qquad B=PT_2P^{-1}
$$ 

\Demonstration. Se fait par récurence sur la dimension de $E$, on la fera probablement en DM.\CQFD







\Section Null, Algorithme de trigonalisation. 

\medskip
\Concept [Title=Algorithme de trigonalisation] Pour Trigonaliser une matrice


On procède presque comme pour la diagonalisation : seules les étapes 3 et 4 sont légèrement différentes. 
\medskip
\noindent
{\bf Etape 1 : }Ecrire la relation $P(\lambda)=\det(A-\lambda\mbox{I}_n)$, en dessinant la matrice. \medskip\noindent
\medskip
\noindent
{\bf Etape 2 : }Calculer et factoriser $P$. En déduire les valeurs propres $\lambda$ et leur multiplicité~$m_\lambda$.\medskip\noindent
\medskip
\noindent
{\bf Etape 3 : }Pour chaque valeur propre $\lambda$ : \pn 
a) Résoudre le système $(A-\lambda\mbox{I}_n)X=0$ et en déduire une base~$\sc B_\lambda$ de l'espace propre de $\lambda$. \pn
b) Prendre $k=1$. Tant que la base $\sc B_\lambda$ comporte moins de $m$ vecteurs, effectuer l'étape nécessaire suivante (éventuellement plusieurs fois) :
c) Ajouter $1$ à $k$, résoudre le système $(A-\lambda\mbox{I}_n)^kX=0$ et compléter $\sc B_\lambda$ en une base de l'espace des solutions de ce système. 
\medskip\noindent
{\bf Etape 4 : } a) Mettre toutes les bases $\sc B_\lambda$ bout à bout pour fabriquer une grande base $\sc B$ en respectant la contrainte suivante : 
$$
\mbox{\it l'ordre $k$ de fabrication de deux vecteurs associés à une même valeur propre doit être respecté}. 
$$
b) Ecrire la relation $A=PTP^{-1}$. \pn
c) Ecrire la matrice $P$ dont les colonnes sont les vecteurs de la grande base $\sc B$. \pn
d) Ecrire la matrice triangulaire $T$ de l'endomorphisme $X\mapsto AX$ dans la base $\sc B$. \pn
{\it Cette matrice aura la particularité utile suivante :}
$$ 
\vbox{\it Chaque coefficient de la diagonale principale de $T$ sera la valeur propre associé au vecteur de $P$ disposé sur la même colonne.} 
$$
{\bf Etape 5 : }Au besoin, inverser la matrice $P$ pour obtenir la matrice $P^{-1}$. 

%\tikzstyle{object}=[ellipse,fill=red!20, draw,inner sep=0.5em,text depth=-0.2em]
%\tikzstyle{operator}=[rectangle,rounded corners,fill=blue!20, draw,text height=1em,text depth=0.2em,inner sep=0.4em]
%\tikzstyle{fork}=[diamond,fill=green!20, draw,text height=1.2em,text depth=0.2em]
%\tikzstyle{line}=[draw,line width=0.5ex]
%\tikzstyle{thinline}=[draw,line width=0.2ex]
\centerline{%
\tikzpicture
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
\node[object,text width=2.3cm] (a) {Valeur propre $\lambda$\pn de multiplicité $m$} ;
\node[operator,node distance=4cm,right of=a,text width=2.4cm,text height=1.8em] (b) {Résolution de\pn$(A-\lambda I_n)X=0$} ;
\node[above of=b,node distance=0.8cm] (bb) {$k=1$};
\node[object,text width=2.4cm, yshift=0.5cm,right of=b,node distance=4cm] (c) {Espace propre $E_\lambda$} ;
\node[object,text width=2.3cm, below of=c] (d) {\eightpts $\sc B$ base de $E_\lambda$} ;
\node[fork,node distance=2cm,below of=d,text width=1.2cm] (e) {$\sc B$ a $m$\pn vecteurs} ;
\node[right of=e,node distance=3cm,regular polygon,regular polygon sides=6,draw=red,line] (g) {$\!\!\!$Stop$\!\!\!$};
\path (e)-- node[midway,fill=white] (f) {oui} (g) ;
\node[operator,node distance=2.5cm,below of=b,text width=2.4cm,text height=1.8em] (i) {Résolution de\pn$(A-\lambda I_n)^kX=0$} ;
\path (e)-- node[midway,fill=white] (h) {non} (i) ;
\node[above of=i,node distance=0.8cm] (ii) {ajouter $1$ à $k$};
\node[object,text width=2.5cm, yshift=0.7cm,left of=i,node distance=4cm] (j) {Espace solution $E$} ;
\node[object,text width=2.5cm,text height=1.4em, node distance=1.3cm,below of=j] (k) {Compléter $\sc B$ en une base de $E$} ;
\pgfonlayer{background}
	\draw[->,thinline] (a) -- (b) ;
	\draw[->,thinline] (b) -- (c.west) ;
	\draw[->,thinline] (c) -- (d) ;
	\draw[->,thinline] (d) -- (e) ;
	\draw [->,thinline] (e) -- (g) ;
	\draw [->,thinline] (e) -- (i) ;
	\draw [->,thinline] (i) -- (j.east) ;
	\draw [->,thinline] (j) -- (k) ;
	\draw [->,thinline] (k.south) -- +(0 cm,-0.3cm) -| (e.south) ;
\endpgfonlayer
\endtikzpicture
}%

\Figure[Index=Algorithme!detrigonalisation@de trigonalisation] Algorithme de trigonalisation.





\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Diagonalisation\Trigonalisation\Réduction}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}


\hautspages{Olus Livius Bindus}{Equations différentielles}%

\Chapter rah, Equations différentielles. 

Dans tout ce chapitre, $I$ désigne un intervalle réel 
contenant au moins $2$ points et $\ob K$ désigne le corps $\ob R$ ou $\ob C$. 
\bigskip

\Section rah, Equation différentielle linéaire du premier ordre. 

\noindent\underbar{Objectif}\medskip

Etant données deux fonctions $(a,b)\in\sc C(I, \ob K)$, 
nous cherchons à trouver toutes les solutions $f\in\sc C^1(I,\ob K)$ de 
l'équation différentielle linéaire du premier ordre
$$
f'(t)=a(t)f(t)+b(t)\qquad(t\in I).\leqno{(E)}
$$


\Concept Variation de la constante 

Soient $t_0\in I$ et $c\in\ob K$. 
Alors, il existe une unique fonction $f\in\sc C^1(I,\ob K)$ vérifiant le problème de Cauchy
$$
\Q\{\eqalign{
f'(t)&=a(t)f(t)+b(t)\qquad(t\in I),\cr
f(t_0)&=c.}
\W.
$$
De plus, cette solution $f$ satisfait l'identité 
$$
\forall t\in I, \qquad f(t)=\underbrace{g(t)\int_{t_0}^t{b(x)\F g(x)}\d x}_{\mbox{\sevenrm sol. part. de $\ss E$}}+\underbrace{c g(t)}_{\mbox{\sevenrm sol. gén. de $\ss H$}},
$$
la fonction $g$ étant définie sur l'intervalle $I$ par 
$$
g(t):=\underbrace{\exp\Q({\int_{t_0}^ta(x)\d x}\W)}_{\mbox{\sevenrm sol. part. de $\ss H$}}\qquad(t\in I).
$$ 

\Concept Méthode de résolution pratique

\noindent
1) 
On cherche une solution particulière $g$, ne~s'annulant pas sur $I$, 
de l'équation différentielle homogène $(H)$ associée à $E$ 
$$
g'(t)=a(t)g(t)\qquad(t\in I).\leqno{(H)}
$$ 
2) On cherche une solution particulière $f$ de l'équation différentielle $(E)$, 
de la forme 
$$
f(t)=g(t)h(t)\qquad(t\in I)
$$ 
Pour cela, on doit résoudre $h'(t)=b(t)g(t)\ \,(t\in I)$, l'application $h$ 
étant l'inconnue. 
\medskip
\noindent 3) Comme l'ensemble des solutions $S_H$ de l'équation différentielle homogène $(H)$ 
est une droite vectorielle, on a 
$S_H=\mbox{Vect}(g)$. Autrement dit, 
$$
\forall\tilde g\in S_H, \quad \exists!c\in\ob K:\qquad 
\forall t\in I, \quad \tilde g(t)=\underbrace{cg(t)}_{\mbox{\sevenrm sol. gén de $H$}}.
$$ 
\noindent 4) L'ensemble des solutions $S_E$ de l'équation différentielle $(E)$ 
étant un espace affine contenant $f$ de direction $S_H$ (de dimension $1$), on a 
$S_H= f+\mbox{Vect}(g)$. Autrement~dit, 
$$
\forall \tilde f\in S_E, \quad\exists!c\in\ob K:\qquad 
\forall t\in I, \quad \tilde f(t)= \underbrace{f(t)}_{\mbox{\sevenrm sol. part. de $\ss E$}}+\underbrace{c g(t)}_{\mbox{\sevenrm sol. gén de $H$}}.
$$ 

\Remarque : Une solution $g$ de $(H)$, non nulle en un point de $I$, 
ne s'annule pas sur~$I$. 
\bigskip

\Remarque : Pour résoudre une équation différentielle du type 
$$
\alpha(t)f'(t)+\beta(t)f(t)+\gamma(t)=0\qquad(t\in J),
$$
on résoud l'équation (E) sur les intervalles $I\subset J$ tels que $\alpha(t)\neq 0$ pour $t\in I$ avec 
$$
a(t)=-{\beta(t)\F\alpha(t)}\qquad\sbox{et}\qquad b(t)=-{\gamma(t)\F\alpha(t)}\qquad(t\in I).
$$ 
puis on regarde comment se font les recollements aux points $t\in I$ vérifiant $\alpha(t)=0$. 


\Exercice{PTSIxi}%

\Section rah, Equation différentielle à variables séparables. 

\Concept But 

Etant donnés deux intervalles réels $I, J$ contenant au moins deux points
et deux fonctions continues $a:J\to\ob K$ et $b:I\to \ob K$, 
nous cherchons à trouver toutes les fonctions $f\in\sc C^1(I, J)$ vérifiant 
l'équation différentielle non-linéaire du premier ordre 
$$
f'(t)a\b(f(t)\b)=b(t)\qquad(t\in I).\leqno{(Es)}
$$

\Remarque : 
L'équation $(E)$ est dite à variables~séparables, ($y'a(y)=b(t), \forall t\in I$). \pn
Une fonction $f\in\sc C^1(I, J)$ vérifiant $(E)$ est appelée solution (sur $I$) 
de l'equa-diff~$(E)$. 

\Propriete [$A$ (resp. $B$) primitives de $a$ sur $J$, (resp. de $b$ sur $I$)]
Si $f$ est une solution de $(E)$ alors, il existe une constante $c\in\ob K$ telle que 
$$
\forall t\in I,\qquad A\b(f(t)\b)=B(t)+c. \eqdef{er}
$$ 
De plus, si la fonction $A$ est injective sur l'intervalle $J$ 
(par exemple si $a$ ne s'annule pas sur $J$), l'application $A:J\to A(J)$ 
est bijective et $f$ satisfait 
$$
\forall t\in I, \qquad f(t)=A^{-1}\b(B(t)+c\b) \eqdef{es}
$$


\Remarque : Etant donné $t_0\in I$, on a $c=0$ dans les relations \eqref{er} 
et \eqref{es} pour le choix 
$$
B(t)=\int_{t_0}^tb(u)\d u\qquad(t\in I)\qquad\sbox{et}\qquad A(x)=\int_{f(t_0)}^x a(u)\d u\qquad(x\in J).
\eqdef{est}
$$

\Remarque : La propriété précédente permet de déterminer 
les solutions s'il y en a. 
Inversement, si $a$ ne s'annule pas sur $J$, si $t_0\in I$ 
et si $B(I)\subset A(J)$ pour le choix~\eqref{est}, 
on peut montrer que la fonction $f:t\mapsto A^{-1}\circ B(t)$ 
est solution de $E$ sur $I$. 
\bigskip

\Concept Méthode de résolution. 

\noindent
1) Mettre l'équation sous la forme $(Es)$. \pn
2) Intégrer la relation $(Es)$ (ne pas oublier la constante). \pn
3) (unicité des solutions) En déduire les solutions $f:I\to J$, si c'est possible. \pn
4) (existence des solutions) Prouver que les fonctions $f$ obtenues satisfont bien $(Es)$. 
\bigskip

\Exercice{PTSIxj}%

\Section rah, Systèmes différentiels linéaires d'ordre $1$. 


Etant donné une matrice $A\in\sc M_n(\ob K)$, un intervalle réel $I$ 
contenant au moins deux points et une application continue 
$B:I\to\sc M_{n,1}(\ob K)$. 
Nous cherchons à trouver toutes les fonctions $X:I\to\sc M_{n,1}(\ob K)$ 
de classe $\sc C^1$ vérifiant le système différentielle 
linéaire du premier ordre (``avec second membre'')
$$
X'(t)=AX(t)+B(t)\qquad(t\in I)\leqno{(E)}
$$
autrement dit le système 
$$
\Q\{\eqalign{x_1'(t)&=a_{1,1}x_1(t)+\cdots+a_{1,n}x_n(t)+b_1(t)\cr
x_2'(t)&=a_{2,1}x_1(t)+\cdots+a_{2,n}x_n(t)+b_2(t)\cr
\cdots\quad&=\qquad\qquad\quad\cdots\cr
x_n'(t)&=a_{n,1}x_1(t)+\cdots+a_{n,n}x_n(t)+b_n(t)\cr
}\W.\qquad(t\in I).
$$


\Theoreme [$t_0\in I$, $X_0\in \sc M_{n,1}(\ob K)$] 
Il existe une unique fonction $X\in\sc C^1\b(I,\sc M_{n,1}(\ob K)\b)$ vérifiant le problème de Cauchy
$$
\Q\{\eqalign{
X'(t)&=AX(t)+B(t)\qquad(t\in I),\cr
X(t_0)&=X_0.}
\W.\eqdef{cauchy}
$$ 

\Theoreme
L'ensemble des solutions $S_H$ du 
système différentiel homogène $(H)$ 
$$
Y'(t)=AY(t)\qquad(t\in I)\leqno{(H)}
$$
associé à $(E)$ est un espace vectoriel de dimension $n$. 

\Theoreme
L'ensemble des solutions $S_E$ du système différentiel $(E)$ est un espace affine 
de direction $S_H$, de dimension $n$. Soit $X_p$ une solution particulière de $(E)$, 
on~a~ainsi $S_E=X_p+S_H=\{X_p+Y:Y\in S_H\}$. 


\Remarque : Lorsque la matrice $A$ est diagonalisable dans $\ob K$, 
il existe $P\in \sc Gl_n(\ob K)$ et une matrice diagonale $D\in\sc M_n(\ob K)$ telle que 
$A=P^{-1}D P$. Posant $Y_0:=PX_0$, $Y(t):=PX(t)$ et $E(t):=PB(t)$ pour $t\in I$, 
résoudre \eqref{cauchy} revient à résoudre 
$$
\Q\{\eqalign{
Y'(t)&=DY(t)+E(t)\qquad(t\in I)
\cr
Y(t_0)&=Y_0.}\W.
$$ 
c'est à dire à résoudre le système différentiel diagonal
$$
\Q\{\eqalign{y_1'(t)&=d_{1,1}y_1(t)+0+\cdots+0+e_1(t)\cr
y_2'(t)&=0+d_{2,2}y_2(t)+0+\cdots+0+e_2(t)\cr
\vdots\quad&=0\cdots+0\quad\ddots\quad\cdots\cr
y_n'(t)&=0+\cdots+0+d_{n,n}y_n(t)+e_n(t)\cr
}\W.\qquad(t\in I).\eqdef{syst2}
$$
pour les conditions initiales $y_1(t_0)=\alpha_1,\cdots,y_n(t_0)=\alpha_n$ 
(avec $Y_0=(\alpha_1,\cdots,\alpha_n)$). 
\bigskip



\Concept Méthode

Pour résoudre un système diagonal tel que \eqref{syst2}, 
on résoud pour chaque entier $i\in\{1,\cdots,n\}$ le problème de Cauchy
$$
\Q\{\eqalign{y_i'(t)&=d_{i,i}y_i(t)+e_i(t)\qquad(t\in I)\cr
y_i(t_0)&=\alpha_i\cr
}\W.
$$ 
En effet, les équations sont toutes indépendantes. 
\bigskip

\Exemple. pour la condition $x(0)=y(0)=z(0)=1$, 
résoudre le système différentiel
$$
\Q\{\eqalign{x'(t)&=x(t)+\ \ 0\ \ +\ \ 0+1\cr
y'(t)&=0\ \ +\ \ y(t)+\ \ 0-\e^t\cr
z'(t)&=0\ \ +\ \ 0\ \ +\ \ 2z(t)\cr
}\W.\qquad(t\in \ob R).
$$

\Remarque : Lorsque la matrice $A$ est trigonalisable dans $\ob K$ 
(c'est toujours vrai pour $K=C$), 
il existe $P\in \sc Gl_n(\ob K)$ et une matrice triangulaire supérieure $C\in\sc M_n(\ob K)$ 
telle que $A=P^{-1}CP$. 
Posant $Y_0:=PX_0$, $Y(t):=PX(t)$ et $D(t):=PB(t)$ pour $t\in I$, 
résoudre \eqref{cauchy} revient à résoudre 
$$
\Q\{\eqalign{
Y'(t)&=CY(t)+D(t)\qquad(t\in I)
\cr
Y(t_0)&=Y_0.}\W.
$$ 
c'est à dire à résoudre le système différentiel triangulaire supérieur
$$
\Q\{\eqalign{y_1'(t)&=c_{1,1}y_1(t)+\cdots+c_{1,n}y_n(t)+d_1(t)\cr
y_2'(t)&=0+c_{2,2}y_2(t)+\cdots+c_{2,n}y_n(t)+d_2(t)\cr
\vdots\quad&=0\cdots+0\quad\ddots\quad\cdots\cr
y_n'(t)&=0+\cdots+0+c_{n,n}y_n(t)+d_n(t)\cr
}\W.\qquad(t\in I).\eqdef{syst}
$$
pour les conditions initiales $y_1(t_0)=\alpha_1,\cdots,y_n(t_0)=\alpha_n$ 
(avec $Y_0=(\alpha_1,\cdots,\alpha_n)$). 
\bigskip

\Concept Méthode

Pour résoudre un système triangulaire tel que \eqref{syst}, 
on résoud pour la condition intitiale $y_n(t_0)=\alpha_n$ 
l'équation différentielle de la dernière ligne
$$
\Q\{\eqalign{y_n'(t)&=c_{n,n}y_n(t)+d_n(t)\qquad(t\in I)\cr
y_n(t_0)&=\alpha_n\cr
}\W.
$$ 
Ensuite, on injecte le résultat dans les lignes du dessus, 
on obtient alors un système différentiel triangulaire de taille moindre 
et on recommence... 
\bigskip

\Exemple. pour la condition $x(0)=y(0)=z(0)=1$, 
résoudre le système différentiel
$$
\Q\{\eqalign{x'(t)&=x(t)+y(t)+z(t)-2\e^t\cr
y'(t)&=0\ \ +\ \ y(t)+z(t)-\e^t\cr
z'(t)&=0\ \ +\ \ 0\ \ +\ \ z(t)
}\W.\qquad(t\in \ob R).
$$

\Remarque : Soient $n\in\ob N$ et $(c_0,\cdots, c_n)\in\ob C^{n+1}$. 
Alors, résoudre l'équation différentielle linéaire d'ordre $n$ 
à coefficients constants
$$
f^{(n+1)}(t)=\sum_{k=0}^nc_kf^{(k)}(t)\qquad(t\in\ob R)\eqdef{eqdf}
$$
revient à résoudre le système différentiel d'ordre $1$
$$
F'(t)=\pmatrix{0&1&0&\cdots&0\cr0&0&1&\ddots&\vdots\cr\vdots&&\ddots&\ddots&0\cr0&0&\cdots&0&1\cr c_0&c_1&c_2&\cdots&c_n}F(t)\qquad(t\in I)
$$
si l'on pose $F(t)=\Q(f(t),f'(t),\cdots,f^{(n)}(t)\W)$ pour $t\in I$. 
\medskip
Le polynôme caractéristique $P$ de l'équation \eqref{eqdf} est 
$$
P=X^{n+1}-\sum_{k=0}^nc_kX^k
$$
La fonction $t\mapsto\e^{zt}$ est solution de \eqref{eqdf} si, et seulement si, $P(z)=0$. 
\bigskip

\Exemple. Trouver les solutions réelles de $y'''=y$ sur $\ob R$. 
\bigskip



\Section rah, Equations linéaires d'ordre $2$.
\bigskip

\Concept But 

Etant données trois fonctions $(a,b,c)\in\sc C(I, \ob K)$, 
nous cherchons à trouver toutes les solutions $f\in\sc C^2(I,\ob K)$ de 
l'équation différentielle linéaire du second ordre (``avec second membre'')
$$
f''(t)=a(t)f'(t)+b(t)f(t)+c(t)\qquad(t\in I).\leqno{(E)}
$$


\Theoreme [$t_0\in I$, $(f_0,f_1)\in\ob K^2$] 
Il existe une unique fonction $f\in\sc C^2(I,\ob K)$ 
vérifiant le problème de Cauchy
$$
\Q\{\eqalign{
f''(t)&=a(t)f'(t)+b(t)f(t)+c(t)\qquad(t\in I),\cr
f(t_0)&=f_0,\cr
f'(t_0)&=f_1.
}
\W.
$$

\Propriete
L'ensemble $S_H$ des solutions $f\in\sc C^2(I,\ob K)$ 
de l'équation différentielle 
homogène $(H)$ 
$$
g''(t)=a(t)g'(t)+b(t)g(t)\qquad(t\in I).\leqno{(H)}
$$
associée à $(E)$ est un espace vectoriel de dimension $2$. 
\bigskip

\ Propriete
L'ensemble $S_E$ des solutions $\tilde f\in\sc C^2(I,\ob K)$ 
de l'équation différentielle $(E)$ est un espace affine de direction $S_H$. 
Etant donnée une solution particulière $f$ de $(E)$, on a $s_E=f+S_H$ 
c'est à dire $S_E=\{f+g:g\in S_H\}$ ou encore
$$
\forall \tilde f\in S_E, \quad \exists ! g\in S_H: \qquad \forall t\in I,\quad \tilde f(t)=f(t)+g(t).
$$ 

\Exercice{akj}%

\Concept Variation de la constante

Si $g$ est une solution, 
ne s'annulant pas sur $I$, de~l'équation différentielle linéaire 
homogène $(H)$ associée à $(E)$
$$
g''(t)=a(t)g'(t)+b(t)g(t)\qquad(t\in I),\leqno{(H)}
$$
on cherche les solutions $f$ de $(E)$ sous la forme $f(t)=g(t)h(t)$. Pour cela, 
on est amené à trouver les fonctions $h$ vérifiant l'équation différentielle 
$$
h''(t)=\Q(a(t)-{g'(t)\F g(t)}\W)h'(t)+{c(t)\F g(t)}\qquad(t\in I).
$$
On remarque alors que la fonction $y(t)=h'(t)$ satisfait 
l'équation~différentielle
$$
y'(t)=\Q(a(t)-{g'(t)\F g(t)}\W)y(t)+{c(t)\F g(t)}\qquad(t\in I) 
$$
que l'on résoud puis on intègre la relation $h'=y$ pour trouver $h$ et donc 
$f=g\times h$. 
\bigskip

\Remarque : Pour résoudre une équation différentielle du type 
$$
\alpha(t)f''(t)+\beta(t)f'(t)+\gamma(t)f(t)+\Delta(t)=0\qquad(t\in I),
$$
on résoud l'équation $f''(t)=a(t)f'(t)+b(t)f(t)+c(t)\ \,(t\in J)$ 
sur les intervalles $J\subset I$ {\bf pour lesquels $\alpha(t)\neq 0\ \,(t\in J)$} 
 avec 
$$
a(t)=-{\beta(t)\F\alpha(t)},\qquad b(t)=-{\gamma(t)\F\alpha(t)},\qquad\sbox{et}\qquad 
c(t)=-{\Delta(t)\F\alpha(t)}\qquad(t\in J)
$$ 
puis on regarde comment se font les recollements aux points $t\in I$ 
vérifiant $\alpha(t)=0$. 
\bigskip

\Section rah, Généralités. 



\Definition [$f:I\to\ob
 K$ solution sur $I$ de $(E)$] 
Le graphe $\Q\{\Q(t,f(t)\W):t\in I\W\}$ est appelé courbe intégrale 
de l'équation différentielle $(E)$. 
\medskip\noindent
Soit $x_1,\cdots,x_n:I\to\sc K$ une solution sur $I$ 
d'un système différentiel $(E)$. Alors le graphe 
$\{\Q(t,x_1(t),\cdots,x_n(t)\W):t\in I\}$ 
est appelé courbe intégrale du système différentiel $(E)$. 
\bigskip

\Remarque : Résoudre une équation différentielle 
(resp. un système différentiel) revient à trouver ses courbe intégrales. 
\bigskip 

\Exercice{akk}%

\Definition [$U$ ouvert non vide de $\ob R^2$, $\varphi$ et $\psi$ 
deux applications réelles continues sur $U$]
Une solution du système différentiel autonome du $1^{\mbox{\sevenrm er}}$ ordre 
$$
\Q\{\eqalign{x'(t)=\varphi\b(x(t),y(t)\b)\cr
y'(t)=\psi\b(x(t),y(t)\b)\cr}\W. \eqdef{reset}
$$
est la donnée d'un intervalle réel $I$ contenant au mois deux points et de deux fonction 
$x,y:I\to U$ de classe $\sc C^1$ sur $I$ vérifiant \eqref{reset}
\bigskip

\Definition [$(x,y)$ solution du système différentiel autonome \eqref{reset} sur $I$]
La trajectoire de la solution $(x,y)$ est le graphe $\B\{\b(x(t),y(t)\b):t\in I\B\}$.

\Remarque : On aimerait obtenir une relation entre $x(t)$ et $y(t)$ en intégrant (ce qui n'est pas toujours possible) 
la relation 
$$
y'(t)\varphi\b(x(t),y(t)\b)-x'(t)\psi\b(x(t),y(t)\b)=0\qquad(t\in I), 
$$
où ``plutôt'' la relation 
$$
\varphi\b(x,y\b)\d y-\psi\b(x,y\b)\d x=0 \eqdef{form}
$$

\Concept Méthode 

{\it condition necéssaire pour la méthode : $\varphi$ et $\psi$ doivent être $\sc C^1$ sur $U$}\pn
1) On regarde si la forme différentielle \eqref{form} est une fermée (totale pour les physiciens) en regardant si 
$$
{\partial\varphi\F\partial x}(x,y)-{\partial\psi\F\partial y}(x,y)=0.
$$
2) oui, la forme différentielle admet une primitive (c'est vrai localement), on peut l'intégrer 
(donc on le fait) et on trouve une relation entre $y$ et $x$, qui nous donne une équation cartésienne des trajectoires. 
\pn
1') non, on multiplie \eqref{form} par un facteur $g(x,y)$ 
choisi pour que la forme différentielle 
$$
g(x,y)\varphi\b(x,y\b)\d y-g(x,y)\psi\b(x,y\b)\d x=0 
$$
soit fermée et 2') on l'intégre pour trouver une relation entre $x$ et $y$. 
\bigskip

\Exercice{akl}%





\eject

\hautspages{Olus Livius Bindus}{Espaces vectoriels préhilbertiens et euclidiens}%

\Chapter fonc, Espaces vectoriels préhilbertiens et euclidiens. 

\Section la1, Espaces pré-hilbertiens (réels). 

\Subsection LA1, Produit scalaire (réel). 

\Definition [$E$ un $\ob R$-espace vectoriel, $f:E\times E\to\ob F$ application]
$$
\eqalign{
f\sbox{ est une forme} \quad&\Leftrightarrow\quad F \sbox{ est le corps des scalaires}.
\cr
f\sbox{ est symétrique} \quad&\Leftrightarrow\quad \forall (x,y)\in E^2,\quad f(x,y)=f(y,x).
\cr
f\sbox{ est positive} \quad&\Leftrightarrow\quad\forall x\in E, \quad f(x,x)\ge0. 
\cr
f\sbox{ est définie} \quad&\Leftrightarrow\quad\forall x\in E, \quad f(x,x)=0\Rightarrow x=0
\cr
f\sbox{ est bilinéaire} \quad&\Leftrightarrow\quad \forall y\in E, \quad x\mapsto f(x,y) \sbox{ et }x\mapsto f(y,x) 
\sbox{ sont linéaires sur }E. }
$$


\Definition [$E$ $\ob R$-espace vectoriel]
Un produit scalaire de $E$ est une forme bilinéaire, symétrique, définie, positive de l'espace vectoriel $E$. Autrement dit, c'est une application 
$$
f:E\times E\to\ob R
$$ 
possédant les qualités précitées.

\Remarque : pour $(x,y)\in\ob E^2$, le produit scalaire $f(x,y)$ est usuellement noté 
$$
\langle x,y\rangle\quad\mbox{ ou }\quad\langle x|y\rangle\quad\mbox{ ou }\quad(x|y)\quad\mbox{ ou }\quad\vec x.\vec y.
$$

\Exemples. L'espace vectoriel $\ob R^n$ est naturellement muni du produit scalaire défini 
par 
$$
\Q\{\eqalign{
&\forall x=(x_1,\cdots,x_n)\in \ob R^n\cr
&\forall y=(y_1,\cdots,y_n)\in\ob R^n
}\W.\qquad\langle x,y\rangle:=\sum_{k=1}^n x_ky_k
$$ 
De même, l'espace vectoriel $\sc M_{n,1}(\ob R)$ est naturellement muni du produit scalaire 
$$
\forall (X,Y)\in\sc M_{n,1}(\ob R)^2, \qquad \langle X,Y\rangle:=^tXY.
$$
L'espace $\sc C\b([a,b],\ob R\b)$ est naturellement muni du produit scalaire défini 
par 
$$
\forall(f,g)\in\sc C\b([a,b],\ob R\b)^2,\qquad
 \langle f,g\rangle:=\int_a^bf(t)g(t)\d t.
$$ 

\Subsection La2, Espace pré-hilbertien (réel). 

\Definition Un espace pré-Hilbertien est un $\ob R$-espace vectoriel muni d'un produit scalaire. 
\bigskip

\Definition [$E$ $\ob R$-espace vectoriel] 
Une application $N:E\to\ob R$ est une norme de $E\ssi$ on a 
$$
\eqalignno{
\forall x\in E,&\qquad N(x)\ge0
\cr
\forall x\in E,&\qquad N(x)=0\Longleftrightarrow x=0
\cr
\forall (\lambda,x)\in\ob R\times E,& \qquad N(\lambda x)=|\lambda|N(x)
\cr
\forall (x,y)\in E\times E,&\qquad N(x+y)\le N(x)+N(y) 
}
$$



\Concept Norme euclidienne


\Propriete [$E$ $\ob R$ espace vectoriel muni d'un produit scalaire $\langle\cdot,\cdot\rangle$] 
L'application~$x\mapsto \|x\|$ définie~par 
$$
\forall x\in E, \qquad \|x\|=\sqrt{\langle x,x\rangle},
$$ 
est une norme de $E$, appelé norme euclidienne (associée au produit scalaire $\langle\cdot,\cdot\rangle$). 


\Remarque : En particulier la norme euclidienne satisfait les $2$ inégalités triangulaires :
$$
\forall(x,y)\in\ob E^2, \qquad \B|\|x\|-\|y\|\B|\le \|x+y\|\le \|x\|+\|y\|.
$$

\Concept Identités de polarisation

\Remarque : le produit scalaire (forme polaire) peut s'exprimer en fonction de la norme. 
\medskip

\Propriete[$(x,y)\in E^2$]
$$
\eqalignno{
\qquad\qquad\qquad\qquad\qquad\qquad\langle x,y\rangle&={\|x+y\|^2-\|x\|^2-\|y\|^2\F 2}&\mbox{(Identité de polarisation \it symétrique)}
\cr
\langle x,y\rangle&={\|x+y\|^2-\|x-y\|^2\F 4}&\mbox{(Identité de polarisation \it asymétrque)}
}
$$

\Concept Identité du parallélogramme


\Propriete La somme des carrés des longueurs des diagonales d'un parallélogramme est égale à la somme des carrés des longueurs de ses cotés. 
$$
\forall (x,y)\in E^2, \qquad \|x+y\|^2+\|x-y\|^2=2\|x\|^2+2\|y\|^2.
$$

\Concept Inégalité de Cauchy-Schwarz

\Theoreme[$E$ $\ob R$-espace vectoriel muni d'un produit scalaire $\langle\cdot,\cdot\rangle$]
\Equation [Inégalité de Cauchy-Schwarz]
$$
\forall (x,y)\in E^2, \qquad \Q|\langle x,y\rangle\W|\le \|x\|\times\|y\|. 
$$
De plus, on a 
\Equation [Cas d'égalité]
$$
\Q|\langle x,y\rangle\W|=\|x\|\times\|y\|\quad\Longleftrightarrow \quad \mbox{la famille de vecteurs }\{x,y\} \mbox{ est liée}.
$$ 


\Application : Ce théorème permet démontrer des inégalités difficiles. 
Par~exemple, on a 
$$
\forall n\in\ob N^*, \qquad \Q|\sum_{k=1}^n{\e^{i n}\F n}\W|\le{\pi\F\sqrt6}\sqrt n. 
$$
On a également 
$$
\forall x\ge0, \qquad 0\le \int_0^x{\e^{-t}\F \sqrt{1+t^2}}\d t\le {\sqrt{\pi}\F 2}.
$$

\Concept Distance associée. 

\Definition [$E$ espace pré-hilbertien] 
La distance (euclidienne) associée au produit scalaire $\langle\cdot,\cdot\rangle$ est l'aplication $\d:E\times E\to\ob R^+$ définie par 
$$
\forall (x,y)\in E^2, \qquad \d(x,y):=\|x-y\|.
$$ 

\Concept Espaces euclidiens. 

\Definition 
Un espace vectoriel euclidien est un $\ob R$-espace vectoriel de dimension finie muni d'un produit scalaire (réel). 

\Remarque : Un espace euclidien est donc un espace préhilbertien réel, de dimension finie. 
\bigskip


\Subsection ahahaha, Géométrie dans les espaces préhilbertiens. 


\Concept Orthogonalité

\noindent
Dans cette section, $E$ désigne un $\ob R$-espace vectoriel muni d'un produit scalaire $\langle\cdot,\cdot\rangle$. 
\bigskip

\Definition [$E$ espace préhilbertien] 
Deux vecteurs $x$ et $y$ de $E$ sont orthogonaux si, et seulement~si, leur produit scalaire est nul. 
$$
x\perp y\quad\Longleftrightarrow\quad \langle x,y\rangle=0.
$$


\Concept Théorème de Pythagore 

\Theoreme [$x$ et $y$ dans $E$ espace préhilbertien] 
\Equation [Pythagore]
$$
x\perp y\ssi \|x+y\|^2=\|x\|^2+\|y\|^2. 
$$

\Concept Familles orthogonales


\Definition [$E$ espace préhilbertien] 
Une famille $\{x_i\}_{i\in I}$ de vecteurs de $E$ est orthogonale si, et~seulement~si, 
ses vecteurs sont non nuls et orthogonaux $2$ à $2$. C'est-à-dire si, et seulement si,
$$
\forall (i,i')\in I^2, \qquad \Q\{\eqalign{\langle x_i,x_{i'}\rangle=
0&\sbox{ si }i\neq i'\cr
\underbrace{\langle x_i,x_{i'}\rangle}_{\|x_i\|^2}\neq 
0&\sbox{ si }i=i'
}\W.
$$

\Propriete Une famille orthogonale est libre. 


\Concept Vecteurs unitaires

\Definition [$E$ espace préhilbertien] 
 Un vecteur $x$ de $E$ est unitaire (ou normalisé) $\ssi $ il est de norme $1$. 
$$
x\sbox{ est un vecteur unitaire de }E\quad\Longleftrightarrow\quad\|x\|=1.
$$

\Concept Familles orthogonormales


\Definition [$E$ espace préhilbertien] 
Une famille $\{x_i\}_{i\in I}$ de vecteurs de $E$ est orthonormale $\ssi$ c'est une famille orthogonale de vecteurs unitaires, 
i.e. 
$$
\forall (i,i')\in I^2,\qquad \langle x_i,x_{i'}\rangle=\Q\{\eqalign{
0&\sbox{ si }i\neq i'\cr
1&\sbox{ si }i=i'
}\W.
$$

\Concept Expression du produit scalaire dans une base orthonormale/orthonormale

\Propriete [${\sc E:=\{e_1,\cdots,e_n\}}$ famille de vecteurs de $E$ espace préhilbertien]
Soient $x=\sum_{k=1}^n\lambda_ke_k$ et $y=\sum_{k=1}^n\mu_ke_k$ deux vecteurs de l'espace engendré par $\sc E$. \pn
Si la famille $\sc E$ est orthogonale, on a 
$$
\langle x,y\rangle=\sum_{k=1}^n\lambda_k\mu_k\|e_k\|^2. 
$$
Si la famille $\sc E$ est orthonormale, on a 
\Equation [*]
$$
\langle x,y\rangle=\sum_{k=1}^n\lambda_k\mu_k. 
$$

\Remarque : La donnée d'une famille orthonormale permet de ramener le 
produit scalaire au cas simple $(*)$. C'est pour cela que les base orthonormales sont si intéressantes et c'est pour cela que l'on cherche à orthonormaliser des familles libres. 
\bigskip

\Concept [Index=Procede de Gram-Schmidt@Procédé de Gram-Schmidt] Procédé d'orthonormalisation de Gram-Schmidt

\Theoreme [$E$ espace préhilbertien muni d'un produit scalaire $\langle\cdot,\cdot\rangle$, $n\in\ob N^*$]
Si $\{x_k\}_{1\le k\le n}$ est une famille libre~de~$E$, il existe une unique famille orthonormale $\{z_k\}_{1\le k\le n}$ de $E$ telle que 
$$
\forall k\in\{1,\cdots,n\}\quad\Q\{\eqalign{&\mbox{Vect}\b( z_1,\cdots,z_k\b)
=\mbox{Vect}\b(x_1,\cdots,x_n\b),\cr
&\langle z_k,x_k\rangle>0.}\W.
$$
La famille $\{z_1,\cdots,z_n\}$ est définie par récurence par 
$$
z_1={x_1\F\|x_1\|}\qquad\sbox{et}\qquad\forall k\in\{2,\cdots, n\}, \quad z_k=
{x_k-\sum_{\ell=1}^{k-1}\langle x_k,z_\ell\rangle z_\ell
\F \b|\!\b|x_k-\sum_{\ell=1}^{k-1}\langle x_k,z_\ell\rangle z_\ell \b|\!\b| }
$$

\Remarque{ \it 1} : Normaliser un vecteur non nul $x$, consiste à le diviser par sa norme $\|x\|$ pour le rendre unitaire. 
\bigskip

\Remarque{ \it 2} : Si $\{x_1,\cdots,x_n\}$ est une famille orthogonale et si $y$ est linéarement indépendant de $\{x_1,\cdots,x_n\}$, on peut fabriquer un vecteur $x_{n+1}$ de la forme 
$$
x_{n+1}=y+\sum_{k=1}^n\lambda_kx_k
$$ 
tel que la famille $\{x_1,\cdots,x_{n+1}\}$ soit orthogonale. Pour cela, il suffit de choisir $(\lambda_1,\cdots,\lambda_n)$ de telle sorte que, pour l'on ait
$$
\forall k\in\{1,\cdots,n\}, \qquad 0=\langle x_{n+1},x_k\rangle=\langle y,x_k\rangle+\lambda_k\underbrace{\|x_k\|^2}_{>0}.
$$

\Remarque{ \it 3} : Pour fabriquer une famille orthonormale $\{z_1,\cdots,z_n\}$ à partir d'une famille libre $\{x_1,\cdots,x_n\}$, 
on peut s'y prendre de deux fa\cced cons différentes (pour le même résultat)~:
\medskip
\noindent
{\bf Méthode de Gram-Schmidt. } On orthogonalise et on normalise en même temps. On~normalise~$x_1$ pour obtenir $z_1$, 
puis on fabrique un vecteur $y_2$ orthogonal à $z_1$ qu'on normalise pour obtenir $z_2$, 
puis on fabrique un vecteur $y_3$ orthogonal à $z_1$ et $z_2$ qu'on normalise pour obtenir $z_3$, etc...jusqu'à obtenir $\{z_1,\cdots,z_n\}$. 
\medskip
\noindent
{\bf Méthode recommandée. }On orthogonalise la famille $\{x_1,\cdots,x_n\}$ pour obtenir une famille orthogonale $\{y_1,\cdots,y_3\}$, qu'on normalise ensuite pour obtenir $\{z_1,\cdots,z_n\}$. 
\bigskip
\Exercice{akm}%


\Propriete Un espace euclidien $E$ admet au moins une base orthonormale (une base qui est une famille orthonormale). 


\Concept Ensembles orthogonaux


\Definition [$E$ espace pré-hilbertien] 
Un vecteur $x$ de $E$ est orthogonal à un sous-ensemble $G$ de $E\ssi x$ est orthogonal à tous les vecteurs de $G$, 
$$
x\perp F \ssi \forall y\in G, \quad \underbrace{\langle x,y\rangle=0}_{x\perp y}. 
$$

\Definition [$E$ espace pré-hilbertien] 
Un sous ensemble $F$ de $E$ est orthogonal à un sous-ensemble $G$ de $E$ $\ssi$ chaque $x$ de $F$ est orthogonal à tous les vecteurs $y$ de $G$, 
$$
F\perp G \ssi \forall x\in F,\quad \underbrace{\forall y\in G, \quad \overbrace{\langle x,y\rangle=0}^{x\perp y}}_{x\perp G}. 
$$

\Remarque : Si $E$ est un espace pré-hilbertien et si $F$ et $G$ sont deux sous-espaces vectoriels de $E$, on note 
$$
F\mathop{+}\limits^\perp G\qquad (\mbox{resp. }F\mathop{\oplus}\limits^\perp G)
$$
si et seulement si l'on a $F\perp G$ (resp. si, et seulement si, l'on a $F\oplus G$ et $F\perp G$). 
\bigskip


\Concept Orthogonal d'un ensemble

\Definition [$E$ espace préhilbertien]
L'orthogonal d'un sous ensemble non vide $F\subset E$ est l'ensemble 
$$
F^\perp:=\{x\in E:\underbrace{\forall y\in F, \overbrace{\langle x,y\rangle=0}^{x\perp y}}_{x\perp F}\}.
$$

\Propriete [$E$ espace préhilbertien] 
L'orthogonal $F^\perp$ d'un sous ensemble $F$ non vide de $E$ est un sous-espace vectoriel de $E$. 

\Propriete [$E$ espace euclidien] 
Pour chaque sous espace vectoriel $F$ de $E$, on a 
$$
E=F\mathop{\oplus}\limits^\perp F^\perp.
$$
L'orthogonal de $F$ est appelé le suplémentaire orthogonal de $F$ (il est unique). 


\Propriete [$E$ espace pré-hilbertien, $F$ un sous ensemble non vide de $E$] 
Alors, on a 
$$
F\subset (F^\perp)^\perp
$$
De plus, si $E$ est un espace euclidien (si $E$ est de dimension finie), on a 
$$
\Vect(F)=(F^\perp)^\perp
$$

\Remarque : en particulier, dans un espace euclidien, on a 
$$
\mbox{l'ensemble non vide }F\sbox{ est un sous-espace vectoriel de $E$}\ssi F=(F^\perp)^\perp.
$$


\Subsection, Applications linéaires fondamentales.

\Concept Formes linéaires d'un espace euclidien

\Propriete [$E$ espace euclidien] 
Pour toute forme linéaire $f:E\to\ob R$ de $E$, il existe un unique vecteur $a\in E$ tel que 
$$
\forall x\in E, \qquad u(x)=\langle x,a\rangle.
$$

\Concept [Index=Applications lineaires@Applications linéaires!Projections orthogonales] Projections orthogonales

\Definition [$E$ $R$-espace vectoriel] 
Un projecteur de $E$ est un endomorphisme $p:E\to E$ vérifiant $p^2=p$. Un tel endomorphisme vérifie
$$
\eqalign{
E=\mbox{Im}&(p)\oplus\mbox{Ker}(p)
\cr
x\in\mbox{Im}(p)&\ssi p(x)=x
\cr
x\in \mbox{Ker}(p)&\ssi p(x)=0
}
$$
et est appelé projecteur sur $\mbox{Im}(p)=\mbox{Ker}(\Id-p)$ parallélement à $\mbox{Ker}(p)$. 

\Definition [$E$ espace pré-hilbertien]
Un projecteur de $E$ est orthogonal $\ssi \mbox{Im}(p)\perp\mbox{Ker}(p)$. 

\Remarque : En particulier, un projecteur orthogonal $p$ d'un espace préhilbertien $E$ satisfait 
$$
E=\mbox{Im}(p)\mathop{\oplus}\limits^\perp\mbox{Ker}(p).
$$

\Propriete [$E$ espace pré-hilbertien, $F\subset E$ sous-espace de dimension~finie]
Pour chaque $x\in E$, il~existe un unique vecteur $p(x)\in F$ tel que 
$$
x-p(x)\perp F.
$$ 
L'application $p:x\mapsto p(x)$ est un projecteur de $E$ (appelé projection orthogonale sur $F$) vérifiant 
$$
F=\mbox{IM}(p)\qquad\sbox{ et }\qquad \mbox{IM}(p)\perp\mbox{Ker}(p).
$$ 
Enfin, pour chaque base orthonormale $\{e_1,\cdots e_n\}$ de $F$, on a 
$$
\forall x\in E, \qquad p(x)=\sum_{k=1}^n\langle x,e_k\rangle e_k.
$$

\Propriete [$E$ espace préhilbertien] 
Un projecteur $p$ de $E$ est orthogonal $\ssi$
$$
\forall x\in E, \qquad \|p(x)\|\le \|x\|. 
$$

\Definition [$x\in E$ espace pré-hilbertien, $F$ sous espace vectoriel de $E$] 
La distance du vecteur $x$ à l'espace $F$, que l'on note $\d(x,F)$, est le nombre positif 
$$
\d(x,F):=\inf_{y\in F}\|y-x\|.
$$

\Propriete [$E$ espace pré-hilbertien de norme associée $\|\cdot\|$. ]
Si $F$ est un sous espace vectoriel de $E$ de dimension finie et si 
$p$ est la projection orthogonale sur $F$, on a 
$$
\forall x\in E, \qquad \d(x,F)=\|p(x)-x\|. 
$$ 

\Remarque : en particulier, les projections orthogonales constituent l'outil adapté au calcul de la distance d'un vecteur à un espace vectoriel. 
\bigskip


\Exercice{akn}%
\bigskip

\Concept Symétries orthogonales

\Definition [$E$ $\ob R$-espace vectoriel] 
Une symétrie de $E$ est un endomorphisme $s:E\to E$ vérifiant $s^2=\Id_E$. Un tel endomorphisme vérifie
$$
\eqalign{
E=\mbox{Ker}(s-\Id_E)\oplus\mbox{Ker}(s+\Id_E)
\cr
x\in\mbox{Ker}(s-\Id_E)&\ssi s(x)=x
\cr
x\in \mbox{Ker}(s+\Id_E)&\ssi s(x)=-x
}
$$
et est appelé symétrie par rapport à l'espace $\mbox{Ker}(s-\Id_E)$ parallélement à l'espace $\mbox{Ker}(s+\Id_E)$. 
\bigskip

\Definition [$E$ espace pré-hilbertien] Une symétrie de $E$ est orthogonale $\ssi$ elle satisfait 
$$
\mbox{Ker}(s-\Id_E)\perp\mbox{Ker}(s+\Id_E).
$$


\Remarque : En particulier, une symétrie orthogonale $s$ d'un espace préhilbertien $E$ satisfait 
$$
E=\mbox{Ker}(s-\Id_E)\mathop{\oplus}\limits^\perp\mbox{Ker}(s+\Id_E).
$$

\Propriete [$E$ espace pré-hilbertien, $F\subset E$ sous-espace de dimension~finie]
Si $p$ esy l'unique projection orthogonale sur $F$, alors l'application 
$$
s:x\mapsto 2p(x)-x
$$ 
est une symétrie de $E$ (appelé symétrie orthogonale par rapport à $F$) vérifiant 
$$
F=\mbox{Ker}(s-\Id_E)\qquad\sbox{ et }\qquad \mbox{Ker}(s-\Id_E)\perp\mbox{Ker}(s+\Id_E).
$$ 
Enfin, pour chaque base orthonormale $\{e_1,\cdots e_n\}$ de $F$, on a 
$$
\forall x\in E, \qquad s(x)=2\sum_{k=1}^n\langle x,e_k\rangle e_k-x.
$$

\Remarque{ \it 1} : Si $p$ est la projection orthogonale sur $F$ et si $s$ est la symétrie orthogonale par rapport à~$F$, on retiendra que 
$$
s=2p-\Id_E.
$$
En particulier, si l'on dispose d'un renseignement sur $p$ (resp. $s$), on peut en déduire quelque chose sur $s$ (resp. $p$). 
\bigskip

\Remarque{ \it 2} : Si $s$ est la symétrie orthogonale par rapport à $F$, alors $-s$ est la symétrie orthogonale par rapport à $F^\perp$. 
\bigskip

\Exercice{ako}%
\bigskip 

\Propriete [$E$ espace préhilbertien] Une symétrie $s$ de $E$ est orthogonale $\ssi$ 
$$
\forall x\in E, \qquad \|s(x)\|=\|x\|. 
$$

\Exercice{akp}%














\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\EspacesPréHilbertiens\InégalitéDeCauchySchwarz\Orthonormalisation}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}




















\Section rah, Espaces Euclidiens. 

Dans tout ce chapitre, $E$ désigne un espace euclidien 
(un $\ob R$-espace vectoriel de dimension finie $n$ 
muni d'un produit scalaire $\langle\cdot.\cdot\rangle$ 
et de sa norme associée $\|\cdot\|$).
\bigskip

\Subsection rah, Groupe orthogonal. 

\Definition [$E$ espace euclidien] 
Un endomorphisme $u$ de $E$ est orthogonal si, et seulement si, il conserve le produit scalaire
$$
\forall (x,y)\in E^2, \qquad \langle u(x),u(y)\rangle=\langle x,y\rangle.
$$

\Concept Caractérisation

\Propriete [$E$ espace euclidien] 
Un endomorphisme $u\in\sc L(E)$ est orthogonal si, et seulement si, 
l'une des propositions é\-qui\-va\-len\-tes suivantes est satisfaite : \medskip\noindent
L'endomorphisme $u$ conserve la norme ($\forall x\in E, \quad \|u(x)\|=\|x\|$) \medskip\noindent
L'image par $u$ d'une (de toute) Base OrthoNormale de $E$
est une B.O.N. de $E$. \medskip\noindent
La matrice $A$ de $u$ dans une base orthonormale $B$ de $E$ vérifie $\null^tAA=I_n$. 

\Propriete [$E$ espace euclidien] 
Le déterminant d'un endomorphisme orthogonal de $E$ vaut $1$ ou $-1$. 

\Propriete [$E$ espace euclidien] 
Un endomorphisme orthogonal de $E$ est bijectif, 
sa bijection réciproque est orthogonale, 
la composée de deux endomorphismes orthogonaux de $E$ est orthogonal

\Propriete [$E$ espace euclidien] 
L'ensemble des endomorphismes orthogonaux de $E$ forme un groupe pour la composition des applications noté $(O(E),\circ)$. 
C'est un sous groupe de $({\sc Aut}(E),\circ)$, le groupe des automorphismes de $E$. 

\Propriete [$E$ espace euclidien] 
L'ensemble $\{u\in \sc O(E):\mbox{det\ }u=1\}$ 
des endomorphismes orthogonaux $u$ de $E$ tels que $\mbox{det\ }u=1$ 
est un sous groupe de $(\sc O(E),\circ)$, appellé groupe special orthogonal de $E$, 
que l'on note ${\sc SO}(E)$ ou $\sc O^+(E)$. 

\Exemple. les symétries orthogonales et leurs composées : rotations, 
retournements, symétrie centrale....
\bigskip

\Remarque : aucune projection orthogonale, sauf l'identité, n'est un endomorphisme orthogonal. 

\Definition [$n\ge1$] 
Une matrice $A\in\sc M_n(\ob R)$ est orthogonale $\ssi\NULL^tAA=I_n$. 

\Propriete [$n\ge1$] 
Les matrices $M\in\sc M_n(\ob R)$ orthogonales 
forment un groupe multiplicatif noté $(O(n),\times)$. 
C'est un sous groupe de $({\sc Gl}_n(\ob R), \times)$, 
le groupe des matrices réelles inversibles. 

\Propriete [$n\ge1$] 
L'ensemble $\{M\in \sc O(n):\mbox{det\ }M=1\}$ 
forme un sous groupe de $(\sc O(n),\times)$, appellé groupe special orthogonal, 
que l'on note ${\sc SO}(n)$ ou $\sc O^+(n)$. 

\Concept Lien entre endomorphismes orthogonaux et matrices orthogonales

\Propriete[$B$ base orthonormale de $E$, euclidien]
Les applications 
$$
\eqalign{(\sc O(E),\circ)&\to (\sc O(n),\times)\cr u&\mapsto\mbox{\sc Mat}_Bu}\qquad\mbox{et}\qquad
\eqalign{ ({\sc SO}(E),\circ)&\to ({\sc SO}(n),\times)\cr u&\mapsto\quad\mbox{\sc Mat}_Bu}
$$ 
sont des isomorphismes de groupe. 


\Propriete [$n\ge1$] 
Une matrice $A\in\sc M_n(\ob R)$ est orthogonale $\ssi$ sa transposée $\NULL^tA$ est orthogonale. 

\Propriete [$n\ge1$] 
Une matrice carrée $A\in\sc M_n(\ob R)$ est orthogonale $\ssi$ 
ses colonnes $C_1,\cdots,C_n$ forment une famille orthonormale 
pour le produit scalaire usuel des colonnes 
$$
\langle C_i,C_j\rangle
=\sum_{k=1}^na_{k,i}a_{k,j}=\Q\{
\eqalign{1\sbox{ si }i=j\cr 0\sbox{ si }i\neq j}\W.
$$
Une matrice $A\in\sc M_n(\ob R)$ est orthogonale $\ssi$ ses lignes $L_1,\cdots,L_n$ forment une famille orthonormale 
pour le produit scalaire des lignes 
$$
\langle L_i,L_j\rangle
=\sum_{k=1}^na_{i,k}a_{j,k}=\Q\{
\eqalign{1\sbox{ si }i=j\cr 0\sbox{ si }i\neq j\cr}\W.
$$

\Exemple. La matrice $\pmatrix{0&0&-1&0\cr1&0&0&0\cr0&0&0&-1\cr0&-1&0&0\cr}$ est orthogonale, 
$\pmatrix{1& -1\cr1& 2}$ ne l'est pas. 

\Propriete Les racines du polynôme caractéristique d'un endomorphisme orthogonal 
(resp. d'une matrice orthogonale) sont de module $1$. \pn
Si un endomorphisme orthogonal admet des valeurs propres, 
elles sont nécessairement dans $\{-1,1\}$. 

\Remarque : On rappelle qu'une symétrie de $E$ (i.e. $s\in\sc L(E)$ vérifiant $s^2=\mbox{Id}_E$ satisfait 
$$
E=\underbrace{\{x\in E:s(x)=x\}}_{\mbox{\sevenrm Ker }(s-\mbox{\sevenrm Id}_E)}+\underbrace{\{x\in E:s(x)=-x\}}_{\mbox{\sevenrm Ker }(s+\mbox{\sevenrm Id}_E)}
$$
Une telle symétrie est orthogonale si, et seulement si, les deux ensembles de droites sont orthogonaux. 



\Concept Classification des endomorphismes orthogonaux pour $\mbox{dim}(E)=2$. 

\Propriete [$B$ une base orthonormale de $E$ euclidien, $u\in\sc O(E)$]
Deux cas sont possibles: \medskip\noindent
$\det u=1\Longleftrightarrow \exists \theta\in\ob R, {\sc Mat}_Bu=\pmatrix{\cos\theta&-\sin\theta\cr\sin\theta&\cos\theta\cr}
\Leftrightarrow u$ est une rotation d'angle~$\theta$. 
\medskip\noindent
$\det u=-1\Longleftrightarrow \exists \theta\in\ob R, \sc{Mat}_Bu=\pmatrix{\cos\theta&\sin\theta\cr \sin\theta&-\cos\theta\cr}
\Leftrightarrow u$ est une symétrie~orthogonale par rapport à une droite vectorielle. 

\Concept Classification des endomorphismes orthogonaux pour $\mbox{dim }E=3$

\Propriete [$B$ une base orthonormale de $E$ euclidien, $u\in\sc O(E)$]
Deux cas sont possibles: \medskip\noindent
$\det u=1\Longleftrightarrow\exists B\sbox{ Base orthonormale}, \exists \theta\in\ob R,
{\sc Mat}_Bu~=~\pmatrix{\cos\theta&-\sin\theta&0\cr\sin\theta&\cos\theta&0\cr0&0&1\cr}\Leftrightarrow$ 
$u$ est une rotation d'angle~$\theta$ par rapport à une droite vectorielle
\medskip\noindent
$\det u=-1\Longleftrightarrow\exists B\sbox{ Base orthonormale}, \exists \theta\in\ob R,
{\sc Mat}_Bu~=~\pmatrix{\cos\theta&-\sin\theta&0\cr\sin\theta&\cos\theta&0\cr0&0&-1\cr}\Leftrightarrow u$ 
est la composée (commutative) d'une rotation d'angle $\theta$ autour d'un axe et d'une symétrie~orthogonale 
par rapport au plan orthogonal à cet axe. 
\bigskip

\Subsection rah, Endomorphismes Symétriques.

\Definition[$E$ espace euclidien] un endomorphisme $u\in\sc L(E)$ est symétrique $\ssi$
$$
\forall (x,y)\in E^2, \qquad \langle u(x),y\rangle=\langle x,u(y)\rangle. 
$$
Une matrice $A\in\sc M_n(\ob R)$ est symétrique si, et seulement si, 
$A=\null^{\mbox{\sevenrm t}}A$. 
\bigskip

\Concept Caractérisation

\Propriete [$B$ base orthonormale de $E$ euclidien, $u\in\sc L(E)$]
L'endomorphisme $u$ est symétrique $\Leftrightarrow A:={\sc Mat}_Bu$ est symétrique $\Leftrightarrow A=\null^{\mbox{\sevenrm t}}A$

\Propriete[$E$ euclidien]
L'ensemble $S(E)$ des endomorphismes symétriques de $E$ forme un sous espace vectoriel de $E$ de dimension 
$n(n+1)/2$. \pn
De même, l'ensemble $S_n(\ob R)$ des matrices réelles symétriques forme un sous-espace vectoriel de $\sc M_n(\ob R)$ de dimension $n(n+1)/2$. 
\pn Enfin, pour chaque base orthonormale $B$ de $E$, on a l'isomorphisme suivant 
$$
\eqalign{S_n(E)&\to S_n(\ob R)\cr u&\mapsto{\sc Mat}_Bu}
$$ 

\Propriete [$u$ endomorphisme symétrique de $E$, euclidien] 
On a $E=\mbox{Ker\ }u\mathop{+}\limits^\per\mbox{Im\ }u$, les~deux ensembles de droite étant orthogonaux. 
\bigskip

\Theoreme [$E$ euclidien]
Pour chaque endomorphisme symétrique $u$ de $E$, 
il existe une base orthonormée $B$ de $E$ telle que ${\sc Mat}_Bu$ soit diagonale 
($u$ est diagonalisable dans~une~B.O.N). 

\Theoreme [$n\ge1$]
Pour chaque matrice symétrique $A\in S_n(\ob R)$, 
il existe une matrice orthogonale $P$ et une matrice diagonale $D$ telle que 
$D=\null^{\mbox{t}}PAP=P^{-1}AP$ ($A$~est~diagonalisable dans une base orthonormale). 

\Remarque : les valeurs propres d'un endomorphisme/d'une matrice symétrique 
sont toutes réelles, leur polynôme caractéristique est scindé. 
\bigskip

\Definition[$n\ge1$] 
$A$ est positive (resp. définie positive) $\ssi$ toutes les valeurs propres de $A\in S_n(\ob R)$ sont positives (resp. strictement positives).

\Propriete [$E$ euclidien]
Les espaces propres d'un endomorphisme symétrique 
(resp. d'une matrice symétrique) sont orthogonaux $2$ à $2$ 
pour le produit scalaire de $E$~(resp.~de~$\ob R^n$).

\Subsection rah, Formes bilinéaires et formes quadratiques.

\Remarque : une forme bilinéaire sur $E$ est une application $f:E\times E\to\ob R$, 
linéaire à gauche et à droite, c'est à dire vérifiant 
$$
\forall(x,y,z)\in\ob E^3, \quad\forall(\lambda,\mu)\in\ob R^2, \quad\Q\{
\eqalign{
&f(\lambda x+\mu y,z)=\lambda f(x,z)+\mu f(y,z)\cr
&f(z,\lambda x+\mu y)=\lambda f(z,x)+\mu f(z,y)\cr
}
\W.
$$

\Definition [$B=\{e_1,\cdots,e_n\}$ base de $E$, espace vectoriel, $f:E\times E\to\ob R$ forme bilinéaire]
La matrice de la forme bilinéaire $f$ dans la base $B$ est 
$$
A=\B(f(e_i,e_j)\B)_{1\le i,j\le n}=\pmatrix{f(e_1,e_1)&\cdots&\cdots&\cdots&f(e_1,e_n)\cr
\vdots&&&&\vdots\cr
\vdots&&f(e_i,e_j)&&\vdots\cr
\vdots&&&&\vdots\cr
f(e_n,e_1)&\cdots&\cdots&\cdots&f(e_n,e_n)\cr
}
$$
Si $X$ et $Y$ sont les matrices de $x\in E$ et $y\in E$ dans la base $B$, 
on a 
$$
f(x,y)=\NULL^{\mbox{t}}XAY
$$


\Definition[$E$ espace vectoriel de dimension finie] 
Une forme bilinéaire $f$ sur $E$ est symétrique si, et seulement si,
$$
\forall (x,y)\in E^2,\qquad f(x,y)=f(y,x) 
$$
c'est-à-dire si, et seulement si, sa matrice $A$ dans une (toute) base $B$ de $E$ vérifie $A=\null^tA$. 


\Definition
[$f:E\times E\to\ob R$ forme bilinéaire {\bf symétrique} de $E$, de dimension finie] 
La forme quadratique associée à $f$ est l'application $q:E\to\ob R$ définie par 
$$
\forall x\in E, \qquad q(x):=f(x,x). 
$$
Réciproquement, on dit que $f$ est la forme polaire associée 
à la forme quadratique $q$. 


\Propriete[$E$ de dimension finie, $q:E\to\ob R$ forme quadratique]
$$
\eqalignno{
&\forall x\in E,\forall\lambda\in\ob R, \quad q(\lambda x)=\lambda^2 q(x), &\cr
&\forall (x,y)\in E^2, \quad q(x+y)=q(x)+2f(x,y)+q(y)&\cr}
$$
De même, on a les identités de polarisation suivantes :
$$
\forall (x,y)\in E^2, \quad f(x,y)={q(x+y)-q(x)-q(y)\F2}={q(x+y)-q(x-y)\F4}
$$

\Remarque : chaque forme quadratique est uniquement associée 
à une forme bilinéaire symétrique et réciproquement. 
 
\Propriete [$B$ base de $E$ de dimension finie, $q$ forme quadratique de forme polaire $f$] 
Soit $A$ la matrice de $f$ dans $B$ (qu'on appelle aussi matrice de $q$ dans $B$) alors, pour tout vecteur $x\in E$ de matrice $X$ dans $B$, 
on a 
$$
q(x)=\null^{\mbox{t}}XAX.
$$

\Propriete [$B=\{e_1,\cdots,e_n\}$ base de $E$, de dimension finie, $q$ forme quadratique de forme polaire $f$] 
$$
\forall x=\sum_{i=1}^nx_ie_i,\qquad q(x)=\sum_{1\le i\le n}x_i^2f(e_i,e_i)+2\sum_{i<j}x_ix_jf(e_i,e_j)
$$
Réciproquement, on retrouve la matrice de $q$ à partir de cette expression. 

\Exemple. $q(x,y,z)=x^2+2xy+3y^2-4zx+5z^2+6yz$ est une forme quadratique de $\ob R^3$. 
Sa matrice dans la base canonique est $\pmatrix{1&1&-2\cr1&3&3\cr-2&3&5}$. 

\Remarque : Dans un certain sens (c'est vrai pour $E=\ob R^n$), une forme quadratique 
est un polynôme homogène de degré $2$ en $n$ variables. 
\bigskip
 

\Concept Lien entre endomorphismes symétriques et formes bilinéaires symétriques/quadratiques

\Propriete [$E$ espace vectoriel]
Un endomorphisme symétrique $u$ de $E$ et une forme bilinéaire symétrique 
$f$ de $E$ sont associées à une même matrice symétrique $A$ 
dans une base orthonormale de $E$ $\Leftrightarrow$ 
$$
\forall(x,y)\in E^2, \quad f(x,y)=\langle u(x),y\rangle=\langle x,u(y)\rangle.
$$ 


\Concept Changement de base

\Propriete [$B$ et $B'$ deux bases de $E$, de dimension finie, $q$ une forme quadratique de $E$]
Etant donnés $P={\sc Mat}_{B'B}\mbox{Id}$ et $A$ (resp. $A'$) 
la matrice de $q$ dans $B$ (resp $B'$), on a 
$$
A'=\null^{\mbox{t}}PAP. 
$$
$P$ est la matrice des vecteurs de la base $B'$ décomposés sur la base $B$. 

\Definition [$E$ de dimension finie, $q$ forme quadratique de forme polaire $f$] 
Une famille de vecteur $e_1,\cdots,e_k$ est dite $q$-orthogonale (ou $f$-orthogonale) $\ssi$
$$
\forall(i,j)\in\{1,\cdots,k\}^2, \qquad i\neq j\Rightarrow f(e_i,e_j)=0.
$$


\Theoreme [$E$ de dimension finie, $q$ forme quadratique de $E$] 
Il existe une base orthonormale $B$ de $E$ dans laquelle la matrice de $q$ est diagonale. 

\Remarque : pour chaque forme quadratique $q$, il existe une base orthonormée de $E$,~qui~est ~$q$-orthogonale. 

\Remarque : {\bf Une méthode de construction possible : }\pn 
1) on détermine les valeurs propres $\lambda_i$ de $A$ matrice symétrique associé à $q$. \pn
2) on prend une base orthonormée $B_i$ de chaque espace propre $E_{\lambda_i}$.\pn
3) on orthonormalise $B_i$ en une base orthonormale $B_i'$ de $E_{\lambda_i}$. \pn\nobreak
4) la réunion $B$ des familles $B_i$ forme une base orthonormée de $E$, $q$-orthogonale. 
\bigskip


\vfill\null\eject







\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\EndomorphismesSymétriques\MatricesSymétriques\FormesQuadratiques\EndomorphismesOrthogonaux\MatricesOrthogonales}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}
























\hautspages{Olus Livius Bindus}{Géométrie différentielle}%

\Chapter Courbes, Géométrie différentielle.


\Section Cadre, Cadre d'étude, généralités.
Dans tout ce chapitre, nous allons étudier des paramétrages du type
$$
\left\{\eqalign{
x(t)=...\cr
y(t)=...\cr
z(t)=...
}\W.
\qquad(t\in I).
$$

Dans tout ce chapitre, $\sc P$ désigne un plan affine euclidien : 
$\sc P$ est un espace affine, 
le~vecteur $\vec{MN}$ étant uniquement défini par $N=M+\vec{MN}$ pour $(M,N)\in\sc P^2$, l'ensemble $\vec{\sc P}=\{\vec{MN}:(M,N)\in\sc P^2\}$ 
est un $\ob R$-espace vectoriel pour des lois $+$~et~$.$ (appelé direction de~$\sc P$) de dimension $2$ muni d'un produit scalaire. \pn
Etant donné un repère orthonormé direct $(O,\vec i,\vec j)$ de $\sc P$, 
l'application $\phi$ définie par 
$$
\eqalign{%
	\Phi:\ob R^2&\to\sc P\cr
	(x,y)&\mapsto O+x\vec i+y\vec j\cr
}%
$$
est une isométrie affine permettant d'identifier le plan $\sc P$ 
à $\ob R^2$ muni de sa structure euclidienne orientée canonique. 
\bigskip

$\sc E$ désigne dans ce chapitre un espace affine euclidien orienté 
de dimension~$3$. 
Etant~donné un repère orthonormé direct $(O,\vec i,\vec j,\vec k)$ 
de $\sc E$, on identifiera $\sc E$ à~$\ob R^3$ muni 
de sa structure euclidienne orientée canonique, 
via l'application 
$$
\eqalign{%
	\Phi:\ob R^3&\to\sc E\cr
	(x,y,z)&\mapsto O+x\vec i+y\vec j+z\vec k
}%
$$ 

\Concept Arcs paramètrés

\Definition [$n\ge2$ et $k\in\overline{\ob N}$] 
Un arc paramètré de classe $\sc C^k$ est le couple $\gamma:=(I,f)$ 
formé par un intervalle réel $I$ contenant au moins $2$ points et par une application $f:I\to\ob R^n$ de classe $\sc C^k$. \medskip\noindent
Le support de l'arc $(I,f)$ est alors l'image $\{f(t):t\in I\}$ de l'intervalle $I$ par l'application $f$. 

\Incrust

\Exemple. L'intervalle $I=\ob R$ et l'application $f:t\mapsto(\cos^3t,\sin^3t)$ 
définissent un arc paramétré~$\gamma$ de classe $\sc C^\infty$ à valeurs dans $\ob R^2$, que l'on peut noter plus simplement 
$$
\Q\{
	\eqalign{%
		x(t)&=\cos^3t\cr
		y(t)&=\sin^3t\cr
	}%
\W.
\qquad (t\in\ob R).
\leqno{(\gamma)}
$$ 
Le support de cet arc est l'ensemble des points bleus de la figure \ref{Astroide}. 

\tikzpicture[domain=0:360,smooth,variable=\x,scale=1,baseline=(current bounding box.north)]
	\draw[very thin,color=black!20,step=0.5] (-1.1,-1.1) grid (1.1,1.1);
	\draw[->] (-1.1,0) -- (1.1,0) node[below] {\eightpts$x$};
	\draw[->] (0,-1.1) -- (0,1.1) node[left] {\eightpts$y$};
	\draw[color=blue] plot ({cos(\x)^3},{sin(\x)^3}) ;
	\node [anchor=north,text width=5cm] at (current bounding box.south) {\eightpts\figure [Index=Courbes!Astroide@Astroïde]Astroïde.};
\endtikzpicture

\Concept Arcs géométriques

\Definition [$n\ge2$ et $k\in\ob N\cup\{\infty\}$] 
Un ensemble $A$ est un arc géométrique de classe $\sc C^k$ $\ssi$ il existe un arc paramétré de classe $\sc C^k$ dont $A$ est le support. 
\bigskip

\Exemple. l'ensemble des points bleus de la figure \ref{Astroide} forme un arc géométrique. 

\Section Etude locale, Etude locale des courbes. 

\Subsection Points singuliers, Points singuliers et tri/bi/réguliers.

\Remarque : L'exemple précédent montre que la classe (continue, dérivable, $\sc C^k$) d'un arc n'est pas un bon indicateur de sa régularité. 
Les définitions que nous allons introduire maintenant permettent de mieux caractériser cette régularité. 
\bigskip

\Definition [$(I,f)$ arc paramètré de classe $\sc C^k$ autour de $t_0\in I$] 
Lorsque les nombres suivants sont définis, on note (officieux) : \medskip\noindent
$p$ le plus petit entier $n\in\{1,\cdots, k\}$ tel que $f^{(n)}(t_0)\neq 0$, 
\medskip
\noindent 
$q$ le plus petit entier $n\in\{p+1,\cdots,k\}$ tel que $\{f^{(p)}(t_0),f^{(n)}(t_0)\}$ soit libre 
\medskip
\noindent 
$r$ le plus petit entier $n\in\{q+1,\cdots,k\}$ tel que $\{f^{(p)}(t_0),f^{(q)}(t_0),f^{(n)}(t_0)\}$ soit libre. 

\Remarque : lorsque les nombres $p$, $q$ et $r$ sont définis, leur parité détermine complètement le comportement qualitatif de l'arc $(I,f)$ au voisinage du point $M(t_0)$ dans le repère $\b(O,\vec i,\vec j,\vec k\b)$ avec 
$$ 
O=f(t_0), \qquad \vec i=f^{(p)}(t_0), \qquad\vec j=f^{(q)}(t_0)\quad\mbox{et}\quad\vec k=f^{(r)}(t_0).
$$ 
En particulier, on a 
$$
f(t_0+h)=O+{h^p\F p!}\b(1+o(1)\b)\vec i+{h^q\F q!}\b(1+o(1)\b)\vec j
+{h^r\F r!}\b(1+o(1)\b)\vec k\qquad(h\to0).
$$ 
\Demonstration. Si $(I,f)$ est un arc de classe $\sc C^n$ en un point $t_0\in I$, il résulte de la formule de Taylor-Lagrange que 
$$
f(t_0+h)=f(t_0)+hf'(t_0)+{h^2\F2}f''(t_0)+{h^3\F6}f^{(3)}(t_0)+\cdots+f^{(n)}(t_0){h^n\F n!}+o(h^n)\qquad(h\to0). 
$$
Il suffit alors de remarquer que le terme ${h^k\F k!}f^{(k)}(t_0)$ peut être rangé dans : \smallskip\noindent
le terme $o(h^p)\vec i$ pour $p<k<q$ en tant que multiple de $\vec i$ négligeable devant $h^p$. \smallskip\noindent
les termes $o(h^p)\vec i$ et $o(h^q)\vec j$ pour $q<k<r$ en tant que combinaison linéaire de $\vec i$ et $\vec j$ à coefficients négligeables devant $h^q$. \smallskip\noindent
les termes $o(h^p)\vec i$, $o(h^q)\vec j$ et $o(h^r)\vec k$ pour $r<k$ en tant que combinaison linéaire de $\vec i$, $\vec j$ et $\vec k$ à coefficients négligeables devant $h^r$. 
\CQFD

\Definition 
La droite $(O,\vec i)$ est appelée droite tangente à l'arc $(I,f)$ au point $O$. \PAR\noindent
Le plan $(O,\vec i,\vec j)$ est appelé plan osculateur de l'arc $(I,f)$ en $O$. 

\Concept Point singulier

\Definition [$(I,f)$ arc paramètré de classe $\sc C^1$]
L'arc $(I,f)$ est singulier en un point $t\in I \ssi f'(t)=0$. 

\Concept Point régulier

\Definition [$(I,f)$ arc paramètré de classe $\sc C^1$]
L'arc $(I,f)$ est régulier en un point $t\in I \ssi f'(t)\neq 0$. \PAR\noindent
L'arc $(I,f)$ est régulier $\ssi$ l'arc $(I,f)$ est régulier en chaque point $t\in I$, i.e. 
$$
\forall x\in I, \qquad f'(t)\neq 0.
$$

\Concept Point bi-régulier

\Definition [$(I,f)$ arc paramètré de classe $\sc C^2$]
L'arc $(I,f)$ est birégulier en un point $t\in I\ssi\{f'(t), f''(t)\}$ est libre. \medskip\noindent 
L'arc $(I,f)$ est birégulier $\ssi$ l'arc $(I,f)$ est birégulier en chaque point $x\in I\ssi$
$$
\forall x\in I,\quad \mbox{la famille }\{f'(t),f''(t)\} \mbox{ est libre}.
$$

\Concept Point tri-régulier

\Definition [$(I,f)$ arc paramètré de classe $\sc C^3$]
L'arc $(I,f)$ est trirégulier en un point $t\in I\ssi\{f'(t), f''(t),f'''(t)\}$ est libre. \medskip\noindent
L'arc $(I,f)$ est trirégulier $\ssi$ l'arc $(I,f)$ est trirégulier en chaque point $x\in I\ssi$
$$
\forall x\in I,\quad \mbox{la famille }\{f'(t),f''(t),f''(t)\} \mbox{ est libre}.
$$

\Exercice{PTakq}%

\Subsection Frenet, Repère de Frenet, abscisse curviligne, courbure.

\Definition [$t\in I$ point régulier d'un arc paramètré $(I,f)$ de classe $\sc C^1$]
Le vecteur unitaire tangent en $M=f(t)$ à l'arc $(I,f)$, orienté par le sens de parcours, est le vecteur 
$$
\vec T:={f'(t)\F\Norme{f'(t)}}\eqdef{defT}
$$

\Rappels. Si $(I,f)$ est un arc paramètré de classe $\sc C^1$, alors l'abscisse curviligne d'un point $M=f(t)$ à partir de l'origine $M_0=f(t_0)$ 
est le nombre réel $s(t)$ défini par 
$$
s(t)=\int_{t_0}^t\Norme{f'(u)}\d u\qquad (t\in I). \eqdef{Defs}
$$
La fonction abscisse curviligne $t\mapsto s(t)$ est croissante, de classe $\sc C^1$ sur $I$ et sa dérivée est 
$$
{\d s\F\d t}=\Norme{f'(t)}\qquad(t\in I).
$$
Ainsi, si l'arc $(I,f)$ est régulier, l'application $t\mapsto s(t)$ est un difféomorphisme croissant de classe $\sc C^1$ et 
$$
{\d\F\d s}\vec{OM}={\d \F\d s}f(t)=f'(t){\d t\F\d s}={f'(t)\F\Norme{f'(t)}}=\vec T\qquad (t\in I).
$$




\Definition [$(I,f)$ arc paramètré {\bf plan} régulier en $t\in I$] 
Le vecteur unitaire normal à l'arc $(I,f)$, orienté par le sens de parcours, est l'unique vecteur unitaire $\vec N$ vérifiant
$$
(\vec T,\vec N)\equiv{\pi\F 2}\quad[2\pi].\eqdef{DefN}
$$ 
Le repère de Frenet de $(I,f)$ en $M=f(t)$ est le repère $(M,\vec T, \vec N)$. 

\Propriete [$(I,f)$ arc paramètré {\bf plan} régulier, de classe $\sc C^2$ en $t\in I$] 
Il existe un unique nombre $\gamma\in\ob R$ tel que 
$$
{\d \vec T\F\d s}={1\F\|f'(t)\|}{\d\vec T\F\d t}=\gamma\vec N.
$$
Ce nombre est appelé courbure de l'arc $(I,f)$ en $M=f(t)$ et satisfait 
$$
\gamma={\d \vec T\F\d s}.\vec N={1\F\Norme{f'(t)}}{\d\vec T\F\d t}.\vec N.
$$

\Propriete 
L'arc {\bf plan} $(I,f)$ est biregulier en $t\ssi\gamma\neq0$. 

\Remarque : La courbure $\gamma$ est nulle en un point d'inflection. 
\bigskip

\Remarque : La courbure $\gamma$ pouvant être négative, on appelle courbure géomètrique de l'arc le nombre positif $|\gamma|$.  
\bigskip

\Definition [$(I,f)$ arc {\bf plan} birégulier en $t\in I$]
Le rayon de courbure de l'arc $(I,f)$ en $M$ est le nombre 
$$
\ds r:={1\F \gamma}.
$$ 
Le centre de courbure de l'arc $(I,f)$ en $M$ est le point $C$ implicitement défini par l'égalité 
$$
\vec MC=r\vec N.
$$ 
Le cercle de courbure ou cercle osculateur de l'arc $(I,f)$ en $M$ est le cercle $\sc C(C,r)$ de centre $C$ et de rayon $r$.

\centerline{%
\tikzpicture[domain=0:360,smooth,variable=\x,scale=1]%,baseline=(0,0)] %(current bounding box.north)]
	\scope
	\clip (-1.2,-1.2) rectangle (1.5,1.5);
	\draw[very thin,color=black!20,step=0.5] (-1.1,-1.1) grid (1.5,1.5);
	\draw[->] (-1.1,0) -- (1.1,0) node[above] {\eightpts$x$};
	\draw[->] (0,-1.1) -- (0,1.1) node[right] {\eightpts$y$};
	\draw[color=blue] plot ({cos(\x)^3},{sin(\x)^3}) ;
	\draw[color=red] (1.41421,1.41421) circle (1.5 cm);
	\fill [color=red] (1.41421,1.41421) circle (1pt);
	\node [anchor=north,color=red] at (1.41421,1.41421) {\eightpts$C$};
	\draw [color=green](0.3535,0.3535) -- (1.41421,1.41421) node[midway,anchor=south,color=green]{\eightpts$R$};
	\draw [color=black,->](0.3535,0.3535)-- (-0.3535,1.0605)node[pos=1,anchor=north,color=black]{\eightpts$\vec T$};
	\draw [color=black,->](0.3535,0.3535)-- (-0.3535,-0.3535)node[pos=1,anchor=north,color=black]{\eightpts$\vec N$};
	\fill [black] (0.3535,0.3535) circle (1pt);
	\node [anchor=south,color=blue] at (0.3535,0.3535) {\eightpts$M$};
	\endscope
\endtikzpicture}%
\Figure [Label=Courbure,Index=Courbes!Curbure@Courbure] Rayon, centre et cercle de courbure.

\Remarque : Le vecteur $\vec T$ étant unitaire, il existe un unique angle $\alpha\in\Q]-\pi,\pi\W]$ tel que
$$
\vec T=(\cos\alpha,\sin\alpha)\qquad\mbox{et on a alors}\qquad\vec N=(-\sin\alpha,\cos\alpha). 
$$
Autrement dit, c'est très facile de déduire $\vec N$ de $\vec T$ et réciproquement. 
\bigskip

\Remarque : On a également 
$$
{\d \vec N\F\d s}={1\F\|f'(t)\|}{\d\vec N\F\d t}=-\gamma\vec T\qquad\mbox{et}\qquad 
\gamma={1\F\|f'(t)\|^3}\det\b(f'(t),f''(t)\b)
$$

\Subsection 3d, Repère de Frenet, torsion.
\bigskip

\Remarque : Pour les arc $f:I\to\ob R^3$ de classe $\sc C^1$, on définit le vecteur tangent~$\vec T$ à l'arc $(I,f)$ en $M=f(t)$ 
en posant \eqref{defT} et l'abscisse curviligne $s(t)$ de $M=f(t)$ à~partir de $M(t_0)$ en posant 
$$
s(t)=\int_{t_0}^t\|f'(u)\|\d u. 
$$
Par contre, on ne peut évidemment 
plus définir le vecteur $\vec N$ en posant \eqref{DefN}. 
\bigskip

\Definition [$f:I\to\ob R^3$ arc de classe $\sc C^2$, biregulier en $t\in I$] 
Le trièdre de Frenet en $M=f(t)$ est le repère orthonoré direct 
$(M,\vec T,\vec N,\vec B)$ définit par 
$$
\vec T:={f'(t)\F\|f'(t)\|},\qquad \vec N=\vec B\wedge\vec T\qquad\mbox{et}\qquad
\vec B={f'(t)\wedge f''(t)\F\|f'(t)\wedge f''(t)\|}. 
$$ 
Le vecteur $N$ est appelé vecteur normal principal de $(I,f)$ en $M$. \pn
Le vecteur $B$ est appelé vecteur binormal de $(I,f)$ en $M$. 
\bigskip

\Remarque : Si $(I,f)$ est un arc $\sc C^2$ birégulier en $t\in I$, 
le plan osculateur de $(I,f)$ au~point $M=f(t)$ est le plan $(M,\vec T,\vec N)$, qui~est orthogonal à $\vec B$. 
\bigskip

\Definition [$f:I\to\ob R^3$ arc de classe $\sc C^3$, birégulier en $t\in I$]
Si $(M,\vec T,\vec N,\vec B)$ est son trièdre de Frenet en $M=f(t)$, alors il existe un unique
$\gamma>0$ et un unique $\tau\in\ob R$ tels que 
$$
{\d \vec T\F\d s}={1\F\|f'(t)\|}{\d\vec T\F\d t}=\gamma\vec N\qquad\mbox{et}
\qquad{\d \vec B\F\d s}={1\F\|f'(t)\|}{\d\vec B\F\d t}=-\tau\vec N
$$
Le nombre $\gamma$ s'appelle courbure de l'arc $(I,f)$ en $M$. \pn
Le nombre $\tau$ s'appelle torsion de l'arc $(I,f)$ en $M$. 


\Propriete [$f:I\to\ob R^3$ arc de classe $\sc C^3$, birégulier en $t\in I$]
L'arc $(I,f)$ est triregulier en $t\ \Longleftrightarrow\ \tau\neq0$. 

\Definition [$(I,f)$ arc birégulier (resp. trirégulier) en $t\in I$], 
Le rayon de courbure (resp. de torsion) de l'arc $(I,f)$ en $M=f(t)$ est le nombre 
$$
\ds r_c={1\F \gamma}\qquad\Q(\mbox{resp. } r_t={1\F\tau}\W).
$$ 
Le point $C$ défini par $\vec MC=r_c\vec N$ est appelé centre de courbure de l'arc $(I,f)$ en $M$. \pn
Le cercle $\sc C(C,r)$ de centre $C$ et de rayon $r$ est appelé cercle de courbure 
ou cercle osculateur de l'arc $(I,f)$ en $M$. 
\bigskip

\Remarque : Le plan $(M,\vec T,\vec N)$ est le plan osculateur de l'arc $(I,f)$ en $M$. \pn
Le plan $(M, \vec N,\vec B)$ est appelé plan normal à l'arc $(I,f)$ en $M$.\pn
Le plan $(M, \vec T,\vec B)$ est appelé plan réctifiant de l'arc $(I,f)$ en $M$. 

\centerline{%
	\Image [Height=4cm,Box=hbox]{\imageFolder/Pdf/hel.pdf}%
	\Image [Height=4cm,Box=hbox]{\imageFolder/Pdf/hell.pdf}%
}%
\Figure [Index=Courbes!Helice@Hélice] Allure et comportement local de l'hélice $t\mapsto \Q(t,\cos\pi t,\sin\pi t\W)$.

\Definition Un arc régulier $(I,f)$ de classe $\sc C^1$ est appelé une hélice 
si ses tangentes font un angle constant $V\in[0,\pi]$ avec une direction fixe, 
cette direction étant appelée axe de l'hélice. 
\bigskip

\Remarque : Le cas $V\in\{0,\pi\}$ est peu intéressant (segment de droite) 
de même que le cas $V=\pi/2$ (courbe plane). 
\bigskip

\Propriete [$(I,f)$ arc trirégulier]
Les propriétés suivantes sont équivalentes : \pn 
a) les tangentes de $(I,f)$ font un angle constant avec une direction fixe (l'arc est une hélice). \pn
b) les normales principales $\vec N$ de $(I,f)$ sont parallèles à un plan fixe. \pn
c) Les binormales $\vec B$ de $(I,f)$ font un angle constant avec une direction fixe. \pn
d) Le quotient de la torsion par la courbure ${\tau\F \gamma}$ est une fonction constante. 
\bigskip

\Remarque : Quelques formules pour calculer courbure et torsion : 
$$
\eqalign{
\gamma&={\|f'(t)\wedge f''(t)\|\F \|f'(t)\|^3}\cr
\tau&={\det\b(f'(t),f''(t),f'''(t)\b)^{\strut}\F\|f'(t)\wedge f''(t)\|^2}\cr
}
$$


\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\AbscisseCurviligne,\RepèreDeFrenet,\Courbure,\Torsion, \Hélices}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}



\Section gah, Enveloppes, dévelopantes et dévelopées. 
\bigskip

Dans cette partie, on se place dans un plan affine $\sc P$ 
identitié à $\ob R^2$ pour simplifier. 
\bigskip


\Subsection enveloppe, Enveloppe d'une famille de droites dans le plan. 

\noindent{\bf But :} Etant donné un intervalle $I$ et, pour chaque $t\in I$, 
une droite $\Delta_t$ de $\sc P$ 
d'équation cartésienne
$$
a(t)x+b(t)y=c(t),\leqno{(\Delta_t)}
$$
nous cherchons un arc paramétré $\eqalign{\Gamma:I&\to\sc P\cr t&\mapsto M(t)}$ 
tel que $\Delta_t$ soit 
la tangente à l'arc $\Gamma$\vskip-1em\noindent au point $M(t)=\b(x(t),y(t)\b)$. 
\bigskip

\Definition Un tel arc $(I,\Gamma)$ est appelé une enveloppe 
de la famille de droite~$(\Delta_t)_{t\in I}$. \pn
Le point $M(t)$ est appelé point caractéristique de la droite $\Delta_t$. 
\bigskip

\Theoreme 
Soit $(\Delta_t)_t\in I$ une famille de droite d'un plan $\sc P$, 
d'équations cartésiennes 
$$
a(t)x+b(t)y+c(t)=0,\leqno{(\Delta_t)}
$$
dans un repère $(O,\vec i,\vec j)$ de $\sc P$, telle que $a$, $b$ et $c$ 
soient de classe $\sc C^1$ sur $I$. \pn
Si $t\mapsto M(t)$ est une enveloppe de la famille de droite $(\Delta_t)_{t\in I}$, 
pour~$t\in I$, le point $M(t)=\b(x(t),y(t)\b)$ est solution du système
$$
\Q\{\eqalign{a(t)x+b(t)y&=c(t),\cr
a'(t)x+b'(t)y&=c'(t).\cr}\W. \eqdef{syster}
$$
Réciproquement, si $a$, $b$ et $c$ vérifient la condition 
$$
\forall t\in I, \qquad \det\pmatrix{a(t)&b(t)\cr a'(t)&b'(t)\cr}\neq 0, \eqdef{conde}
$$
alors, pour $t\in I$, le système \eqref{syster} 
admet une unique solution $M(t)=\b(x(t),y(t)\b)$ : 
$$
\Q\{\eqalign{&x(t)={b'(t)c(t)-b(t)c'(t)\F a(t)b'(t)-a'(t)b(t)}\cr
&y(t)={-a'(t)c(t)+a(t)c'(t)\F a(t)b'(t)-a'(t)b(t)}}\W. \qquad(t\in I). \eqdef{defxy}
$$ 
De plus, si les fonctions $x$ et $y$ définies par \eqref{defxy} 
sont de classe $\sc C^1$ sur $I$ et si 
$$
(x'(t),y'(t))\neq (0,0)\qquad(t\in I), \eqdef{condef}
$$
l'application $t\mapsto M(t)$ est une enveloppe de la famille de droite $(\Delta_t)_{t\in I}$. 

\Remarque : Si $a$, $b$, et $c$ sont $\sc C^2$ sur $I$, 
les fonctions $x$ et $y$ définies par \eqref{defxy} sont $\sc C^1$. 

\centerline{%
	\Image [Height=4cm]{\imageFolder/Pdf/Env.pdf}%
}%
\Figure [Index=Courbes!Enveloppe]  Enveloppe de la famille de droite $\Delta_t:x\cos t+y\sin t=\cos t\sin t$.

\Remarque : Si on a \eqref{conde}, il existe au plus une enveloppe de la famille 
de droites~$(\Delta_t)_{t\in I}$. 
\bigskip

\Remarque : La condition \eqref{conde} induit que 
la droite vectorielle $D_t:a(t)x+b(t)y=0$ (qui est parallèle à $\Delta_t$) ``tourne'' 
et réciproquement. 
\bigskip

\noindent{\bf Méthode.} 1) on regarde si \eqref{conde} est vérifié. \pn
2) si oui, on détermine $x$ et $y$ en résolvant le système \eqref{syster} 
(ou en utilisant \eqref{defxy}). \pn
3) on regarde si $x$ et $y$ sont $\sc C^1$ sur $I$ 
et si \eqref{condef} est satisfaite. \pn
4) si oui, c'est gagné. 
\bigskip

\Remarque : Etant donnée une famille de droites $(\Delta_u)_{u\in E}$, 
on appelle également enveloppe de droite tout arc (géométrique ou paramétré) 
régulier $\Gamma$ tel que l'ensemble des tangentes à $\Gamma$ 
soit l'ensemble $\{\Delta_u:u\in E\}$. 
\bigskip

\Section dev, Développée. 

\noindent{\bf But. }soient $I$ un intervalle et $f:t\mapsto\b(\alpha(t),\beta(t)\b)$ 
un arc plan régulier~et~$\sc C^1$~sur~$I$. Pour~$t\in I$,
on note $\Delta_t$ la droite normale à l'arc $(I,f)$ en $f(t)$ 
(la droite perpendiculaire à la tangente de $(I,f)$ en $f(t)$, passant par $f(t)$) 
$$
\underbrace{-\beta'(t)}_{a(t)}x+\underbrace{\alpha'(t)}_{b(t)}y=
\underbrace{-\beta'(t)\alpha(t)+\alpha(t)\beta(t)}_{c(t)}.\leqno{(\Delta_t)}
$$ 
On cherche l'enveloppe de la famille de droite $(\Delta_t)_{t\in I}$, 
c'est à dire un arc $\eqalign{\Gamma:I&\to\sc P\cr t&\mapsto M(t)}$ \vskip-1.2em\noindent
tel que $\Delta_t$ soit 
la tangente à l'arc $(I,f)$ au point $M(t)=\b(x(t),y(t)\b)$. 
\bigskip\goodbreak

\Definition Un tel arc $(I,\Gamma)$ est appelé une développée de l'arc $(I,f)$. 
\bigskip

\Remarque : Une développée d'un arc $(I,\Gamma)$ est l'enveloppe 
de ses droites normales. \bigskip

\centerline{%
	\Image [Height=3cm]{\imageFolder/Pdf/Card.pdf}%
}%
\Figure [Index=Courbes!Developpee@Développée!Cardioide@Cardioïde] Développée de la cardioïde d'équation $r(\theta)=1+\cos\theta$.

\centerline{%
	\Image [Height=3cm]{\imageFolder/Pdf/Dell.pdf}%
}%
\Figure [Index=Courbes!Developpee@Développée!Cardioide@Cardioïde] Développée de l'ellipse d'équation $x^2+{y^2\F 3}=1$.

\Theoreme [$f:t\mapsto(\alpha(t),\beta(t))$ arc plan birégulier de classe $\sc C^2$ sur $I$]
L'arc $(I,f)$ possède au plus une développée. Si elle existe, c'est l'arc 
$\eqalign{\Gamma:&I\to\sc P\cr t&\mapsto C(t)}$ \vskip-1em\noindent où $C(t)$~est 
le centre de courbure de l'arc $(I,f)$ en $f(t)$. 
$$
C(t)=f(t)+r_c(t)\vec N(t)\qquad(t\in I). 
$$

\Subsection dev, Développante. 
\bigskip

\noindent{\bf But. }soient $I$ un intervalle et $f:I\to\sc P$ 
un arc plan régulier~et~$\sc C^1$~sur~$I$. 
Pour~$t\in I$, on note $\Delta_t$ la tangente à l'arc $(I,f)$ en $f(t)$. 
On cherche un arc $\Gamma:t\mapsto M(t)$ régulier de classe $\sc C^1$ tel que, pour chaque $t\in I$, 
la droite $\Delta_t$ soit la normale au point $M(t)$ de l'arc $\Gamma$. 
Autrement dit, on cherche un arc $\eqalign{\Gamma:&I\to\sc P\cr t&\mapsto M(t)}$ 
de classe $\sc C^1$ régulier tel que la développée de $(I,\Gamma)$ soit $(I,f)$. 
\bigskip

\Definition Un tel arc $(I,\Gamma)$ est appelé une développante de l'arc $(I,f)$. 
\bigskip

\Theoreme [$f:I\to\sc P$ arc plan birégulier de classe $\sc C^2$ et $O\in\sc P$] 
Les développantes de $(I,f)$ sont de la forme $\Gamma_k:t\mapsto M(t)$ avec 
$$
\vec {OM(t)}=\vec{Of(t)}+(k-s(t))\vec T
$$ 
où $s(t)$ est l'abscisse curviligne. De plus, si $s(t)\neq k$ pour $t\in I$, 
l'arc $\Gamma_k$ est une développante de $(I,f)$. 

\Remarque : Les développantes d'une même courbe sont dites ``parallèles''. 
\bigskip

\centerline{%
	\Image[Height=4cm]{\imageFolder/Pdf/Dcercle.pdf}%
}%
\Figure [Index=Courbes!Developpante@Développante!de cercle] Développantes de cercle.


\Subsection rol, Roulement sans glissement.
\bigskip

\noindent{\bf But. }soit $I$ un intervalle, $(I,f)$ et $(I,g)$ deux arcs. 
On aimerait modéliser le roulement sans glissement d'un arc ``mobile'' $(I,f)$ (roulante)
sur un arc ``fixe'' $(I,g)$ (base). 
\bigskip
Pour cela, on transforme l'arc $(I,f)$ à l'aide d'un déplacement 
(composée d'une translation et d'une rotation). 
\medskip
On modèlise le contact entre les deux courbes en un point $M$ 
par le fait que le repère de Frenet de l'arc $(I,f)$ en $M$ 
est égal au repère de Fernet de l'arc $(I,g)$ en ce même point. 
\bigskip
Si $(I,f)$ et $(I,g)$ coincident respectivement en deux points $M_0$ et $M$ 
au cours du roulement, L'abscisse curviligne de $M_0$ à $M$ sur $(I,f)$ est égale à celle sur $(I,g)$. 
\bigskip

\centerline{%
	\Image[Height=4cm]{\imageFolder/Pdf/Cycloide.pdf}%
}%
\Figure [Index=Courbes!Cycloide@Cycloïde] Cycloïde $t\mapsto(t-\sin t,1-\cos t)$.

\Subsection eq, Equations cartésiennes des courbes planes.
\bigskip

\Definition [$k\in\overline{\ob N}$ et $(O,\vec i,\vec j)$ repère de $\sc P$] 
Une partie $\sc C$ du plan $\sc P$ est appelée courbe de classe $\sc C^k$ si  
et seulement s'il existe une fonction $F:U\to\ob R$ de classe $\sc C^k$ 
sur un ouvert $U$ de $\ob R^2$ telle que 
$$
M\in\sc C\Longleftrightarrow F(x,y)=0, 
$$
où $(x,y)$ désignent les coordonnées du point $M$ dans le repère $(O,\vec i,\vec j)$. 
\medskip

\Remarque : On dit alors que la courbe $\sc C$ est d'équation cartésienne 
$$
\Q\{\eqalign{(x,y)\in U\cr F(x,y)=0\cr}\W. \eqdef{equa}
$$
dans le repère $(O,\vec i,\vec j)$. {\it En pratique on a\/ $U=\ob R^2$ et on écrit juste $F(x,y)=0$. }
\bigskip

\Definition Soit $\sc C$ une courbe de classe $\sc C^k$ d'équation \eqref{equa} 
dans un repère $(O,\vec i,\vec j)$ et soit $M$ un point de $\sc C$ 
de coordoonées $(x,y)$ dans $(O,\vec i,\vec j)$. Alors, on dit que $M$ 
est un point régulier de $\sc C$ si, et seulement si, 
$$
\underbrace{\Q({\partial F\F\partial x}(x,y),{\partial F\F\partial x}(x,y)\W)}_{\vec{\grad}\ F(x,y)}\neq(0,0).
$$ 
On dit que $M$ est un point singulier de $\sc C$ dans le cas contraire. 
\bigskip

\Remarque : Pour avoir affaire à de ``vraies'' courbes $\sc C$, on demandera en général 
que tous les points de $\sc C$ soient réguliers. 
\bigskip

\noindent{\bf Problématique : }est-il possible de passer d'une équation cartésienne 
à une paramétrisation et réciproquement ? {\bf Réponse :}
localement à certains points : oui. 
\medskip

\Concept Théorème des fonctions implicites

\Theoreme [$(O,\vec i,\vec j)$ repère de $\sc P$, $k\in\overline{\ob N}^*$ et 
$\sc C$ courbe de classe $\sc C^k$ d'équation cartésienne ${F(x,y)=0}$ dans $\sc P$] 
Pour chaque point~régulier~$M$, il existe $r>0$ tel que $\sc C\cap B(M,r)$ 
soit un arc géométrique régulier de classe $\sc C^k$. 
\medskip\noindent
De plus, la tangente à $\sc C$ en $M$ de coordoonnées $(x_0,y_0)$ est d'équation cartésienne 
$$
{\partial F\F\partial x}(x_0,y_0)(x-x_0)+{\partial F\F\partial y}(x_0,y_0)(y-y_0)=0,
$$
le vecteur $\vec{\grad}\ F(x_0,y_0)$ étant normal à la courbe $\sc C$ en $M$.
\bigskip

\Remarque : Autrement dit, il existe $\epsilon>0$ 
et un arc régulier $f:\Q]-\epsilon,\epsilon\W[\to\ob R^2$~de~classe~$\sc C^k$ 
tel que : $N\in \sc C\cap B(M,r)\Leftrightarrow $ il existe $t\in\Q]-\epsilon,\epsilon\W[$ 
tel que $N$ soit de coordonnées $f(t)$. 
\bigskip

\Remarque : Inversement, si l'arc $M:I\mapsto\b(f(t),g(t)\b)$ de classe $\sc C^k$ pour $k\ge1$ 
est régulier en $t_0$, on peut en obtenir une équation cartésienne localement 
au point $M(t_0)$. 
\bigskip



\Section cp, Courbes polaires. 
\bigskip

\noindent
Dans cette section, le plan $\sc P$ est muni d'un repère orthonormé fixe
$(O,\vec i,\vec j)$. 
\bigskip

\Subsection der, Repère mobile des coordonnées polaires et repère de Frenet. 
\bigskip

\Definition Pour $\theta\in\ob R$, on pose $\vec u_\theta:=\cos\theta\vec i+\sin\theta\vec j$ et 
$\vec v_\theta:=-\sin\theta\vec i+\cos\theta\vec j$ et on appelle repère mobile 
des coordonnées polaires le repère $(O, \vec u_\theta,\vec v_\theta)$. 
\bigskip

\Remarque : Quand il n'y a pas d'ambiguité, on note $\vec u$ et $\vec v$ plutôt 
que $\vec{u_\theta}$ et $\vec{v _\theta}$. \pn
Le repère $(O,\vec u,\vec v)$ est orthonormé et l'on a 
$$
\widehat{(\vec i,\vec u)}=\theta,\qquad \widehat{(\vec u,\vec v)}={\pi\F2}, \qquad {\d \vec u\F\d\theta}=\vec v
\quad\mbox{ et }
\quad{\d \vec v\F\d\theta}=-\vec u. 
$$

\Remarque : Les axes $(O,\vec i)$, $(O,\vec u)$ et $(O,\vec v)$ sont respectivements 
appelés axe~polaire, axe radial et axe orthoradial. 
Le point $O$ est appelé ``pôle''. 
\bigskip

\Definition On dit qu'un point $M$ de $\sc P$ 
admet $(r,\theta)$ pour coordonnées polaires dans le repère $(O, \vec i,\vec j)$ si, 
et~seulement~si, $\vec {OM}=r\vec u_{\theta}$
\bigskip

\Remarque : Soit $I$ un intervalle et $\rho:I\to\ob R$. 
On dit qu'un arc géométrique $\sc C$ est une courbe d'équation polaire 
$$
r=\rho(\theta)\qquad(\theta\in I)
$$
si, et seulement si, $\vec{OM}=\rho(\theta)\vec{u_\theta}\ \,(\theta\in I)$ 
est une paramétrisation de~$\sc C$. 
\bigskip

\Propriete [$I$ intervalle, $\rho\in\sc C^1(I,\ob R)$ et $f:\theta\mapsto \rho(\theta)\vec u$]
Alors, on a 
$$
f'(\theta)=\rho'(\theta)\vec u+\rho(\theta)\vec v \quad\mbox{et}\quad \|f'(\theta)\|=\sqrt{\rho'(\theta)^2+\rho(\theta)^2}
\qquad(\theta\in I).
$$ 
L'arc $(I,f)$ est régulier en 
$\theta \Leftrightarrow f'(\theta)\neq 0 \Leftrightarrow \b(\rho(\theta),\rho'(\theta)\b)
\neq (0,0)$. Dans ce cas, on a 
$$
\vec T={\rho'(\theta)\vec u+\rho(\theta)\vec v\F\sqrt{\rho'(\theta)^2+\rho(\theta)^2}}
\qquad\mbox{et}\qquad \vec N={-\rho(\theta)\vec u+\rho'(\theta)\vec v\F\sqrt{\rho'(\theta)^2+\rho(\theta)^2}}
$$ 
Si de plus, $\rho$ est de classe $\sc C^2$, l'arc $(I,f)$ est de classe $\sc C^2$ et on a 
$$
f''(\theta)=\b(\rho''(\theta)-\rho(\theta)\b)\vec u+2\rho'(\theta)\vec v \quad\mbox{et}\quad 
\gamma={\rho(\theta)^2+2\rho'(\theta)^2-\rho(\theta)\rho''(\theta)\F\b(\rho'(\theta)^2+\rho(\theta)^2\b)^{3/2}}
$$

\Remarque : Soit $\sc T$ la tangente en un point régulier $\theta\in I$ 
d'un arc polaire $\theta\mapsto\rho(\theta)\vec{u_{\theta}}$. Alors, 
$\rho'(\theta)\vec u+\rho(\theta)\vec v$ est un vecteur directeur de $\sc T$ et 
$-\rho(\theta)\vec u+\rho'(\theta)\vec v$ est un vecteur normal simple de $\sc T$. 
\bigskip

\Subsection cp, Bestiaire. 
\bigskip

\Concept Droites ne passant pas par l'origine. 

Pour chaque droite $\sc D$ ne passant pas par $O$, $\exists!(a,b)\neq (0,0)$ tel que 
$\sc D$ soit d'équation cartésienne $ax+by=1$. Et alors, 
$\sc D$ est d'équation polaire 
$$
r={1\F a\cos(\theta)+b\sin\theta}\eqdef{eq}
$$
Inversement, \eqref{eq} définit une droite $\sc D$ d'équation $ax+by=1$ ne passant pas pas $O$. 
\bigskip

\Concept Cercle passant par l'origine

\Propriete $\sc C$ est un cercle de centre $\ds\Q({a\F2},{b\F2}\W)$ passant par $O\Longleftrightarrow$ 
$\sc C$ est d'équation polaire 
$$
r=a\cos\theta+b\sin\theta
$$

\Concept Coniques 

\Propriete [$\sc C_e(O,\sc D)$ conique de foyer $O$, d'excentricité $e$ et de 
droite directrice $\sc D$]
On note $H$ le projeté orthogonal de $O$ sur $\sc D$, 
$\theta_0:=\widehat{(\vec i,\vec{OH})}$, la conique $\sc C_e$ est d'équation polaire 
$$
r={e\|\vec {OH}\|\F1+e\cos(\theta-\theta_0)}
$$
On appelle paramètre de la conique $\sc C_e(O,\sc D)$ le nombre $p=e\|\vec{OH}\|$. 
\bigskip

\Propriete [$(a,b,c)\in\ob R^3$ avec $a\neq 0$]
Soit $\sc C$ la courbe d'équation polaire 
$$
r={1\F a+b\cos\theta+c\sin\theta}
$$ 
Si $(b,c)=(0,0)$, $\sc C$ est un cercle de centre $O$ et de rayon $1/|a|$ . \pn
Si $(b,c)\neq(0,0)$, $\sc C$ est une conique de foyer $O$, d'exentricité $\ds e$ 
et de directrice $\sc D$ avec 
$$
e={\sqrt{b^2+c^2}\F|a|}\qquad\mbox{et}\qquad \sc D:\quad \ds r={1\F b\cos\theta+c\sin\theta}
$$

\Concept Algorithme d'étude d'une courbe polaire $r=\rho(\theta)$. 

Etant donnée une courbe $\sc C$ déquation polaire $\vec{OM}=\rho(\theta)\vec u\ \,(\theta\in I)$.  
\bigskip

\noindent
0) Déterminer l'ensemble de définition $\sc D\rho$ de la fonction $\rho$. \pn
1) Etudier la périodicité de $\rho$. \pn
2) Etudier la parité de $\rho$. \pn
3) Déterminer (la dérivabilité de $\rho$) le signe de $\rho'$ 
et les zéros de $\rho$ (passage au pôle). \pn
4) Faire un beau tableau que l'on complètera par l'étude des points ``spéciaux'' 
(tangentes, asymptotes, position par rapport aux asymptotes, branches infinies, 
points d'inflection...)
\pn
5) Dessiner le graphe de la courbe (les renseignements précédents donnent son allure). 
\bigskip

\Definition La courbe $\sc C$ présente une branche infinie 
en un réel $\theta_0\in I\ssi\lim_{\theta\to\theta_0}|\rho(\theta)|=+\infty$. \pn
Si $\lim_{\theta\to\theta_0}\rho(\theta)\sin(\theta-\theta_0)=L$, la courbe $\sc C$ admet une asymptote 
d'équation $y=L$ dans le repère $(O,\vec{u_{\theta_0}},\vec{v_{\theta_0}})$. \pn
Si $\lim_{\theta\to\theta_0}\b|\rho(\theta)\sin(\theta-\theta_0)\b|=+\infty$, on dit que $\sc C$ 
admet une branche parabolique dans la direction $\vec {u_{\theta_0}}$. 
\bigskip

\Remarque : La courbe $\sc C$ présente une branche infinie spiralée en $+\infty$ si $\lim_{\theta\to+\infty}|\rho(\theta)|=\infty$ 
(même chose pour $-\infty$). 
\bigskip

\Section con, Coniques.
\bigskip

\Subsection def, Définitions et Réduction.


\Definition [$F$ point du plan, $\sc D$ droite ne passant pas par $F$ et $e>0$]
La conique de foyer $F$, de directrice $\sc D$ et d'excentricité $e$ est l'ensemble 
$$
\sc C=\b\{M\in\sc P:F\/M=e\ \mbox{d}(M,\sc D)\b\}=\b\{M\in\sc P:F\/M=e MH\b\}.
$$
où $H$ désigne le projeté orthogonal de $M$ sur $\sc D$. 
\bigskip

\Definition Une courbe $\sc C$ du plan $\sc P$ est une conique $\Longleftrightarrow$ la courbe 
$\sc C$ admet dans un repère orthonormé $(O,\vec i,\vec j)$ de $\sc P$ une équation cartésienne du type 
$$
\underbrace{ax^2+bxy+cy^2}_{q(x,y)}+dx+ey+f=0.\eqdef{eqnr}
$$
avec $(a,b,c,d,e,f)\in\ob R^6$ et $(a,b,c)\neq(0,0,0)$. 
\bigskip

\noindent{\bf But. }On applique une rotation puis une translation (ou l'inverse) 
au repère $(O,\vec i,\vec j)$ pour obtenir un nouveau repère orthonormé 
$(C,\vec {e_1},\vec{e_2})$ dans lequelle la conique à une équation ``réduite'', 
plus simple que \eqref{eqnr}. 
\bigskip 

\centerline{%
	\Image[Height=4cm]{\imageFolder/Pdf/Ellip.pdf}%
}%
\Figure ]Index=Courbes!Ellipse] Une ellipse et un repère orthonormée $(C,e_1,e_2)$ qui la réduit.

\Propriete [$q:\ob R^n\to\ob R$ forme quadratique de matrice $A$] 
Si~$\sc B=\{e_1,\cdots,e_n\}$ est une base orthonormale de vecteurs propres de la matrice $A$, associés 
aux valeurs propres $\{\lambda_1,\cdots,\lambda_n\}$, alors la matrice de $q$ dans $\sc B$ est 
$$
\pmatrix{\lambda_1&0&\cdots&0\cr
0&\lambda_2&\ddots&\vdots\cr
\vdots&\ddots&\ddots&0\cr
0&\cdots&0&\lambda_n\cr}.
$$
En particulier, pour chaque $x=\sum_{i=1}^nx_ie_i$ dans $\ob R^n$, on~a~alors 
$q(x)=\sum_{i=1}^n\lambda_ix_i^2$. 
\bigskip

\Methode [pour réduire une forme quadratique $q$] 
1a) Ecrire la matrice de la forme quadratique  $A=\pmatrix{a&b/2\cr b/2&c}$. \pn
1b) Trouver les valeurs propres $\lambda_1$ et $\lambda_2$ de $A$. \pn
1c) Trouver une base orthonormale $\{\vec{e_1},\vec{e_2}\}$ de vecteurs propres associés à $\lambda_1$~et~$\lambda_2$. \pn
1d) Dans la nouvelle base ($x\vec i+y\vec j=X\vec{e_1}+Y\vec{e_2}$), l'équation de $\sc C$ est du type
$$
\lambda_1X^2+\lambda_2Y^2+\alpha X+\beta Y+\gamma=0.
$$ 
2) Faire une translation pour se débarasser (si possible) 
des termes en $X$ et $Y$
$$
X=X'+X_0 \qquad\mbox{et}\qquad Y=Y'+Y_0
$$
3) Identifier la conique : ellipse, parabole, hyperbole ou dégénérée ($\emptyset$, $\{a\}$, droite(s)). \pn
4) Identifier ses éléments caractéristiques.

\Remarque : Selon que $\det A=\lambda_1\lambda_2$ est $<0$, $=0$ ou $>0$ 
on trouve respectivement une ellipse, une parabole ou une hyperbole, 
sauf si la conique est dégénérée. 
\bigskip

\Subsection el, Ellipse.
\bigskip

Soit $a\ge b>0$ et soit $\sc E$ la courbe d'équation, 
$$
{x^2\F a^2}+{y^2\F b^2}=1 \eqdef{eqrel}
$$
dans un repère orthonormé $(O,\vec i,\vec j)$. 
Alors, $\sc E$ est appelée ellipse d'axe focal $(O,\vec i)$ 
de demi-grand axe $a$, de demi-petit axe $b$. L'ellipse $\sc E$ est d'exentricité 
$$
e=\sqrt{1-{b^2\F a^2}} 
$$
et admet deux couples foyer-directrice 
$(F,D)$ et $(F',D')$, avec $F$ et $F'$ de coordonnées $(ae,0)$ et $(-ae,0)$ et 
$D$ et $D'$ d'équation cartésiennes $x=a/e$ et $x=-a/e$. 
\medskip

\centerline{%
	\Image[Height=4cm]{\imageFolder/Pdf/Ellip2.pdf}%
}%
\Figure [Index=Courbes!Ellipse] L'ellipse et ses éléments caractéristiques.

\Remarque : Un paramètrage usuel de l'ellipse $\sc E$ est 
$$
\Q\{\eqalign{x(t)=a\cos t\cr y(t)=b\sin t\cr}\W.\qquad(t\in\ob R). 
$$
\medskip
\noindent
Inversement, si $\sc E$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$0\!<\!e\!<\!1$, alors $\sc E$ satisfait l'équation \eqref{eqrel} pour 
$$
a={e FH\F1-e^2}\qquad\mbox{et}\qquad b={e FH\F\sqrt{1-e^2}}
$$
dans le repère orthonormé direct $(O,\vec i,\vec j)$ vuniquement déterminé par 
$$
\vec i={\vec {FH}\F FH}\qquad \vec{HO}={\vec {HF}\F1-e^2}
$$

\Subsection hyp, Hyperbole. 
\bigskip

Soient $a>0$, $b>0$ et $\sc H$ la courbe d'équation
$$
{x^2\F a^2}-{y^2\F b^2}=1. \eqdef{eqrel2}
$$
dans un repère orthonormé $(O,\vec i,\vec j)$. 
Alors, $\sc H$ est appelée hyperbole de centre $O$ et d'axe transverse (ou focal) 
$(O,\vec i)$. L'hyperbole $\sc H$ est d'exentricité 
$$
e=\sqrt{1+{b^2\F a^2}} 
$$
et admet deux couples foyer-directrice $(F,D)$ et $(F',D')$, 
avec $F$ et $F'$ de coordonnées $(ae,0)$ et $(-ae,0)$ et 
$D$ et $D'$ d'équation cartésiennes $x=a/e$ et $x=-a/e$.
\pn
Les points $S$ et $S'$ 
de coordonnées $(a,0)$ et $(-a,0)$ sont appelés sommets de $\sc H$ 
et l'hyperbole $\sc H$ admet deux asymptotes $A$ et $A'$ 
d'équation $ay+bx=0$ et $ay-bx=0$. 

\centerline{%
	\Image[Width=4cm]{\imageFolder/Pdf/Hyper.pdf}%
}%
\Figure [Index=Courbes!Hyperbole] L'hyperbole et ses éléments caractéristiques.

\Remarque : Un paramètrage usuel des branches de l'hyperbole $\sc H$ est 
$$
\Q\{\eqalign{x(t)&=a\ch t\cr y(t)&=b\sh t\cr}\W.\qquad\mbox{et}\qquad
\Q\{\eqalign{x(t)&=-a\ch t\cr y(t)&=b\sh t\cr}\W.\qquad(t\in\ob R). 
$$
\medskip
\noindent
Inversement, si $\sc H$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$e>1$, alors $\sc H$ satisfait l'équation \eqref{eqrel2} pour 
$$
a={e FH\F e^2-1}\qquad\mbox{et}\qquad b={e FH\F\sqrt{e^2-1}}
$$
dans le repère orthonormé direct $(O,\vec i,\vec j)$ uniquement déterminé par 
$$
\vec i={\vec {FH}\F FH}\qquad \vec{HO}={\vec {HF}\F1-e^2}
$$

\Subsection par, Parabole.
\bigskip

Soit $p>0$ et soit $\sc P$ la courbe d'équation, 
dans un repère orthonormé $(O,\vec i,\vec j)$, 
$$
y^2=2px \eqdef{eqrel3}
$$
Alors, $\sc P$ est appelée parabole de sommet $O$, d'axe $(O,\vec i)$ 
et de paramètre $p$. Elle~admet un foyer $F$ de coordonnées $(0,{p\F2})$ 
et une directrice $\sc D$ d'équation $x=-p/2$. Son excentricité est $e=1$. 
\medskip

\centerline{%
	\Image[Width=4cm]{\imageFolder/Pdf/Parabo.pdf}%
}%
\Figure [Index=Courbes!Parabole] La Parabole et ses éléments caractéristiques.

\Remarque : Un paramètrage usuel de la parabole $\sc P$ est 
$$
\Q\{\eqalign{x(t)&={t^2\F 2p}\cr y(t)&=t\cr}\W.\qquad(t\in\ob R). 
$$
\medskip

\noindent
Inversement, si $\sc P$ est une conique de foyer $F$, de directrice $\sc D$ 
et d'exentricité~$e=1$, alors $\sc P$ satisfait l'équation \eqref{eqrel3} pour 
$p=FH$ 
dans le repère orthonormé direct $(O,\vec i,\vec j)$ 
uniquement déterminé par 
$$
\vec i=-{\vec {FH}\F FH}\qquad \vec{FO}={\vec {FH}\F2}, 
$$
où $H$ est la projection orthogonale de $F$ sur $\sc D$.
\bigskip










\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Enveloppes\Développées\Développantes\Coniques}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}









\hautspages{Olus Livius Bindus}{Topologie, suites et continuité dans $\ob R^n$}%

\Chapter Calculdif, Topologie, suites et continuité dans $\ob R^n$.

\Section Topo, Topologie de $\ob R^n$. 

\Subsection Null, Normes et distance euclidienne. 

\Definition [$n\ge1$]
Une norme de $\ob R^n$ est une application $N:\ob R^n\to\ob R^+$ vérifiant les propriétés suivantes : 
\Bullet L'application $N$ est à valeurs réelles positives :
$$
\forall x\in\ob R^n, \qquad N(x)\ge0.
$$
\Bullet L'application $N$ est dite ``définie'' :
$$
\forall x\in\ob R^n, \qquad N(x)=0\ \Longleftrightarrow\ x=0.
$$
\Bullet L'application $N$ est positivement homogène :
$$
\forall x\in\ob R^n, \quad \forall \lambda\in\ob R,\qquad N(\lambda x)=|\lambda|N(x).
$$
\Bullet L'application $N$ satisfait l'inégalité triangulaire :
$$
\forall x\in\ob R^n, \quad\forall y\in\ob R^n, \qquad N(x+y)\le N(x)+N(y).\eqdef{eqtriangulaire}
$$

\Remarque{ \it1}. le ``définie'' de cette propriété n'est pas le ``défini=existe" standard. 
\medskip 

\noindent
\Remarque{ \it2.} Une norme $N$ satisfait nécéssairement les deux inégalités triangulaires 
$$
\forall x\in\ob R^n, \quad \forall y\in\ob R^n, \qquad\b|N(x)-N(y)\b|\le N(x+y)\le N(x)+N(y).
$$

\Propriete [$n\ge1$] On définit une norme de $\ob R^n$, appelée norme euclidienne de $\ob R^n$, en posant 
$$
\forall x=(x_1,\cdots, x_n)\in\ob R^n, \qquad \Norme{x}:=\sqrt{x_1^2+x_2^2+\cdots+x_n^2}
$$

\Demonstration. Preuve effectuée en PTSI à l'aide de l'inégalité de Cauchy-Schwartz.\CQFD

\Remarque : Si $n=1$, la norme euclidienne de $\ob R^n=\ob R$ est la valeur absolue. 
\bigskip


\Definition [$n\ge1$]
Dans $\ob R^n$, la distance euclidienne entre deux points $x=(x_1,\cdots,x_n)$ et $y=(y_1,\cdots,y_n)$ 
est le nombre réel positif 
$$
\d(x,y):=\|x-y\|=\sqrt{(x_1-y_1)^2+\cdots+(x_n-y_n)^2}, 
$$
où $\|\cdot\|$ désigne la norme euclidienne $\|\cdot\|$ de l'espace $\ob R^n$. 


\Subsection Null, Ouverts et fermés. 

\Definition [$n\ge1$, norme $\|.\|$ euclidienne de $\ob R^n$]
La boule ouverte (euclidienne) de centre $a\in\ob R^n$ et de rayon $r>0$ est l'ensemble 
$$
\sc B(a,r):=\b\{x\in\ob R^n\ :\ \|x-a\|<r\b\}.
$$


\Definition [$n\ge1$] 
Un ensemble $E\subset\ob R^n$ est ouvert $\Longleftrightarrow$ pour chaque élément $x$ de $E$, il existe une boule ouverte de centre $x$ et de rayon $r>0$ contenue dans $E$
$$
E\mbox{ est ouvert}\ssi\forall x\in E, \quad \exists r>0\ :\quad \sc B(x,r)\subset E.
$$


\Definition [$n\ge1$] 
Un ensemble $V\subset\ob R^n$ est un voisinage du point $a\in\ob R^n \Longleftrightarrow$ il contient une boule de centre $a$ et de rayon $r>0$. 
$$
V\mbox{ est un voisinage de }a\ssi\exists r>0\ :\quad \sc B(a,r)\subset V.
$$


\Propriete
Une réunion d'ouverts est un ouvert. \pn
Une intersection d'un nombre fini d'ouverts est un ouvert.

\Definition [$n\ge1$] La boule fermée (euclidienne) de centre $a\in\ob R^n$ et de rayon $r\ge0$ est l'ensemble 
$$
\overline{\sc B(a,r)}:=\b\{x\in\ob R^n\ :\ \|x-a\|\le r\b\}.
$$


\Definition [$n\ge1$] 
Un ensemble $E\subset\ob R^n$ est fermé $\Longleftrightarrow$ $E$ est le complémentaire d'un ouvert de $\ob R^n$. 
$$
E\sbox{ est fermé}\ssi \ob R^n\ssm E\sbox{ est ouvert}
$$


\Propriete
Une intersection de fermés est un fermé. \pn
Une réunion d'un nombre fini de fermés est un fermé.


\Definition [$n\ge1$] 
Un ensemble $E\subset\ob R^n$ est borné $\Longleftrightarrow$ il existe une boule ouverte contenant $E$ 
$$
E\sbox{ est borné}\ssi \exists r>0\ :\quad E\subset\sc B(0, r)
$$

\Remarque : Un ensemble $E\subset\ob R^n$ est borné $\ssi$ il existe un nombre $M>0$ vérifiant 
$$
\forall x\in E,\qquad \|x\|\le M.
$$

\Definition [$n\ge1$]
Un ensemble $E\subset\ob R^n$ est compact $\Longleftrightarrow$ $E$ est fermé et borné. 


\Propriete Si $E$ est un ensemble fermé et borné de nombres réels, alors 
$$
\inf E=\min E\in E\qquad \sbox{ et }\qquad \sup E=\max E\in E.
$$


\Subsection Null, Suites d'éléments de $\ob R^n$. 


\Concept [Index=Suites!limites!Definition@Définition] Limite d'une suite

\Definition [$u$ suite d'éléments de $\ob R^n$, $\ell\in\ob R^n$] 
La suite $u$ converge vers la limite $\ell$ si, et seulement si
$$
\forall\epsilon>0, \qquad \exists N\in\ob R:\qquad \forall n\ge N, \quad \|u_n-\ell\|\le \epsilon. \eqdef{limlim}
$$

\Exemple. la suite de terme général $u_n=\big({6n\F2n+1}, {4n+1\F 2n+1}\big)$ converge dans $\ob R^2$ vers $\ell=\big(3,2\big)$. 

\Demonstration.  Soit $\epsilon>0$ . Alors, pour {$\ds N:={\sqrt{10}\F2\epsilon}$}, nous avons
$$\eqalign{
{\forall n\ge N,} \qquad {\left|u_n-\ell\right|}&=\Q|\Q|\Q({6n\F 2n+1}-3, {4n+1\F2n+1}-2\W)\W|\W|=\Q|\Q|\Q({-3\F 2n+1}, {-1\F2n+1}\W)\W|\W|={\Q|\Q|\Q(-3, -1\W)\W|\W|\F 4n+2}\cr
&={\sqrt{10}\F 2n+1}{\le} {\sqrt{10}\F 2N}={\epsilon}.}
$$
\CQFD

\Exercice{PTakr}%


\Concept [Index=Suites!limites!limite nulle] Suite de limite nulle

\Propriete [$n\ge1$, $u$ suite d'éléments de $\ob R^n$, $\ell\in\ob R^n$] 
La suite $u_n$ converge vers $\ell\ssi$ la suite $v_n=u_n-\ell$ converge vers $0_{\ob R^n}=(0,\cdots, 0)$.

\Demonstration. C'est une conséquence immédiate de l'identité $\forall n\in\ob N, \ \|u_n-\ell\|=\|v_n-0\|$. \CQFD

\Exercice{PTaks}%

\Concept [Index=Suites!convergente] Suite convergente ou divergente

\Definition 
Lorsqu'une suite converge vers une limite, on dit qu'elle converge, qu'elle est convergente ou qu'elle admet une limite. \pn
Dans le cas contraire, lorsqu'une suite ne converge vers aucune limite, on dit qu'elle diverge, qu'elle est divergente ou qu'elle 
n'admet aucune limite. 

\Concept [Index=Suites!limites!Unicite@Unicité]Unicité de la limite

\Propriete 
Lorsqu'elle existe, la limite d'une suite $u$ est unique. On la note $\ds\lim\limits_{n\to+\infty}u_n$ ou $\lim u$. 

\Demonstration. Soit $u$ une suite de nombres complexes convergeant vers les limites $\ell$ et $\ell'$.  Pour $\epsilon>0$ , il~existe des nombres $N\in\ob R$ et $N'\in\ob R$ tels que 
$$
\forall n\ge N, \qquad \|u_n-\ell\|\le \epsilon\qquad\mbox{et}\qquad 
\forall n\ge N', \qquad \|u_n-\ell'\|\le \epsilon. 
$$ 
Pour un entier $n\ge\max\{N,N'\}$, nous déduisons alors des inégalités triangulaires que 
$$
{\|\ell-\ell'\|}=\big|\big|(\ell-u_n)+(u_n-\ell')\big|\big|\le \|\ell-u_n\|+\|u_n-\ell'\|\le \epsilon+\epsilon\ {\le2\epsilon.}
$$ 
Comme nous obtenons que $\forall \epsilon>0, \|\ell-\ell'\|\le2\epsilon$, nous concluons que $\|\ell-\ell'\|=0$ et donc que 
$\ell=\ell'$. 
\CQFD


\Concept [Index=Suites!Propriété des gendarmes] Propriété des gendarmes

\Propriete [$u$ suite d'éléments de $\ob R^n$]
S'il existe une suite réelle $\alpha$ telle que 
$$
\forall n\in\ob N, \qquad |u_n|\le \alpha_n\qquad \mbox{et}\qquad \lim_{n\to+\infty}\alpha_n=0, 
$$
alors la suite $u$ converge vers $0$. 


\Concept [Index=Suites!limites!limite de $\|u\|$] Limite de la norme d'une suite

\Propriete [$u$ suite d'éléments de $\ob R^n$]
Si la suite $u$ converge vers une limite $\ell\in\ob R^n$, alors la suite de terme général $v_n=\|u_n\|$ converge vers la limite $\ell':=\|\ell\|$. 


\Concept [Index=Suites!limites!majoration] Majoration

\Propriete 
Une suite convergente est bornée. 

\Demonstration. Soit $u$ une suite de limite $\ell\in\ob R^n$. Alors, pour $\epsilon=1$, il existe $N\in\ob N$ tel que 
$$
\forall n\ge N, \qquad \|u_n-\ell\|\le \epsilon=1
$$
Alors, nous posons $M:=\max\{\|u_0\|,\cdots, \|u_{N-1}\|, \|\ell\|+1\}$ et nous déduisons des inégalités triangulaires que 
$$
 \forall n\ge N , \qquad {\|u_n\|}=\big|\big|(u_n-\ell)+\ell\big|\big|\le \|u_n-\ell\|+\|\ell\|\le 1+\|\ell\|{\le M}. 
$$
Comme l'inégalité en rouge est aussi satisfaite pour $0\le n<N$, par définition de $M$, la suite $u$ est bornée. 
\CQFD

\Exemple. La suite $u_n= \big(\ln(n^2+1)-2\ln(n), \th(n)\big)$ est bornée pour $n\ge1$ car elle converge vers $(0, 1)$. 


\Concept [Index=Suites!limites!produit par un scalaire] Multiplication par une constante

\Propriete [$u$ suite d'éléments de $\ob R^n$, $\lambda\in\ob R$]
Le produit d'une suite convergente $u$ par une constante $\lambda$ est une suite convergente et 
$$
\lim_{n\to\infty}(\lambda.u_n)=\lambda.\lim_{n\to\infty}u_n
$$

\Demonstration. Soit $\lambda\in\ob R$ et soit $u$ une suite d'éléments de $\ob R^n$ convergeant vers $\ell\in\ob R^n$. 
\pn
Soit $\epsilon>0$ . Comme la suite $u$ converge vers $\ell$, pour $\epsilon'={\epsilon\F|\lambda|+1}$,  il existe un
rang $N$ tel que $$ \forall n\ge N, \qquad \|u_n-\ell\|\le \epsilon'={\epsilon\F |\lambda|+1}.  $$ Et alors, nous remarquons
que la suite $v$ de terme général $v_n:=\lambda.u_n$ satisfait $$  \forall n\ge N,  \qquad  \|v_n-
\lambda\ell\|=|\lambda|.\|u_n-\ell\|\le |\lambda|.{\epsilon\F|\lambda|+1} \le\epsilon . $$ En particulier, la suite de
terme général $v_n=\lambda u_n$ converge vers $\lambda\ell$.  \CQFD

\Concept [Index=Suites!limites!addition] Addition

\Propriete [$u$ et $v$ suites d'élements de $\ob R^n$] 
La somme de deux suites convergentes $u$ et $v$ est une suite convergente et 
$$
\lim_{n\to+\infty}(u_n+v_n)=\lim_{n\to+\infty}u_n+\lim_{n\to+\infty}v_n.
$$

\Demonstration. Soient $u$ et $v$ deux suites d'éléments de $\ob R^n$ convergeant respectivement vers $\ell'$ et $\ell''$. 
Prouvons que la suite de terme général $w_n:=u_n+v_n$ converge vers $\ell:=\ell'+\ell''$. \pn 
 Soit $\epsilon>0$ . Comme $u$ converge vers $\ell'$, pour $\epsilon'={\epsilon\F2}$,  il existe un rang $N'$  tel que 
$$
\forall n\ge N', \qquad \|u_n-\ell'\|\le \epsilon'={\epsilon\F2}. 
$$
De même, comme $v$ converge vers $\ell''$, pour $\epsilon''={\epsilon\F2}$,  il existe un rang $N''$  tel que 
$$
\forall n\ge N'', \qquad \|v_n-\ell''\|\le \epsilon''={\epsilon\F2}. 
$$
Alors,  pour $N=\max\{N',N''\}$, nous déduisons des inégalités triangulaires que 
$$
\forall n\ge N , \qquad \|w_n-\ell\| =\big|\big|(u_n-\ell')+(v_n-\ell'')\big|\big|\le \|u_n-\ell'\|+\|v_n-\ell''\|\le
{\epsilon\F 2}+{\epsilon\F2}=\epsilon. 
$$ \CQFD


\Propriete [$u$ et $v$ suites d'éléments de $\ob R^n$]
Si la suite $u$ converge et si la suite $v$ diverge, alors la suite $u+v$ est divergente. 


\Remarque : si $u$ et $v$ sont deux suites divergentes, la suite $u+v$ peut converger (par exemple pour $u_n=n$ et $v_n=-n$) ou diverger (par exemple pour $u_n=v_n=n$). 
\bigskip


\Concept [Index=Suites!limites!multiplication] Multiplication

\Propriete [$u$ suite réelle et $v$ suite d'éléments de $\ob R^n$] 
Le produit de deux suites convergentes $u$ et $v$ est une suite convergente et 
$$
\lim_{n\to+\infty}(u_n.v_n)=\lim_{n\to+\infty}u_n.\lim_{n\to+\infty}v_n
$$

\Demonstration. Etant donnés une suite réelle $u$ et une suite $v$ d'éléments de $\ob R^n$ convergeant respectivement vers $\ell'\in\ob R$ et $\ell''\in\ob R^n$, 
prouvons que la suite de terme général $w_n:=u_nv_n$ converge vers $\ell:=\ell'\ell''$. \pn 
 Soit $\epsilon>0$ . Commen\cced ons par remarquer que 
$$
\forall n\ge0, \qquad \|w_n-\ell\|=\|(u_n-\ell'+\ell')v_n-\ell'\ell''\|=\big|\big|(u_n-\ell')v_n+\ell'(v_n-\ell'')\big|\big|. 
$$
Comme la suite $v$ converge, elle est bornée. Ainsi, il existe un nombre $M>0$ tel que 
$$
\forall n\ge0, \qquad \|v_n\|\le M.
$$
Il résulte alors des inégalité triangulaires que 
$$
\forall n\ge0, \qquad \|w_n-\ell\|\le |u_n-\ell'|.\|v_n\|+|\ell'|.\|v_n-\ell''\|\le |u_n-\ell'|.M+|\ell'|.\|v_n-\ell''\|.
$$
Comme la suite $u$ converge vers $\ell'$, pour $\epsilon'={\epsilon\F2M+1}$, il existe un rang $N'$ tel que 
$$
\forall n\ge N', \qquad |u_n-\ell'|\le \epsilon'={\epsilon\F 2M+1}. 
$$
De même, comme $v$ converge vers $\ell''$, pour $\epsilon''={\epsilon\F2|\ell'|+1}$, il existe un rang $N''$ tel que 
$$
\forall n\ge N'', \qquad \|v_n-\ell''\|\le \epsilon''={\epsilon\F2|\ell'|+1}. 
$$
Alors, pour $N=\max\{N',N''\}$, nous obtenons finalement que 
$$
\forall n\ge N, \qquad \|w_n-\ell\|\le {\epsilon\F2M+1}.M+|\ell'|.{\epsilon\F2\ell'|+1}\le{\epsilon\F2}+{\epsilon\F2}=\epsilon.
$$
\CQFD


\Concept [Index=Suites!limites!multiplication] Matrices

\Propriete [$A$ et $B$ suites de matrices de $\sc M_n(\ob R)$, $(\lambda,\mu)\in\ob R^2$] 
Si les suites de matrices $A$ et $B$ convergent, alors les suites $\lambda A+\mu B$ et $AB$ convergent et 
$$
\eqalign{
\lim\limits_{n\to\infty}(\lambda A_n+\mu B_n)&=\lambda\lim\limits_{n\to\infty}A_n+\mu\lim\limits_{n\to\infty}B_n\cr
\lim\limits_{n\to\infty}(A_nB_n)=&\lim\limits_{n\to\infty}A_n\times\lim\limits_{n\to\infty}B_n.
}
$$
De plus, si la suite matricielle $A$ converge vers une matrice inversible, alors la suite $A^{-1}$ est définie à partir d'un certain rang et converge vers
$$
\lim\limits_{n\to\infty}(A_n)^{-1}=\Q(\lim\limits_{n\to\infty}A_n\W)^{-1}.
$$


\Concept [Index=Suites!limites!Suites coordonnées] Suites coordonnées

\Theoreme [$u$ suite d'éléments de $\ob R^k$] 
Soient $u^{(1)}, \cdots, u^{(k)}$ les suites coordonnées de $u$ définies par 
$$
\forall n\ge0, \qquad u_n=\big(u^{(1)}_n, \cdots, u^{(k)}_n\big).
$$
Alors, la suite $u$ converge $\Leftrightarrow$ chacune de ses suites coordonnées converge. Et dans ce cas, 
$$
\lim\limits_{n\to\infty}u_n=\Q(\lim_{n\to\infty}u^{(1)}_n, \cdots, \lim\limits_{n\to\infty}u^{(k)}_n\W).
$$


\Section Null, Limites et continuité dans $\ob R^n$.

\bigskip
Dans tout cette section, $m$, $n$ et $p$ désignent des nombres entiers strictements positifs et les espaces vectoriels $\ob R^m$, $\ob R^n$ et $\ob R^p$ sont munis de leur norme euclidienne respective. 
\bigskip

\Subsection LIM, Limites et continuité d'une fonction de plusieurs variables. 

\Definition [$D\subset\ob R^n$]
Une fonction $f:D\to\ob R^p$ est appelée fonction de $n$ variables, à valeurs réelle si $p=1$ et à valeurs vectorielles sinon. En particulier, on a 
$$
f:(\underbrace{x_1,\cdots, x_n}_{x\in D\subset\ob R^n})\mapsto \underbrace{f(x_1,\cdots, x_n)}_{f(x)\in\ob R^p}=\B(f_1(x),f_2(x),\cdots,f_p(x)\B).
$$ 

\Concept [Index=Fonctions!Limites finies] Limites finies

\Definition [$D\subset\ob R^n$, $\ell\in\ob R^p$, $a\in\ob R^n$]
Une fonction $f:D\to \ob R^p$ admet la limite $\ell$ en un point $a$ si, et seulement si,
$$
\forall \epsilon> 0, \quad \exists \alpha>0:\qquad \forall x\in D\mbox{ vérifiant } \|x-a\|\le \alpha, \mbox{ on a }\|f(x)-\ell\|\le\epsilon 
$$
Lorsqu'elle existe, cette limite $\ell$ est unique et on la note $\ds\lim_{x\to a}f(x)$. 
\bigskip

\Remarque{ \it 1} : On dit parfois la fonction $f$ converge vers $\ell$ en $a$ (resp. tends vers $\ell$ en $a$) lorsque $x$ tends vers~$a$ à la place de ``la fonction admet la limite $\ell$ en $a$". 
\bigskip


\Remarque{ \it 2} : on peut caractériser les limites à l'aide de voisinages.  
\bigskip

\Propriete [$D\subset\ob R^n$, $\ell\in\ob R^p$, $a\in\ob R^n$]
Une fonction $f:D\to \ob R^p$ admet la limite $\ell$ en un point $a$ si, et seulement si, 
$$
\sbox{Pour $V$ voisinage de $\ell$, il existe $U$ voisinage de $a$ tel que }\forall x\in D\cap U, f(x)\in V
$$

\Concept [Index=Fonctions!Limites infinies] Limites infinies


\Remarque{} : Pour les fonction réelles, on peut également définir la notion de divergence vers $\pm\infty$ d'une fonction de $n$ variables. 
\bigskip


\Definition [$D\subset\ob R^n$, $a\in\ob R^n$]
Une fonction réelle $f:D\to \ob R$ admet la limite $\ell=+\infty$ (resp. $\ell=-\infty$) en un point $a$ si, et seulement si, 
$$
\forall M\in\ob R, \ \exists \alpha>0\ :\quad \forall x\in D\sbox{ vérifiant } \|x-a\|\le \alpha, \sbox{ on a } f(x)\ge M\quad(\sbox{resp. }f(x)\le M).
$$

\Remarque{} : lorsque l'on travaille avec des vecteurs $x\in\ob R^n$, on ne peut ni faire tendre $x$ vers l'infini, ni faire tendre $x$ vers $a$ par la droite ou par la gauche : cela n'a pas de sens. 
\bigskip

\Concept [Index=Fonctions!Continuite locale@Continuité locale] Continuité locale

\Definition [$a\in D\subset\ob R^n$] 
Une application $f:D\to\ob R^p$ est continue en $a$ si, et~seulement~si
$$
\forall \epsilon>0,\qquad \exists \delta>0\ :\qquad \forall x\in D\sbox{ vérifiant }\|x-a\|\le\alpha, \sbox{ on a }\|f(x)-f(a)\|\le\epsilon.
$$

\Propriete [$a\in D\subset\ob R^n$, $f:D\to\ob R^p$]
$$
\mbox{$f$ est continue en $a$}\ssi \lim\limits_{x\to a}f(x)=f(a). 
$$

\Remarque{} : Continuité et limites sont étroitement liées et constituent deux aspects d'un même problème. Pour les exercices, garder à l'esprit que ``continuité=limite''.

\Concept [Index=Fonctions!Continuite globale@Continuité globale] Continuité globale

\Definition 
Une fonction $f$ est continue sur un ensemble $D\ \Leftrightarrow\ f$ est continue en chaque point $a\in D$. 

\Propriete
Une fonction $f:\ob R^n\to\ob R^p$ est continue si, et seulement si
$$
\forall U\sbox{ ouvert de }\ob R^p, \quad f^{-1}(U)\sbox{ est un ouvert de }\ob R^n
$$

\Propriete
Une fonction $f:\ob R^n\to\ob R^p$ est continue si, et seulement si
$$
\forall U\sbox{ fermé de }\ob R^p, \quad f^{-1}(U)\sbox{ est un fermé de }\ob R^n
$$

\Remarque : ces deux propriétés sont surtout utiles pour prouver de manière élégante que des ensembles sont ouverts ou fermés. 
\bigskip


\Concept [Index=Fonctions!Prolongement par continuité] Prolongement par continuité

\Definition [$D\subset\ob R^n$, $a\not\in D$ un point au contact de $D$] 
Une application $f:D\to\ob R^p$ est prolongeable par continuité en $a\ \Longleftrightarrow\ f$ converge en $a$. 
Dans ce cas, la fonction définie par 
$$
\tilde f(x):=\Q\{
\eqalign{
f(x)\quad &\sbox{ si }x\in D,\cr
\ds \lim\limits_{x\to a}f(x)&\sbox{ si }x=a
}
\W.
$$
est une application de $D':=D\cup\{a\}$ dans $\ob R^p$, continue en $a$, qui prolonge la fonction $f$. 

\Remarque : En général, s'il n'y a pas de contre indications, on confondra l'application $f$ avec son prolongement pas continuité $\tilde f$. 
\bigskip

\Exercice{PTaks}%


\Concept [Index=Fonctions!Translation à l'arrivée] Translation à l'arrivée


\Propriete[$D\subset\ob R^n$, $a\in\ob R^n$, $\ell\in\ob R^p$] 
La fonction $f:D\to\ob R^p$ converge vers $\ell$ en $a\Longleftrightarrow$ la fonction $f-\ell$ converge vers $0$ en $a$. 


\Concept [Index=Fonctions!Translation au départ] Translation au départ

\Propriete[$D\subset\ob R^n$, $a\in\ob R^n$, $\ell\in\ob R^p$] 
$f:D\to\ob R^p$ converge vers $\ell$ en $a\ \Longleftrightarrow\ $ la fonction $g:h\mapsto f(a+h)$ converge vers $\ell$ en $0$. 

\Invertedtrue

\Propriete[$D\subset\ob R^n$, $a\in\ob R^n$] 
$f:D\to\ob R^p$ est continue en $a\ \Longleftrightarrow\ $ la fonction $g:h\mapsto f(a+h)$ est continue en $0$. 

\Remarque : les propriétés précédentes permettent de transformer un problème de limite vers $\ell$ en $a$ 
en un problème de limite vers $0$ en $0$, qui est {\it a priori}, plus facile à résoudre (on translate le problème pour se ramener en $0$). 
\bigskip


\Subsection Ope, Opérations algèbriques. 

\Theoreme 
Si $u:\ob R^n\to\ob R^p$ est une application linéaire, alors
$$
\forall a\in\ob R^n, \qquad \lim_{x\to a}u(x)=u(a). 
$$

\Invertedtrue
\Theoreme 
Une application linéaire $u:\ob R^n\to\ob R^p$ est continue en chaque point de $\ob R^n$. 


\Remarque : Soit $a=(a_1,a_2,\cdots,a_n)\in\ob R^n$. Alors, pour $1\le k\le n$, on a 
$$
\lim_{(x_1,\cdots,x_n)\to a}x_k=a_k. 
$$

\Propriete 
Pour $1\le k\le n$, la forme linéaire $\d x_k: (x_1, \cdots, x_n)\mapsto x_k$ est continue sur $\ob R^n$. 



\Propriete [$D$ ouvert de $R^n$, $a\in D$]
Si la fonction réelle $f:D\to\ob R$ converge en $a$ vers $\ell\neq 0$, 
alors il existe un voisinage $V\subset D$ de $a$ pour lequel on a 
$$
\forall x\in V,\qquad f(x)\neq0.\qquad\qquad \sbox{($f$ ne s'annule pas au voisinage de $a$)}
$$
Si $f:D\to\ob R$ converge en $a$ vers $\ell> 0$, alors il existe un voisinage $V\subset D$ de $a$ tel que
$$
\forall x\in V,\qquad f(x)\ge{\ell\F 2}> 0. 
$$
($f$ est minorée par un nombre strictement positif au voisinage de $a$). 
\bigskip



\Theoreme [$a\in D$ ouvert de $\ob R^n$, $\lambda\in\ob R$]
Si les fonctions $f:D\to\ob R^p$ et $g:D\to\ob R^p$ admettent une limite en $a$, alors les fonctions $\lambda.f$, $f+g$ et $f\times g$ convergent en $a$ et 
$$
\eqalign{
&\lim\limits_{x\to a}\b(\lambda.f(x)\b)=\lambda.\lim_{x\to a} f(x),\cr
&\lim\limits_{x\to a}\b(f(x)+g(x)\b)=\lim_{x\to a}f(x)+\lim_{x\to a}g(x),\cr
&\lim\limits_{x\to a}\b(f(x)\times g(x)\b)=\lim_{x\to a}f(x)\times\lim_{x\to a}g(x).
}
$$
Si $\ds\lim\limits_{x\to a} g(x)\neq 0$, la fonction $f/g$ est définie au voisinage de $a$ et converge vers 
$$
\lim\limits_{x\to a}\Q({f(x)\F g(x)}\W)={\lim\limits_{x\to a}f(x)\F\lim\limits_{x\to a}g(x)}.
$$

\Invertedtrue
\Theoreme [$D\subset\ob R^n$, $\lambda\in\ob R$, $a\in D$] 
Si $f:D\to\ob R$ et $g:D\to\ob R$ sont continues en $a$, 
alors, les fonctions $\lambda.f$, $f+g$ et $f\times g$ sont continues en $a$. 
Si $g(a)\neq 0$, la fonction $f/g$ est définie autour de $a$ et continue en $a$. \pn


\Theoreme 
Une fonction polynôme de $n$ variables est continue sur $\ob R^n$. \pn 
Une fraction rationnelle de $n$ variables est continue sur son ensemble de définition. 


\Theoreme [$n\ge1$, $p\ge1$, $a\in D$ ouvert de $\ob R^n$, $b\in D'$ ouvert de $\ob R^p$, $\ell\in\ob R^p$]
Si $f:D\to\ob R^p$ converge vers $b$ en $a$ et si $g:D'\to\ob R^p$ converge vers $\ell$ en $b$ et si 
$$
\forall x\in D, \qquad f(x)\in D',
$$
alors, l'application $g\circ f$ est définie sur $D$ et converge en $a$ vers $\ell$. 


\Invertedtrue
\Theoreme [$a\in D\subset\ob R^n$, $D'\subset\ob R^p$] 
Si $f:D\to\ob R^p$ est continue en $a$, si $g:D'\to\ob R^q$ est continue en $b=f(a)$ et si 
$$
\forall x\in D, \qquad f(x)\in D',
$$
Alors l'application $g\circ f$ est définie sur $D$ et continue en $a$. \pn


\Theoreme [$D\subset\ob R^n$, $a\in\ob R^n$, $(\ell_1,\cdots, \ell_p)\in\ob R^p$]
Pour $1\le k\le p$, la fonction $f_k:D\to\ob R$ converge en $a$ vers $\ell_k \Longleftrightarrow$ la fonction définie par 
$$
\forall x\in D, \qquad f(x):=\B(f_1(x),f_2(x),\cdots,f_p(x)\B)
$$
converge en $a$ vers $\ell=(\ell_1,\ell_2,\cdots,\ell_p)$. 

\Invertedtrue
\Theoreme [$a\in D\subset\ob R^n$] 
Pour $1\le k\le p$, la fonction $f_k:D\to\ob R$ est continue en $a\ \Longleftrightarrow\ $ l'application $f$ définie par 
$$
\forall x\in D, \qquad f(x):=\B(f_1(x),f_2(x),\cdots,f_p(x)\B)
$$
est continue en $a$. \pn

\Remarque : Cette propriéte permet en particulier de ramener l'étude de la convergence (resp. de la continuité) d'une fonction vectorielle à plusieurs études de convergence (reps. de continuité) de fonctions réelles. 
\bigskip

\Remarque : Par contre, on ne peut pas ramener l'étude de la convergence (resp. de la continuité) d'une fonction de plusieurs variables à plusieurs études de convergence (resp. de la continuité) de fonctions d'une variable. 
\bigskip

\Remarque : Retenir ``à l'arrivée, cela se passe bien mais au départ, cela se passe mal''. 


\Propriete [$D\subset R^n$, $a\in D$, $\ell\in\ob R^p$, $\|.\|$ norme de $\ob R^p$] 
Si $f:D\to\ob R^p$ converge en $a$ vers $\ell$, alors la fonction $\|f\|$ converge en $a$ vers $\|\ell\|$. 

\Invertedtrue
\Propriete [$D\subset R^n$, $a\in D$, $\|.\|$ norme de $\ob R^p$] 
Si $f:D\to\ob R^p$ est continue en $a$, alors la fonction $\|f\|$ est continue en $a$. \pn


\Propriete [$D\subset\ob R^n$, $a\in\ob R^n$]
Si $f:D\to\ob R^p$ converge en $a$ et si $g:D\to\ob R^p$ diverge en $a$, alors $f+g$ diverge en $a$. 

\Invertedtrue
\Propriete [$D\subset\ob R^n$]
Si $f:D\to\ob R^p$ est continue en $a$ et si $g:D\to\ob R^p$ est discontinue en $a$, alors l'application $f+g$ est discontinue en $a$. \pn

\Remarque : si $f$ et $g$ sont deux fonctions n'admettant pas de limite en $a$, la fonction $f+g$ peut converger ou diverger. 
\bigskip


\Propriete [$D$ voisinage de $a\in\ob R^n$] 
L'ensemble des fonctions $f:D\to\ob R$ de limite nulle en $a$ forme un espace vectoriel. 

\Propriete [$D\subset\ob R^n$] 
Si $f:D\to\ob R$ converge vers $0$ en $a$ et si $g:D\to\ob R$ est bornée au voisinage de $a$, alors, la fonction $f\times g$ converge vers $0$ en $a$.

\Subsection Ope, Propriétés importantes. 

\Concept [Index=Fonctions!Principe des gendarmes] Principe des gendarmes

\Theoreme [$D\subset\ob R^n$ voisinage de $a\in\ob R^n$] 
Si les fonctions $f:D\to\ob R^p$ et $g:D\to\ob R$ vérifient 
$$
\forall x\in D, \qquad \b|\!\b|f(x)\b|\!\b|\le g(x)\qquad \mbox{et}\qquad \lim_{x\to a}g(x)=0,
$$
alors, la fonction $f$ converge en $a$ vers $0$ . 

\Theoreme [$D$ voisinage de $a\in\ob R^n$]
Soient $f$, $g$ et $h$ trois fonctions de $D$ dans $\ob R$ vérifiant $f\le g\le h$. 
Si $f$ et $h$ convergent en $a$ vers le même nombre réel $\ell$, alors l'application $g$ converge vers $\ell$ en $a$. 


\Concept [Index=Fonctions!Conservation des inégalités larges] Conservation des inégalités larges par passage à la limite

\Propriete [$D$ voisinage de $a\in\ob R^n$] 
Soient $f$ et $g$ deux fonctions de $D$ dans $\ob R$ telles que $f\le g$. Alors, 
$$
\eqalign{
\sbox{$f$ et $g$ convergent en $a$} &\quad\Longrightarrow \quad\lim_{x\to a}f(x)\le \lim_{x\to a}g(x), \cr
\sbox{$f$ diverge vers $+\infty$ en $a$} &\quad\Longrightarrow\quad\lim_{x\to a}g(x)=+\infty, \cr
\sbox{$g$ diverge vers $-\infty$ en $a$}&\quad\Longrightarrow\quad\lim_{x\to a}f(x)=-\infty
}
$$


\Theoreme [$D$ un voisinage de $a\in\ob R^n$, $f:D\to\ob R^p$]
L'application $f$ est continue en $a$ si, et seulement si, 
$$
\mbox{pour tout suite }u\in D^{\ob N} \mbox{ vérifiant }\lim_{n\to+\infty}u_n=a, \mbox{ on a }\lim_{n\to+\infty}f(u_n)=f(a). 
$$


\Theoreme [$D$ voisinage de $a\in\ob R^n$] 
Si $f:D\to\ob R^p$ est une application continue en $a$ et si $u$ est une suite déléments de $D$ convergeant vers $a$, alors 
$$
\lim_{n\to+\infty}f(u_n)=f(a)=f(\lim_{n\to\infty}u_n).
$$

\Remarque : Cette propriété est extrèmement utile pour deux raisons : \pn
a) Si une suite de terme général $u_n\in D$ tends vers $a$, si $f:D\to\ob K$ est continue en $a$ 
alors 
$$
\lim_{n\to+\infty}f(u_n)=f(a).
$$
b) Si une suite de terme général $u_n\in D$ converge vers $a$ et si la suite de terme général $v_n=f(u_n)$ diverge, alors, la fonction $f$ n'est pas continue en $a$. 
\bigskip

\Concept [Index=Theoreme@Théorème!de compacite@de compacité] Théorème de compacité

\Theoreme 
L'image d'un ensemble fermé et borné (i.e. d'un compact) par une fonction continue est un ensemble fermé et borné (i.e. compact). 

\Remarque : ce théorème fondamental permet entre autres : \pn
a) de prouver que l'on peut minorer ou majorer une fonction. \pn
b) d'établir l'existence théorique de maxima et de minima locaux ou globaux. \pn
{\it Il sert surtout dans les exercices théoriques. }

\hautspages{Olus Livius Bindus}{Calcul Différentiel}%

\Chapter cdif§, Calcul Différentiel. 

\bigskip
Dans tout ce chapitre, $m$, $n$ et $p$ désignent des nombres entiers strictements positifs et les espaces vectoriels $\ob R^m$, $\ob R^n$ et $\ob R^p$ sont munis de leur norme euclidienne respective. 
\bigskip

\Section cdiff§fonc, Fonctions d'une variable à valeurs vectorielles. 

\Subsection Gfoncder, Dérivée. 

\Concept Définition locale. 

\Definition [$a\in I$ intervalle]
La fonction $f:I\to\ob R^n$ est dérivable en $a\ssi$ il existe un nombre $\ell\in\ob R^n$ tel que 
$$
\lim_{\ss x\to a\atop\ss x\neq a}{f(x)-f(a)\F x-a}=\ell. 
$$
Ce vecteur $\ell$, qui est unique et que l'on note $f'(a)$, est appelé ``vecteur dérivé de $f$ en $a$''. 

\Exemple. $f:x\mapsto(\cos x,\sin x)$ est dérivable et admet la dérivée $f'(a)=(-\sin a, \cos a)$ en $a\in\ob R$. 

\Definition [$a\in I$ intervalle] 
Une fonction $f:I\to\ob R^n$ est dérivable à gauche (resp. à droite) en $a\ssi$ il existe un vecteur $\ell\in\ob R^n$ tel que 
$$
\lim_{\ss x\to a\atop\ss x<a}{f(x)-f(a)\F x-a}=\ell\qquad\qquad\Q(\mbox{resp. }\lim_{\ss x\to a\atop\ss x>a}{f(x)-f(a)\F x-a}=\ell\W). 
$$
Ce vecteur $\ell$, qui est unique et que l'on note $f'(a^-)$ (resp. $f'(a^+)$, est appelé ``vecteur dérivé à gauche (resp. à droite) 
de la fonction $f$ en $a$''. 

\Remarque : Le nombre dérivée est la limite des taux d'accroissements en $a$. En utilisant le changement de variable $x=a+h$, on peut également écrire que 
$$
f'(a)=\lim_{\ss h\to 0\atop\ss h\neq 0}{f(a+h)-f(a)\F h}. 
$$

\Propriete [$I$ intervalle, $a\in I$ qui n'est pas une extrémité de $I$]
Une fonction $f:I\to\ob R^n$ est dérivable en $a\ssi$ $f$ est dérivable à gauche et à droite en $a$ avec $f'(a^-)=f'(a^+)$. 
Dans ce cas, on a alors
$$
f'(a)=f'(a^-)=f'(a^+).
$$

\Exemple. La valeur absolue $x\mapsto|x|$ n'est pas dérivable en $0$. 
\bigskip

\Propriete [$a\in I$ intervalle] 
Si $f:I\to\ob R^n$ est dérivable en $a$, alors $f$ est continue en $a$. 

\Remarque : En d'autres mots, la continuité en $a$ est une condition nécéssaire pour que $f$ soit dérivable en $A$. 
\bigskip
\Remarque : chercherà prouver une dérivabilité pour établir une continuité est une erreur (fatale) répendue courante chez les étudiants qui leur coûte cher (cela montre que vous n'avez pas compris la logique du cours). 
\bigskip

\Theoreme [$I$ intervalle réel]
Soit $f:I\to\ob R^n$ une applications et $f_1, \cdots, f_n$ ses fonctions coordonnées définies implicitement par  
$$
\forall x\in I,\qquad f(x)=\Big(f_1(x),\cdots, f_n(x)\Big).
$$
Alors, la fonction vectorielle $f$ est dérivable en $a\ssi$ chaque fonction réelle $f_k$ est dé\-ri\-va\-ble en $a$. 
Et dans ce cas, on a 
$$
f'(a)=\Big(f_1'(a),\cdots, f_n'(a)\Big).
$$

\Concept Définition globale. 

\Definition [$D\subset E$ ensembles]
Une fonction $f:E\to\ob R^n$ est dérivable sur un ensemble $D\ssi$ $f$ est dérivable en chaque point $x\in D$. 
Alors, on note $f'$ et on appelle fonction dérivée de $f$ l'application 
$$
\eqalign{f': D&\to\ob R^n\cr x&\mapsto f'(x)}.
$$ 

\Remarque { 1 }: la fonction dérivée $f'$ de l'application $f$ est notée parfois $\ds {\d f\F \d x}$ ou rarement~$Df$. 
\bigskip

\Remarque { 2 }: La définition précédente permet de faire le lien entre la dérivabilité en un point $a$ (notion locale) et la dérivabilité sur un ensemble (notion globale). 
\bigskip

\Remarque { 3 }:Il existe des fonctions continues sur $\ob R$ qui ne sont dérivables en aucun point. 

\Subsection gah2, Opérations. 

\Theoreme [$a\in I$ intervalle, $(\lambda,\mu)\in\ob R^2$]
Si $f:I\to\ob R^n$ et $g:I\to\ob R^n$ sont dérivables en $a$, alors l'application $\lambda f+\mu g$ est dérivable en $a$ et 
$$
(\lambda f+\mu g)'(a)=\lambda f'(a)+\mu g'(a). 
$$
Si $f:I\to\ob R$ et $g:I\to\ob R^n$ sont dérivables en $a$, alors $f.g$ est dérivable en $a$ et
$$
(fg)'(a)=f'(a)g(a)+f(a)g'(a).
$$
Si $g(a)\neq 0$, le quotient $f/g$, qui est défini autour de $a$, est dérivable en $a$ et 
$$
\Q({f\F g}\W)'(a)={f'(a)g(a)-f(a)g'(a)\F g^2(a)}.
$$

\Remarque : si $g(a)\neq0$ et si $g$ est dérivable en $a$, la fonction $1/g$ est dérivable en $a$ et 
$$
\Q({1\F g}\W)'(a)=-{g'(a)\F g(a)^2}.
$$
En particulier, on peut dériver le quotient $f/g=f\times{1\F g}$ en écrivant 
$$
\Q({f\F g}\W)'(a)=f'(a)\times {1\F g(a)}+f(a)\times{-g'(a)\F g(a)^2}.
$$

\Remarque : Les fonctions polynômes $x\mapsto P(x)=\sum_{k=0}^n a_xx^k$ sont dérivables sur $\ob R$. 
\bigskip

\Remarque  : Les fractions rationnelles $x\mapsto {P(x)\F Q(x)}$ (quotient de deux polynômes) sont dérivables sur $\ob R$ privé des points annulant le dénominateur. 
\bigskip


\Subsection gah2, Dérivées itérées. 

\Definition [$I$ intervalle, $n\ge2$] 
Une fonction $f:I\to\ob R^p$ est $n$ fois dérivable sur $I$ si, et~seulement~si, la fonction $f$ est dérivable sur $I$ et si $f'$ est $n-1$ fois 
dérivable sur $I$. Dans ce cas, on pose 
$$
\forall x\in I,\qquad f^{(n)}(x):=(f')^{(n-1)}(x). 
$$


\Remarque : Soit $I$ un intervalle et soit $f:I\to\ob K$ une fonction. Par convention, on note 
$$
\forall x\in I, \qquad f^{(0)}(x):=f(x).
$$
La fonction $f^{(0)}=f$ est parfois appelée dérivée d'ordre $0$ de la fonction $f$. 
\bigskip

\Remarque : Une application $n$ fois dérivable $f:I\to\ob R^n$ satisfait 
Alors, on a 
$$
\forall x\in I, \qquad f^{(n)}(x):=\underbrace{{\d\F \d x}{\d\F\d x}{\d\F \d x}\cdots{\d\F \d x}{\d\F \d x}}_{\mbox{$n$ dérivations successives}}f(x)
$$

\Remarque : la dérivée $n^\ieme$ de $f$ se note $\ds f^{(n)}$, ou parfois $\ds{\d^n\F\d x^n}f$ ou plus rarement $\ds D^nf$. 
\bigskip

\Propriete [$I$ intervalle, $n\ge2$]
Si $f:I\to\ob R^p$ est $n$ fois dérivable sur $I$, alors on a 
$$
\forall p\in\{0,\cdots, n\}, \qquad \forall x\in I, \qquad f^{(n)}(x)=\Q(f^{(p)}\W)^{(n-p)}(x).
$$

\Definition [$I$ intervalle non vide]
On note $\sc C^0(I,\ob R^n)$ l'ensemble des applications $f:I\to\ob R^n$, continues sur $I$. 

\Definition [$I$ intervalle non vide, $n\ge1$] 
On note $\sc C^n(I,\ob R^p)$ l'ensemble des applications $f:I\to\ob R^p$ 
dérivables sur $I$, dont la dérivée $f'$ appartient à $\sc C^{n-1}(I,\ob R^p)$. 


\Propriete [$I$ intervalle non vide] 
L'ensemble $(\sc C^n(I,\ob R^p), +, .)$ est l'ensemble des fonctions $f:I\to\ob K$ de classe $\sc C^n$ sur $I$, i.e. des applications $f:I\to\ob R^p$ dérivables $n$ fois sur $I$ dont les dérivées $f$, $f'$, $f'', \cdots, f^{(n)}$ sont continues sur $I$. 

\Definition [$I$ intervalle non vide] 
Une fonction $f:I\to\ob R^p$ est de classe $\sc C^\infty$ sur $I\ssi$ $f$ est de classe $\sc C^n$ sur $I$ pour chaque entier $n\in\ob N$. \pn
On note $\sc C^\infty(I,\ob K)$ l'ensemble des fonctions $f:I\to\ob K$ de classe $\sc C^\infty$ sur $I$. 

\Propriete Pour $n\in\ob N\cup\{\infty\}$, l'ensemble $(\sc C^n(I,\ob R^p), +, .)$ est un $\ob R$-espace vectioriel. 

\Propriete
$$
\forall n\in\ob N, \qquad \sc C^{n+1}(I,\ob R^p)\subset\sc C^n(I,\ob R^p).
$$ 

\Remarque : la suite des ensembles $\sc C^n(I,\ob R^p)$ est construite par récurence à partir de $n=1$. A noter que l'on note souvent $\sc C^n(I)$ plutôt que $\sc C^n(I,\ob R)$. 
\bigskip


\Subsection gah2, Opérations sur les dérivées itérées. 

\Theoreme [$I$ intervalle, $(\lambda,\mu)\in\ob R^2$]
Si les fonctions $f:I\to\ob R^p$ et $g:I\to\ob R^k$ sont $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. 
Alors, la fonction $\lambda f+\mu g$ est $n$ fois dérivable (resp. de classe $\sc C^n$) sur $I$ et 
$$
\forall x\in I, \qquad (\lambda f+\mu g)^{(n)}(x)=\lambda f^{(n)}(x)+\mu g^{(n)}(x)
$$

\Theoreme [Title=Formule de Leibniz;$I$ intervalle, $n\in\ob N$]
Si $f:I\to\ob R$ et $g:I\to\ob R^p$ sont $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. 
Alors, la~fonction $f\times g$ est $n$ fois dérivable sur $I$ (resp. de classe $\sc C^n$) et 
$$
\forall x\in I, \qquad (f\times g)^{(n)}(x)=\sum_{0\le k\le n}{n\choose k}f^{(k)}(x)g^{(n-k)}(x).
$$


\Theoreme [$I$ intervalle, $n\in\ob N$, $\left\langle\cdot,\cdot\right\rangle$ produit scalaire de $\ob R^p$]
Si $f:I\to\ob R^p$ et $g:I\to\ob R^p$ sont $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. 
Alors, le produit scalaire $\Q\langle f,g\W\rangle$ est $n$ fois dérivable sur $I$ (resp. de classe $\sc C^n$) et 
$$
\forall x\in I, \qquad \Q\langle f,g\W\rangle^{(n)}(x)=\sum_{0\le k\le n}{n\choose k}\Q\langle f^{(k)}(x),g^{(n-k)}(x)\W\rangle.
$$

\Theoreme [$I$ intervalle, $n\in\ob N$]
Si $f:I\to\ob R^3$ et $g:I\to\ob R^3$ sont $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$. 
Alors, le produit vectoriel $f\wedge g$ est $n$ fois dérivable sur $I$ (resp. de classe $\sc C^n$) et 
$$
\forall x\in I, \qquad (f\wedge g)^{(n)}(x)=\sum_{0\le k\le n}{n\choose k}f^{(k)}(x)\wedge g^{(n-k)}(x).
$$


\Propriete [$I$ intervalle réel]
Si $f:I\to\ob R^n$ et $g:I\to\ob R$ sont $n$ fois dérivables (resp. de classe $\sc C^n$) sur $I$ et si 
$$
\forall x\in I, \qquad g(x)\neq 0,
$$ 
alors, la fonction $f/g$ est $n$ fois dérivable (resp. de classe $\sc C^n$) sur $I$. 

\Remarque : Les fonctions polynômes $x\mapsto P(x)=\sum_{k=0}^n a_xx^k$ sont de classe $\sc C^\infty$ 
sur $\ob R$. 
\bigskip

\Remarque  : Les fractions rationnelles $x\mapsto {P(x)\F Q(x)}$ (quotient de deux polynômes) sont de classe $\sc C^\infty$ sur $\ob R$ privé des points annulant le dénominateur. 
\bigskip

\Theoreme [$I$ et $J$ intervalles] 
Si $f:I\to J$ est $n$ fois dérivable en $a\in I$ et si $g:J\to\ob R^p$ est $n$ fois dérivable en $f(a)\in J$, 
alors la fonction $g\circ f$, qui est définie sur $I$, est $n$ fois dérivable (resp. de classe $\sc C^n$) sur $I$. 

\Section cdiff§fonc, Fonctions de plusieurs variables. 

\Subsection cdifǧdiff, Différentielle.

\Definition [$U$ ouvert de $\ob R^n$, $f:U\to\ob R^p$, $a\in U$] 
$f$ est différentiable en $a\ssi$ il existe une application linéaire $u:\ob R^n\to\ob R^p$ telle que 
$$
f(a+h)=f(a)+u(h)+o(\|h\|)\qquad(h\to0), 
$$
c'est à dire telle que l'application $\epsilon$ définie par 
$$
f(a+h)=f(a)+u(h)+\|h\|\epsilon(h)\qquad(a+h\in U)
$$ 
vérifie $\ds\lim\limits_{h\to0}\epsilon(h)=0$. 

\Propriete Lorsqu'elle existe, l'application linéaire $u$ est unique. On l'appelle différentielle de $f$ en $a$ et on la note $\d f_a$. 


\Exemple. Si la fonction vectorielle $f:I\to\ob R^p$ est dérivable en un point $a$ de l'intervalle $I\subset\ob R$, la~différentielle de $f$ en $a$ est l'application $\diff f_a:h\mapsto f'(a)h$ car 
$$
f(a+h)=f(a)+\underbrace{f'(a)h}_{u(H)}+o(h)\qquad(h\to 0). 
$$


\Exemple. La différentielle en $A\in\sc M_n(\ob R)$ de la fonction
$$
\eqalign{f:\sc M_n(\ob R)&\to\sc M_n(\ob R)\cr
M&\mapsto M^2}
$$
est l'application $\diff f_A:H\mapsto HA+AH$ car 
$$
f(A+H)=(A+H)\times(A+H)=\underbrace{A^2}_{f(A)}+\underbrace{HA+AH}_{u(H)}
+\underbrace{H^2}_{o(\|H\|)}\qquad\big(H\in\sc M_n(\ob R)\big)
$$


\Subsection cdiff§Aparti, Dérivée suivant un vecteur.


\Definition[$a\in U$ ouvert de $\ob R^n$]
La fonction $f:U\to\ob R^k$ admet une dérivée ${\d f\F\d\vec v}(a)$ en $a$ selon le vecteur $\vec v\in\ob R^n\ssi$ 
$$
{\d f\F\d\vec v}(a)=\lim\limits_{t\to0\atop t\neq 0}{f(a+t\vec v)-f(a)\F t}
$$
c'est à dire si, et seulement si, la fonction $g:t\mapsto f(a+t\vec v)$ est dérivable en $0$ et vérifie $$
{\d f\F\d\vec v}(a)=g'(0).
$$ 

\Remarque : on note aussi $D_{\vec v}f(a)$ la dérivée de $f$ en $a$ selon le vecteur $\vec v$. 
\bigskip

\Subsection cdiff§Aparti, Applications partielles. 

\Definition [$a=(a_1,\cdots,a_n)\in U$ ouvert de $\ob R^n$, $1\le i\le n$]
La $i^\ieme$ application partielle d'une fonction $f:U\to\ob R^p$ en $a$ est la fonction 
$$
f_i:t\mapsto f(a+te_i)=f(a_1,\cdots,a_{i-1},a_i+t,a_{i+1},\cdots,a_n).
$$
Lorsqu'elle existe, la $i^\ieme$ dérivée partielle de la fonction $f$ en $a$ est la dérivée $f_i'(0)$ de la $i^\ieme$ application partielle de $f$ en $0$. On la note $\ds {\partial f\F\partial x_i}(a)$ ou $\partial_if(a)$ et elle satisfait 
$$
\ds {\partial f\F\partial x_i}(a)=\lim_{t\to0\atop t\neq0}{f(a+te_i)-f(a)\F t},
$$
où $\{e_1, \cdots, e_n\}$ designe la base canonique de $\ob R^n$. 


\Definition [$a=(a_1,\cdots,a_n)\in U$ ouvert de $\ob R^n$, $1\le i\le n$]
La $i^\ieme$ dérivée partielle (dérivée partielle par rapport à $x_i$) d'une application $f:U\to \ob R^p$ est 
la fonction 
$$
\eqalign{
{\partial f\F\partial x_i}:U&\to\ob R^p\cr
a&\mapsto {\partial f\F\partial x_i}(a)
}$$

\Remarque : la $i^\ieme$ dérivée partielle de $f$ en $a$ est la dérivée de $f$ en $a$ selon le $i^\ieme$ vecteur de la base canonique $\{e_1, \cdots, e_n\}$ 
$$
{\partial f\F\partial x_i}(a)={\partial f\F\partial \vec{e_i}}(a).
$$


\Theoreme [$a\in U$ ouvert de $\ob R^n$, $1\le i\le n$]
L'application $f:I\to\ob R^n$ admet une dérivée partielle par rapport à $x_i$ en $a$ $\ssi$ les fonctions coordonnées $f_1, \cdots, f_n$, implicitement définies par  
$$
\forall x\in U,\qquad f(x)=\Big(f_1(x),\cdots, f_n(x)\Big),
$$
admettent toutes une dérivée partielle par rapport à $x_i$ en $a$. 
Et dans ce cas, on a 
$$
{\partial f\F\partial x_i}(a)=\Q({\partial f_1\F\partial x_i}(a),\cdots, {\partial f_n\F\partial x_i}(a)\W).
$$


\Subsection cdiff§Aparti, Matrice jacobienne et jacobien. 

\Definition [$a\in U$ ouvert de $\ob R^n$]
Soit $f:U\to\ob R^p$ une application dont toutes les dérivées partielles sont définies en $a\in U$. et soient $f_1, \cdots, f_p$ les applications coordonnées implicitement définies par 
$$
\forall x\in U, \qquad f(x)=\big(f_1(x),\cdots,f_p(x)\big).
$$ 
Alors, la matrice jacobienne de $f$ en $a$ est la matrice $J[f](a)$ de $\sc M_{p,n}(\ob R)$ 
définie par 
$$
J[f](a):=\Q({\partial f\F\partial x_1}(a),\ldots,{\partial f\F\partial x_n}(a)\W)
=\pmatrix{
{\partial f_1\F\partial x_1}(a)&\ldots&\ldots&{\partial f_1\F\partial x_n}(a)
\cr
{\partial f_2\F\partial x_1}(a)&\ldots&\ldots&{\partial f_2\F\partial x_n}(a)
\cr
\vdots&\ldots&{\partial f_i\F\partial x_j}(a)&\vdots
\cr
{\partial f_p\F\partial x_1}(a)&\ldots&\ldots&{\partial f_p\F\partial x_n}(a)
\cr
}.
$$

\Definition [$a\in U$ ouvert de $\ob R^n$, ${n=p}$]
Le jacobien de $f:U\to\ob R^p$ en $a$ est le déterminant 
de la matrice jacobienne de $f$ en $a$
$$
{\mbox D(f_1,\cdots,f_n)\F\mbox D(x_1,\cdots,x_n)}:=\det J[f](a).
$$ 

\Exemple. Variables sphériques. 
$(r,\theta,\phi)\mapsto (r\cos\varphi\cos\theta,r\cos\varphi\sin\theta,r\sin\varphi)$. 

\Subsection cdiff§Aparti, Lien entre différentielles et dérivées partielles. 

\Definition [$1\le i\le n$]
On note $\d x_i$ la forme linéaire 
$$
\eqalign{
\d x_i:\qquad \ob R^n\qquad&\to\ob R\cr
(x_1,\cdots,x_n)&\mapsto x_i
}
$$
 
\Propriete [$a\in U$ ouvert de $\ob R^n$]
La différentielle d'une fonction $f$ en un point $a$ est l'application linéaire
$$
\d f_a:=\sum_{1\le i\le n}{\partial f\F\partial x_i}(a)\d x_i,
$$
qui appartient à l'espace $\sc L(\ob R^n,\ob R^p)$. De plus, sa matrice dans la base canonique satisfait
$$
\sc Mat_{{\rm BC}}\d f_a=J[f](a).
$$ 



\Subsection cdiff§Aparti, Gradient. 


\Definition [$a\in U$ ouvert de $\ob R^n$]
Si $f:U\to\ob R$ admet toutes ses dérivées partielles en $a$, 
le gradient de $f$ en $a$ est 
le vecteur 
$$
\vec{\grad}\ f(a):=\pmatrix{{\partial f\F\partial x_1}(a)\cr\vdots\cr {\partial f\F\partial x_n}(a)}.
$$ 
Etant données $f_1, \cdots, f_n$ les fonctions coordonnées de $f$ implicitement définies par 
$$
\forall x\in U, \qquad f(x)=\big(f_1(x),\cdots,f_p(x)\big), 
$$
la matrice jacobienne de $f$ en $a$ satisfait 
$$
J[f](a)=\pmatrix{\NULL^t\vec{\grad}\ f_1(a)\cr\vdots\cr\NULL^t\vec{\grad}\ f_p(a)}
$$

\Remarque : La différentielle de $f$ en $a$ s'exprime à l'aide du gradient et du produit scalaire. 
$$
\d f_a:=\vec{\grad}\ f(a).\d\vec x\qquad \mbox{avec}\qquad \d\vec x:=\pmatrix{\d x_1\cr\vdots\cr \d x_n}.
$$
Ainsi, le gradient pointe dans la direction selon laquelle $f$ augmente le plus et est orthogonal aux courbes de niveau de $f$.

\Section cdiff§AppCk, Applications de classe $\sc C^k$. 


\Definition [$k\ge1$, $U$ ouvert de $\ob R^n$] 
L'application $f:U\to\ob R^p$ est de classe $\sc C^k$ sur $U$ $\ssi$ toutes les dérivées partielles de $f$ 
sont définies et de~classe $\sc C^{k-1}$ sur $U$. 

\Propriete [$k\ge1$, $U$ ouvert de $\ob R^n$] 
L'ensemble des fonctions $f:U\to\ob R^p$ de classe $\sc C^k$ sur $U$ forme un espace vectoriel que l'on note $\sc C^k(U,\ob R^p)$. 

\Concept Composée de deux fonctions
 

\Propriete [$U$ ouvert de $\ob R^n$, $V$ ouvert de $\ob R^p$] 
Soient $f:U\to\ob R^p$ et $g:V\to\ob R^q$ deux fonctions de classe~$\sc C^1$ telles~que~$f(U)\subset V$. Alors, l'application $g\circ f:U\to\ob R^q$ 
est de classe $\sc C^1$ sur $U$ et 
$$
\forall a\in U, \qquad J[g\circ f](a)=J[g]\big(f(a)\big)\times J[f](a). 
$$ 
Autrement dit, pour $1\le i\le q$ et $1\le j\le n$, on a 
$$
\forall a\in U, \qquad {\partial (g\circ f)_i\F\partial x_j}(a)=\sum_{1\le k\le p}{\partial g_i\F\partial f_k}\big(f(a)\big){\partial f_k\F\partial x_j}(a)
=\sum_{1\le k\le p}\partial_k g_i\big(f(a)\big)\partial_j f_k(a)
$$


\Remarque : Sous les hypothèses du théorème précédent, on a 
$$
\d(g\circ f)_a=\d g_{f(a)}\circ \d f_a
$$ 
Ainsi, pour $1\le j\le n$, on a 
$$
\forall a\in U,\qquad {\partial(g\circ f)\F\partial x_j}(a)
=\sum_{1\le k\le p}{\partial g\F\partial f_k}\b(f(a)\b){\partial f_k\F\partial x_j}(a)
=\sum_{1\le k\le p}\partial_k g\b(f(a)\b)\partial_j f_k(a)
$$


\Propriete [$U$ ouvert de $\ob R^n$] 
L'application $F:U\to\ob R^p$ est de classe $\sc C^k$ sur un ouvert $U\subset\ob R^n$ si, et seulement si, 
les applications coordonnées $f_i$, implicitement définies par 
$$
\forall x\in U, \qquad F(x)=\big(f_1(x),\cdots,f_p(x)\big),
$$
sont de classe $\sc C^k$ sur $U$. Etant donné $a\in U$, on a alors 
$$
\forall h\in \ob R^n, \qquad \d F_a(h)=\big(\d(f_1)_a(h),\cdots,\d(f_p)_a(h)\big)
$$


\Exemple. Pour $f\in\sc C^1\big(\ob R^2,\ob R\big)$, calculer la dérivée 
de la fonction $g:x\mapsto f(1-x^2,\e^x)$. 
\bigskip

\Definition [$k\ge0$, $U$ et $V$ deux ouverts de $\ob R^n$]
L'application $f:U\to V$ est un difféomorphisme de classe $\sc C^k$ $\ssi$ $f$ est une bijection de $U$ sur $V$, 
de classe $\sc C^k$ sur $U$, 
dont la bijection réciproque $f^{-1}$ est de classe $\sc C^k$ sur $V$. 


\Propriete [$U$ et $V$ ouverts de $\ob R^n$]
Si $f:U\to V$ est un difféomorphisme de classe $\sc C^1$, alors, $J[f](a)$ est inversible et 
$$
\forall a\in U, \qquad J[f^{-1}]\big(f(a)\big)=J[f](a)^{-1}.
$$ 

\Exemple. Calcul de l'inverse de la matrice jacobienne des changements de variables : 
Polaire $(r,\theta)\mapsto(r\cos\theta,r\sin\theta)$ et Cylindrique $(r,\theta,z)\mapsto(r\cos\theta,r\sin\theta,z)$.  
\bigskip


\Theoreme [$U$ ouvert de $\ob R^n$] Si $f:U\to\ob R^p$ est une application de classe $\sc C^2$ sur $U$ 
alors, pour $(i,j)\in\{1,\cdots ,n\}$, on a 
\Equation [\bf Théorème de Schwartz]
$$
\forall a\in U,\qquad {\partial\F\partial x_i}\Q({\partial f\F\partial x_j}\W)(a)={\partial\F\partial x_j}\Q({\partial f\F\partial x_i}\W)(a). 
$$
\smallskip


\Definition [$U$ ouvert de $\ob R^n$, $f:U\to\ob R^p$ de classe $\sc C^k$]
Pour $1\le i\le n$ et $0\le\ell\le k$, on pose 
$$
\partial_i^\ell f(a)={\partial^\ell f\F\partial x_i^\ell}(a)
:=\underbrace{{\partial\F\partial x_i}\cdots{\partial\F\partial x_i}}_{\ell\mbox{ fois}}f(a)\qquad(a\in U). 
$$
Etant donné $(\ell_1,\cdots,\ell_n)\in\ob N^n$ 
vérifiant $\ell_1+\cdots+\ell_n\le k$, on pose de même 
$$
\partial_1^{\ell_1}\cdots\partial_n^{\ell_n}f(a)={\partial^{\ell_1+\cdots+\ell_n}f\F\partial x_1^{\ell_1}\cdots\partial x_n^{\ell_n}}(a):=
{\partial^{\ell_1}\F\partial x_1^{\ell_1}}\cdots{\partial^{\ell_n}\F\partial x_n^{\ell_n}}f(a)\qquad(a\in U). 
$$
L'entier $\ell_i$ est l'odre partiel de dérivation par rapport à 
la $i^{\mbox{\sevenrm ème}}$ variable $x_i$. \pn
L'entier~$L:=\ell_1+\cdots+\ell_n$ est l'ordre total de dérivation 

\Remarque : pour $k\ge2$, on peut calculer 
les dérivées partielles de~$f\in\sc C^k(U,\ob R^p)$ (d'ordre total inférieur à $k$) 
 dans l'ordre que l'on veut. 
\bigskip

\Concept [] Fonctions de classe $\sc C^\infty$

\Definition [$U$ ouvert de $\ob R^n$] 
L'application $f:U\to\ob R^p$ est de classe $\sc C^\infty$ 
sur $U\ssi$ $f$ est de classe $\sc C^k$ sur $U$ 
pour chaque entier $k\in\ob N$. 

\Propriete [$U$ ouvert de $\ob R^n$, $k\ge1$] 
Une fonction $f:U\to\ob R^p$ est de classe $\sc C^\infty$ (resp. $\sc C^k$) sur $U$ si, et seulement si, toutes ses dérivées partielles 
(resp. d'ordre total inférieur à $k$) existent et sont continues sur $U$ 
(en~tenant compte de l'ordre de dérivation). 

\Exemple. Toute fonction polynomiale $f:\ob R^n\to\ob R$ est de classe $\sc C^\infty$ 
sur $\ob R^n$. 

\Concept Différentielle

\Definition [$U$ ouvert de $\ob R^n$] 
La différentielle d'une fonction $f\in\sc C^1(U,\ob R^p)$ est l'application 
$$
\eqalign{
\d f:U&\to\sc L(\ob R^n,\ob R^p)\cr 
a&\mapsto\d f_a.}
$$
\bigskip

\Exemple. $\forall u\in\sc L(\ob R^n,\ob R^p)$, $u$ 
est de classe $\sc C^\infty$ sur $\ob R^n$ et~vérifie~$\d u=u$. 
\bigskip 

\Concept Opérations sur les différentielles

\Propriete [$U$ ouvert de $\ob R^n$, $k\in\ob N^*\cup\{\infty\}$]
Si $f:U\to\ob R^p$ et $g:U\to\ob R^p$ sont de classe $\sc C^k$, alors $\lambda f+\mu g$ est de classe $\sc C^k$ sur $U$ et 
$$
\forall a\in U, \qquad \d(\lambda f+\mu g)_a=\lambda\d f_a+\mu\d g_a.
$$
Si $f:U\to\ob R^p$ et $g:U\to\ob R$ sont de classe $\sc C^k$ sur $U$, alors $fg$ est de classe $\sc C^k$ sur $U$ et 
$$
\forall a\in U, \qquad \d(fg)_a=g(a)\d f_a +f(a)\d g_a.
$$ 
De plus, si $g(x)\neq0$ pour $a\in U$, alors $f/g$ est de classe $\sc C^k$ sur $U$ et 
$$
\forall a\in U, \qquad \d\Q({f\F g}\W)_a={g(a)\d f_a-f(a)\d g_a\F g(a)^2}.
$$ 

\Subsection cdiff§Aparti, Extrema.


\Definition [$a\in A\subset\ob R^n$] 
L'application $f:A\subset\ob R^n\to\ob R$ admet un minimum (resp. un maximum) relatif en $a$ si, et seulement si, 
il existe $r>0$ tel que 
$$
\forall x\in B(a,r)\cap A, f(x)\ge f(a)\qquad\b(\mbox{resp. }\ f(x)\le f(a)\b).
$$
On appelle extremum relatif tout maximum ou minimum relatif. 

\Definition [$a\in D\subset\ob R^n$] 
Le point $a$ est un point critique d'une fonction $f:D\to \ob R$, si et~seulement~si $f$ admet des dérivées partielles en $a$, 
qui sont nulles, i. e. 
$$
\forall k\in\{1,\cdots, n\}, \qquad {\partial f\F\partial x_k}(a)=0.
$$

\Remarque : Si $f$ est différentiable en un point critique $a$, on a forcément $\d f_a=0$. 
\bigskip


\Theoreme [$a\in U$ ouvert de $\ob R^n$] 
Si $f\in\sc C^1(U,\ob R)$ admet un extremum relatif en un point $a$ de l'{\bf ouvert} $U$, 
alors $a$ est un point critique de $f$.  

\Exemple. extrema de $(x,y)\mapsto \sqrt{x^2+y^2}-(x+y)/2$. 
\bigskip

\Remarque : C'est une condition nécéssaire mais pas suffisante, la réciproque est fausse. 
\bigskip

\Theoreme [$V$ voisinage de $a\in\ob R^2$]
Soit $f: V\to\ob R^p$ une application de classe $\sc C^2$ sur un voisinage de $a$. Alors, lorsque $h=(x,y)$ tends vers $(0,0)$, on a 
\Equation [{\bf(Taylor Young)}]
$$
f(a+h)=f(a)+\underbrace{{\partial f\F\partial x}(a)x+{\partial f\F\partial y}(a)y}_{\d f_a(x,y)}+
\underbrace{{\partial^2f\F\partial x^2}(a){x^2\F2}+{\partial^2f\F\partial x\partial y}(a)xy
+{\partial^2f\F\partial y^2}(a){y^2\F2}}_{q(x,y)}+o\b(\|h\|^2\b). 
$$

\Remarque : Si $a\in U$ est un point critique de l'application $f\in\sc C^2(U,\ob R^p)$, on a 
$$
f(a+h)=f(a)+{rx^2+2sxy+ty^2\F2}+o\b(x^2+y^2\b)\qquad\b(h\to(0,0)\b). 
$$
où l'on a posé $h=(x,y)$, 
$$
\ds r={\partial^2f\F\partial x^2}(a),\qquad 
\ds s={\partial^2f\F\partial x\partial y}(a)\quad \mbox{ et }\quad 
\ds t={\partial^2f\F\partial y^2}(a). \leqno{(*)}
$$

\Theoreme [$U$ ouvert de $\ob R^2$, $a\in U$ point critique de $f\in\sc C^2(U,\ob R)$] 
Soient $r$, $s$ et $t$ les nombres réels définis par ($*$). 
Alors, quatre cas se présentent : 
\medskip
\noindent 
1) Si $rt-s^2>0$ et $r<0$, la fonction $f$ admet un maximum relatif en $a$. \pn
2) Si $rt-s^2>0$ et $r>0$, la fonction $f$ admet un minimum relatif en $a$. \pn
3) Si $rt-s^2<0$, la fonction $f$ n'admet pas d'extremum relatif en $a$ {\it (selle de cheval).} \pn
4) Si $rt-s^2=0$, on ne peut conclure. {\it Il faudrait affiner l'estimation de $f$ en $a$.} 


\Exemple. Extrema de $(x,y)\mapsto x^2+y^2-1$, $(x,y)\mapsto x^2-y^2$ et de $(x,y)\mapsto x^2-2xy+y^2+y^n$ ?




\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Normes\Topologie\Suites\FonctionsDePlusieursVariables\Continuité\Dérivation\Extrema\EquationsAuxDérivéesPartielles}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}

\eject











\hautspages{Olus Livius Bindus}{Intégration sur un segment}%

\Chapter gah, Intégration sur un segment. 

\Section gah2, Fonctions de classe $\sc C^k$ par morceaux. 

\Concept Subdivisions

\Definition [$a<b$] 
Une subdivision d'un segment $[a,b]$ est une suite finie $\sigma=\{x_0,x_1,\cdots, x_n\}$ strictement croissante telle que $x_0=a$ et $x_n=b$. Autrement dit, telle que
$$
a=\underbrace{x_0<x_1<\cdots <x_{n-1} < x_n}_{\mbox{subdivision $\sigma$}}=b. 
$$ 

\Remarque : on note $\sigma\prec\sigma'$ et l'on dit que la subdivision $\sigma$ est plus fine que la subdivision~$\sigma'$ 
si la subdivision $\sigma$ contient tous les points de $\sigma'$ (et eventuellement d'autres). 
\bigskip

\Definition [$a<b$, $k\in\overline{\ob N}$] 
La fonction $f:[a,b]\to\ob K$ est de classe $\sc C^k$ par morceaux sur $[a,b]\ssi $ il existe une subdivision $\sigma=\{x_0,\cdots,x_n\}$ de $[a,b]$ telle que la restriction de $f$ à l'intervalle $\Q]x_{i-1},x_i\W[$ est prolongeable en une fonction de classe $\sc C^k$ sur $[x_{i-1},x_i]$ pour chaque $i\in\{1,\cdots,n\}$. 


\Propriete [$a<b$, $k\in\overline{\ob N}$] 
La fonction $f:[a,b]\to\ob K$ est de classe $\sc C^k$ par morceaux sur $[a,b]\ssi$ il existe une subdivision $\sigma=\{x_0,\cdots,x_n\}$ de $[a,b]$ telle que
$$
\sbox{pour }1\le i\le n\qquad 
\Q\{
\eqalign{
&f \sbox{ est de classe } \sc C^k \sbox{ sur l'intervalle ouvert } \Q]x_{i-1},x_i\W[    \cr
&\sbox{les dérivées } f, f',\cdots, f^{(k)} \sbox{ admettent une limite à droite en } x_{i-1} \cr
&\sbox{les dérivées } f, f',\cdots, f^{(k)} \sbox{ admettent une limite à gauche en } x_i
}
\W.
$$

\Remarque : une telle subdivision $\sigma$ est alors dite adaptée (ou subordonnée) à~$f$. 
\bigskip

\Remarque : on se moque totalement des valeurs de $f$ aux points $x_0, x_1, \cdots, x_n$. 
\bigskip

\Propriete [$a<b$, $k\in\overline{\ob N}$] L'ensemble des fonctions de classe $\sc C^k$ par morceaux sur $[a,b]$ 
est stable par addition, par multiplication externe, par produit. C'est un sous-espace vectoriel de $\sc F\big([a,b],\ob K\big)$. 

\Propriete [$a<b$, $k\in\overline{\ob N}$]
La composée $g\circ f$ d'une application $g$ de classe $\sc C^k$ avec une fonction $f$ de classe $\sc C^k$ 
par mor\-ceaux sur $[a,b]$ est de classe $\sc C^k$ par morceaux sur $[a,b]$. 

\Definition [$I$ intervalle, $k\in\overline{\ob N}$] 
La fonction $f:[a,b]\to\ob K$ est de classe $\sc C^k$ par morceaux sur l'intervalle $I$ $\ssi $ La fonction $f$ est de classe $\sc C^k$ par morceaux sur chaque segment $[a,b]\subset I$. 

\Exercice{PTakt}% 

\Definition [$k\in\overline{\ob N}$] 
Une fonction $T$-périodique $f:\ob R\to\ob K$ est de classe $\sc C^k$ par morceaux $\ssi $ la fonction $f$ est de classe $\sc C^k$ par morceaux sur l'une de ses période $[a,a+T]$. 

\Exercice{PTaku}%

\Propriete [$k\in\overline{\ob N}$]
La restriction au départ d'une fonction $\sc C^k$ par morceaux est de classe $\sc C^k$ par morceaux. 

\Section lab4, Intégrale d'une fonction continue par morceaux. 

\Definition[$a< b$] 
Si $f:[a,b]\to\ob R$ est continue par morceaux sur $[a,b]$, il existe une suite croissante $(\varphi_n)_{n\in\ob N}$ et une suite décroissante $(\psi_n)_{n\in\ob N}$ de fonctions en escaliers sur $[a,b]$ telles que 
$$
\forall n\in\ob N, \qquad \varphi_n\le f\le \psi_n\qquad\mbox{et}\qquad \psi_n-\varphi_n\le {1\F n+1}.\eqdef{gah}
$$
De plus, les suite $(\int_a^b\varphi_n)_{n\in\ob N}$ et $(\int_a^b\psi_n)_{n\in\ob N}$ convergent vers une même limite $\ell$, qui ne dépend pas du choix des suites $(\varphi_n)_{n\in\ob N}$ et $(\psi_n)_{n\in\ob N}$ vérifiant \eqref{gah}. \pn
Cette limite $\ell$ est appelée intégrale de la fonction $f$ sur le segment $[a,b]$ et on la note
$$
\int_a^bf(x)\d x=\int_{[a,b]}f= \int_a^bf:=\lim_{n\to+\infty}\int_a^b\varphi_n=\lim_{n\to+\infty}\int_a^b\psi_n.
$$
Par convention, on pose 
$$
\int_b^af(x)\d x:=-\int_a^bf(x)\d x\qquad \mbox{et}\qquad \int_a^af(x)\d x=0.
$$

\Remarque: Avant d'intégrer une fonction sur $[a,b]$, vérifier qu'elle est intégrable sur $[a,b]$, c'est-à-dire qu'elle est continue par morceaux sur $[a,b]$. 
\bigskip

\Remarque : le fait de modifier les valeurs de la fonction en un nombre fini d'endroits ne modifie pas son intégrale. 
\bigskip

\Definition [$a<b$] 
Si $f:[a,b]\to\ob C$ est une application continue par morceaux sur $[a,b]$, alors ses parties réelles et imaginaires $g:[a,b]\to\ob R$ et $h:[a,b]\to\ob R$ implicitement définies par 
$$
\forall x\in[a,b], \qquad f(x)=\underbrace{g(x)}_{\re f(x)}+i\underbrace{h(x)}_{\im f(x)}
$$
sont continues par morceaux et l'intégrale de $f$ de $a$ à $b$ est par définition le nombre 
$$
\int_a^bf(x)\d x:=\int_a^bg(x)\d x+i\int_a^bh(x)\d x.
$$

\Remarque : l'intégrale d'une fonction sur $[a,b]$ d'une fonction continue par morceaux est (par définition) l'aire (algèbrique) de la zone délimitée par l'axe des abscisses et le graphe de la fonction $f$ pour $a\le x\le b$. 
\bigskip
\Remarque  : le fait de modifier les valeurs de la fonction en un nombre fini de points ne modifie pas son intégrale. 
\bigskip

\Section lab6, Propriétés fondamentales de l'intégrale. 
\bigskip

\Propriete [$I$ intervalle, $(a,b,c)\in I^3$] 
Si $f:I\to\ob K$ est une fonction continue par morceaux sur $I$, alors la fonction $f$ est continue par morceaux sur les segments $[a,b]$, $[b,c]$, $[a,c]$ et 
\Equation [\bf Relation de Chasles]
$$
\int_a^cf(x)\d x=\int_a^bf(x)\d x+\int_b^cf(x)\d x. 
$$

\Propriete [$(a,b)\in\ob R^2$, $(\lambda,\mu)\in\ob K^2$] 
Si $f:[a,b]\to\ob K$ et $g:[a,b]\to\ob K$ sont deux fonctions continues par morceaux sur $[a,b]$, alors la fonction $\lambda f+\mu g$ est continue par morceaux sur $[a,b]$ et 
\Equation [\bf Linéarité]
$$
\int_a^b\big(\lambda f(x)+\mu g(x)\big)\d x=\lambda \int_a^bf(x)\d x+\mu \int_a^bg(x)\d x. 
$$

\Propriete [$a\le b$] 
Si $f:[a,b]\to\ob R$ est une fonction continue par morceaux sur $[a,b]$, alors 
\Equation [\bf Positivité]
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\ge0}_{f\ge0}\qquad\Longrightarrow\qquad \int_a^bf(x)\d x\ge0.
$$ 


\Propriete [$a\le b$]
Si $f:[a,b]\to\ob R$ et $g:[a,b]\to\ob R$ sont continues par morceaux sur $[a,b]$, alors
\Equation [ \bf Croissance]
$$
\underbrace{
\forall x\in[a,b],\quad f(x)\le g(x)}_{f\le g}\qquad\Longrightarrow\qquad \int_a^bf(x)\d x\le \int_a^bg(x)\d x.
$$ 

\Propriete[$a\le b$] 
Si $f:[a,b]\to\ob R$ est continue par morceaux sur $[a,b]$, alors l'application $|f|$ est continue par morceaux sur $[a,b]$ et 
$$
\Q|\int_a^bf(x)\ dx\W|\le \int_a^b\big|f(x)\big|\d x.
$$ 

\Propriete [$a\le b$] 
Si $f:[a,b]\to\ob K$ et $g:[a,b]\to\ob K$ sont continues par morceaux sur $[a,b]$, alors 
\Equation [\bf Inégalité de la moyenne]
$$
\Q|\int_a^bf(x)g(x)\d x\W|\le \sup_{a\le x\le b}\b|f(x)\b|\times\int_a^b\big|g(x)\big|\d x.
$$
En particulier, on a 
$$
\Q|\int_a^bf(x)\d x\W|\le (b-a)\sup_{a\le x\le b}\b|f(x)\b|.
$$



\Theoreme [$a<b$] 
Si $f:[a,b]\to\ob R$ est une fonction 
{\bf continue} et à valeurs positives ou nulles sur $[a,b]$, alors 
$$
\underbrace{\forall x\in[a,b],\quad f(x)=0}_{f=0}\qquad\Longleftrightarrow \qquad\int_a^bf(t)\d t=0.
$$

\Section mdr, Développements limités. 

\Subsection def, Développement limité en un point. 
\bigskip

\Definition [$a\in i$ intervalle] 
La fonction $f:I\to\ob K$ admet $P\in\ob K_n[X]$ comme développement limité en~$a$ à l'ordre $n\Leftrightarrow$ 
$$
f(x)=\underbrace{\sum_{k=0}^n\alpha_k(x-a)^k}_{P(x)}+o_a\B((x-a)^n\B).
$$
Le développement limité de $f$ en $a$ à l'ordre $n$ est unique. 
\bigskip

\Propriete [$a\in i$ intervalle, $0\le m\le n$] 
Si $P(x)=\sum_{k=0}^n\alpha_k(x-a)^k$ est le développement limité de $f:I\to\ob K$ en $a$ à l'ordre $n$, i.e. 
$$
f(x)=\underbrace{\sum_{k=0}^n\alpha_k(x-a)^k}_{P(x)}+o_a\B((x-a)^n\B), 
$$
alors la troncature $Q(x)=\sum_{k=0}^m\alpha_k(x-a)^k$ à l'ordre $m$ de $P$ est le développement limité 
de $f$ en $a$ à l'ordre $m$, i.e. 
$$
f(x)=\underbrace{\sum_{k=0}^m\alpha_k(x-a)^k}_{Q(x)}+o_a\B((x-a)^m\B).
$$


\Subsection def, Opérations algébriques. 

\Propriete [$P$ et $Q$ développements limités de $f$ et $g$ en $a$ à l'orde $n$]
La somme $f+g$ admet en $a$ un développement limité à l'ordre $n$ égale à $P+Q$ . 


\Propriete [$P$ et $Q$ développements limités de $f$ et $g$ en $a$ à l'orde $n$]
Le produit $f\times g$ admet en $a$ un développement limité à l'ordre $n$, donné par la troncature du polynôme $P\times Q$ à l'ordre $n$. 

\Propriete [$P$ et $Q$ développements limités de $f$ et $g$ en $a$ à l'orde $n$] 
Si $g(a)\neq0$, le quotient $f/g$ admet en $a$ un développement limité à l'ordre $n$, donné par la division suivant les puissances croissantes de $P$ par $Q$ à l'ordre $n$. 


\Propriete [$n\ge0$] 
$$
{1\F 1-u}=\sum_{k=0}^nu^k+o_0(u^n).
$$

\Remarque : on peut utiliser la propriété précédente pour calculer le développement limité d'un quotient (plutot que d'effectuer la division suivant les puissances croissantes). 
\bigskip

\Theoreme [$a\in I$ intervalle]
Si $f$ est de classe $\sc C^n$ sur $I$, alors $f$ admet un développement limité à l'ordre $n$ en $a$ et 
\Equation [\bf Formule de Taylor Young]
$$
f(x)=\sum_{k=0}^n{f^{(k)}(a)\F k!}(x-a)^k+o_a\B((x-a)^n\B).
$$

\Propriete [$a\in I$ intervalle]
Si $f:I\to\ob K$ est une fonction dérivable sur $I$ dont la dérivée $f'$ admet en $a$ le développement limité à l'ordre $n$ suivant 
$$
f'(x)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B), 
$$
alors, la fonction $f$ admet en $a$ un développement limité à l'ordre $n+1$ donné par 
$$
f(x)=f(0)+\sum_{k=0}^{n+1}\alpha_k{(x-a)^{k+1}\F k+1}+o_a\B((x-a)^{n+1}\B).
$$

\Propriete [$a\in I$ intervalle] Si $f:I\to\ob K$ est une fonction continue sur $I$ admettant en $a$ le développement limité à l'ordre $n$ suivant 
$$
f(x)=\sum_{k=0}^n\alpha_k(x-a)^k+o_a\B((x-a)^n\B), 
$$
alors, une primitive $F$ de $f$ admet en $a$ un développement limité à l'ordre $n+1$ donné par 
$$
F(x)=F(a)+\sum_{k=0}^{n+1}\alpha_k{(x-a)^{k+1}\F k+1}+o_a\B((x-a)^{n+1}\B). 
$$

\Remarque : on peut donc intégrer un développement limité pour trouver celui d'une primitive (on y gagne un ordre de précision). Ne pas oublier la constante d'intégration 
\bigskip

\Remarque : {\bf si $f'$ admet un développement limité}, $f$~admet aussi un développement limité que l'on peut dériver pour trouver celui de $f'$ (on y perd un ordre de précision). 
\bigskip











\Section intgen, Intégrales généralisées.

\Subsection intimp, Intégrales impropres.


\Definition [$f$ continue par morceaux sur $I$ intervalle d'extrémités $a\in \ob R$ inclus et $b\in\overline{\ob R}$ exclus] 
L'intégrale impropre $\int_a^bf(t)\d t$ converge $\Longleftrightarrow$ la limite $\lim\limits_{\ss x\to b\atop\ss x\in I}\int_a^xf(t)\d t$ existe (et est finie), 
\vskip-1em \noindent auquel cas, on pose\vskip.5em
$$
\int_a^bf(t)\d t:=\lim_{\ss x\to b\atop\ss x\in I}\int_a^xf(t)\d t. 
$$ 
Lorsque l'intégrale $\int_a^bf(t)\d t$ ne converge pas, on dit qu'elle diverge.

\Exemple. $\int_0^1\ln t\d t=-1$. 

\Propriete [$\alpha\in\ob R$]
$$
\sbox{L'intégrale impropre }\int_0^{+\infty}\e^{-\alpha t}\d t \sbox{ converge }\ssi \alpha>0.
$$ 

\Propriete [$\alpha\in\ob R$]
$$
\eqalignno{
\qquad\qquad\qquad\qquad\qquad\qquad\sbox{L'intégrale }\int_1^{+\infty}t^{-\alpha}\d t \sbox{ converge }&\ssi \alpha>1& 
\mbox{(\bf Intégrales de Riemann)}\cr
\sbox{L'intégrale }\int_0^1t^{-\alpha}\d t \sbox{ converge }&\ssi\alpha<1.&}
$$

\Propriete [$I$ intervalle d'extrémités $a\in \ob R$ inclus et $b\in\overline{\ob R}$ exclus] 
Si l'application $f:\Q]a,b\W[\to\ob C$ est continue par morceaux sur $\Q[a,b\W[$ et si $c\in\Q[a,b\W[$, alors les~intégrales $\int_a^bf(t)\d t$ et $\int_c^bf(t)\d t$ ont même nature. En cas de convergence, on a 
\Equation [Relation de Chasles]
$$
\int_a^bf(t)\d t=\int_a^cf(t)\d t+\int_c^bf(t)\d t. 
$$

\Definition [$k\ge 1$ et $-\infty\le a_0<a_1<\cdots<a_k\le +\infty$, suite 
strictement croissante]
Si $f:\Q]a_0,a_k\W[\to\ob C$ est continue par morceaux l'intervalle $\Q]a_{n-1},a_n\W[$ pour $1\le n\le k$. Alors, 
l'intégrale $\int_{a_0}^{a_k}f(t)\d t$ converge $\Longleftrightarrow$ pour chaque $n\in\{1,\cdots,k\}$, il~existe $c_n\in\Q]a_{n-1},a_n\W[$ tel que 
les intégrales $\int_{a_{n-1}}^{c_n}f(t)\d t$ et $\int_{c_n}^{a_n}f(t)\d t$ convergent. Dans ce cas, on pose
$$
\int_{a_0}^{a_k}f(t)\d t=\sum_{1\le n\le k}\Q(\int_{a_{n-1}}^{c_n}f(t)\d t+\int_{c_n}^{a_n}f(t)\d t\W). 
$$


\Exemple. L'intégrale $\ds\int_{-1}^1{\d t\F t}$ diverge.
\bigskip

\Exemple. Les intégrales $\ds \int_{-1}^1{\d t\F\sqrt{1-t^2}}$ et $\ds\int_{-\infty}^{+\infty}\e^{-|t|}\d t$ 
convergent. 
\bigskip


\Subsection rah, Propriétés.


\Propriete [$(a,b)\in\ob R^2$, $a\le b$] 
Si $f:\Q[a,b\W[\to\ob C$ est une application continue sur $\Q[a,b\W[$ admettant une limite finie en $b$, 
alors la fonction~$\tilde f$ définie par 
$$
\tilde f(x):=\Q\{
\eqalign{
f(x)&\sbox{ si }a\le x<b\cr 
\lim_{x\to b}f(t)&\sbox{ si }x=b\cr}\W.
$$
est continue sur $[a,b]$ et l'intégrale impropre $\int_a^bf(t)\d t$ converge et vérifie 
\Equation [Fausse intégrale impropre]
$$
\underbrace{\int_a^bf(t)\d t}_{\mbox{\sevenrm intégrale généralisée}}
=\underbrace{\int_a^b\tilde f(t)\d t.}_
{\mbox{\sevenrm integrale ``normale''}}
$$


\Exemple. $\ds \int_0^1{\sin t\F t}\d t$ est une fausse intégrale impropre. 
\bigskip


\Propriete [$(a,b)\in\overline{\ob R}^2$, $\lambda\in\ob C^*$]
Si $f:\Q]a,b\W[\to\ob C$ est continue par morceaux sur~$\Q]a,b\W[$, alors $\int_a^b \lambda f(t)\d t$ et $\int_a^b f(t)\d t$ ont même nature et, 
en cas de convergence, vérifient 
\Equation [multiplication par une constante non nulle]
$$
\int_a^b \lambda f(t)\d t=\lambda \int_a^b f(t)\d t. 
$$


\Propriete [$(a,b)\in\overline{\ob R}^2$] 
Soient $f$ et $g$ deux fonctions continues par morceaux sur $\Q]a,b\W[$. \pn
Si $\int_a^bf(t)\d t$ converge et si $\int_a^bg(t)\d t$ diverge, alors $\int_a^b\b(f(t)+g(t)\b)\d t$ diverge. \medskip\noindent
Si $\int_a^bf(t)\d t$ et $\int_a^bg(t)\d t$ convergent, alors $\int_a^b\b(f(t)+g(t)\b)\d t$ converge et l'on a 
\Equation [\bf addition]
$$
\int_a^b\b(f(t)+g(t)\b)\d t=\int_a^bf(t)\d t+\int_a^bg(t)\d t. 
$$

\Exemple. $\ds\int_0^1{\cos(2t)\F t}\d t=\int_0^1{1-\sin(2t)^2\F t}\d t$ diverge car $\ds\int_0^1{\d t\F t}$ diverge et $\ds\int_0^1{\sin(2t)^2\F t}\d t$ converge. 
\bigskip

\Remarque : $\int_a^b\b(f(t)+g(t)\b)\d t$ peut converger alors que $\int_a^bf(t)\d t$ et $\int_a^bg(t)\d t$ divergent. Dans ce cas, vous n'avez pas le droit d'écrire que 
$$
\int_a^b\b(f(t)+g(t)\b)\d t=\underbrace{\int_a^bf(t)\d t}_{\mbox{\Red{diverge!}}}
+\underbrace{\int_a^bg(t)\d t. }_{\mbox{\Red{diverge !}}}
$$

\Exercice{akv}%

\Propriete [$(a,b)\in\ol{\ob R}^2$, {$f:\Q]a,b\W[\to \ob R$ et $g:\Q]a,b\W[\to\ob R$ continues par morceaux sur~$\Q]a,b\W[$}] 
L'intégrale $\int_a^b\b(f(t)+ig(t)\b)\d t$ converge $\Leftrightarrow$ les intégrales $\int_a^bf(t)\d t$ et $\int_a^bg(t)\d t$ convergent. 
Dans ce cas, on a 
$$
\int_a^b\b(f(t)+ig(t)\b)\d t=\int_a^bf(t)\d t+i\int_a^bg(t)\d t. 
$$

\Exemple. $\ds\int_0^1{\e^{2it}-1\F t}\d t$ converge car $\ds\int_0^1{\cos(2t)-1\F t}\d t$ et $\ds\int_0^1{\sin(2t)\F t}\d t$ convergent. 
\bigskip

\Concept Intégration par partie

\Theoreme[$a$ et $b$ dans $\ol{\ob R}$, $f$ et $g$ deux fonctions de classe $\sc C^1$ sur {$\Q]a,b\W[$}] 
Si deux des trois nombres $\int_a^bf'(t)g(t)\d t$, $\int_a^bf(t)g'(t)\d t$ et 
$$
\Big[f(t)g(t)\Big]_a^b:=\lim\limits_{t\to b}\Big(f(t)g(t)\Big)-\lim\limits_{t\to a}\Big(f(t)g(t)\Big)
$$
sont définis, alors le troisième l'est aussi et l'on a 
\Equation [\bf Intégration par partie]
$$
\int_a^bf'(t)g(t)\d t=\Big[f(t)g(t)\Big]_a^b-\int_a^bf(t)g'(t)\d t. 
$$



\Concept Théorème de changement de variable


\Theoreme [$a<b$ dans $\overline{\ob R}$, $\varphi$ difféomorphisme de classe $\sc C^1$ de {$\Q]a,b\W[$} dans un intervalle $I$]
Si $f:I\to\ob C$ est continue par morceaux sur $I$. 
Alors $\int_{\varphi(a)}^{\varphi(b)}f(x)\d x$ et $\int_a^bf\big(\varphi(t)\big)\varphi'(t)\d t$ 
ont même nature. En cas de convergence, on a 
\Equation [\bf Changement de variable]
$$
\int_{\varphi(a)}^{\varphi(b)}f(x)\d x=\int_a^bf\big(\varphi(t)\big)\varphi'(t)\d t.
$$
{\it Remarque : si $\varphi$ est croissante on a $J=\Q]\varphi(a), \varphi(b)\W[$, sinon on a $J=\Q]\varphi(b),\varphi(a)\W[$. }

\Exemple. $\ds\int_0^\pi{\sin t\F\sqrt{1+\cos t}}\d t$ et $\ds\int_{-1}^1{\d x\F\sqrt{1-x}}$ ont même nature car $t\mapsto -\cos t$ 
est un difféomorphisme de classe $C^1$ de $\Q]0,\pi\W[$ sur $\Q]-1,1\W[$. 
\bigskip

\Remarque : Le non-respect des hypothèses de ce théorème mène à des horreurs, surtout avec les changement de variables trigonométriques... 
\medskip

\Exercice{PTakw}%

\Propriete [$(a,b)\in\ol{\ob R}^2$ et {$f:\Q]a,b\W[\to\ob C$ continue par morceaux sur~$\Q]a,b\W[$}]
Si l'intégrale $\int_a^b\big|f(t)\big|\d t$ converge, alors l'intégrale $\int_a^bf(t)\d t$ converge et l'on a 
\Equation [\bf convergence absolue]
$$
\Q|\int_a^bf(t)\d t\W|\le \int_a^b\b|f(t)\b|\d t. 
$$

\Concept Convergence absolue

\Definition
L'intégrale $\int_a^bf(t)\d t$ converge absolument $\ssi$ l'intégrale $\int_a^b\big|f(t)\big|\d t$ converge. 

\Exemple. $\ds\int_1^\infty{\d t\F t^2}$ converge, donc $\ds \int_1^\infty{\e^{it}\F t^2}\d t$ 
est absolument convergente. 
\bigskip

\Concept Semi-convergence

\Definition
L'intégrale $\int_a^bf(t)\d t$ est ``semi-convergente'' $\ssi$ $\int_a^bf(t)\d t$ converge et $\int_a^b\big|f(t)\big|\d t$ diverge.

\Exemple. $\ds\int_1^\infty{\sin t\F t}\d t$ est semi-convergente car elle converge et car $\ds\int_1^\infty{|\sin t|\F t}\d t$ diverge.  
\bigskip


\Concept [] Fonctions intégrables

\Definition [$I$ intervalle]
Une fonction $f:I\to\ob C$ continue par morceaux sur $I$ est intégrable sur $I\ssi$ $f$ admet sur $I$ une intégrale absolument convergente $\ssi$ il existe un réel $M\ge0$ tel que 
$$
\sbox{Pour tout segment } [a,b]\subset I, \qquad \int_a^b\big|f(t)\big|\d t\le M
$$
Dans ce cas, on appelle intégrale de $f$ sur $I$, l'intégrale (eventuellement généralisée)
$$
\int_If(t)\d t:=\int_{\inf I}^{\sup I}f(t)\d t. 
$$

\Section Intpos, Intégrales généralisées des fonctions positives. 

\Subsection intposprop, Propriété fondamentale. 


\Propriete [$a\in\ob R$, $b>a$ un élément de $\ol{\ob R}$, {$f:\Q[a,b\W[\to\ob R$} 
une fonction croissante]
Si $f$ est majorée sur $\Q[a,b\W[$, alors $\lim\limits_{x\to b}f(x)=\sup\{f(x):a\le x<b\}$. \medskip\noindent
Si $f$ n'est pas majorée sur $\Q[a,b\W[$, alors $\lim\limits_{x\to b}f(x)=+\infty$. 



\Propriete [$a\in\ob R$, $b>a$ dans $\ol{\ob R}$, {$f:\Q]a,b\W]\to\ob R$} continue par morceaux] 
Si $f$ est positive sur $\Q[a,b\W[$, alors l'intégrale $\int_a^bf(t)\d t$ converge $\Longleftrightarrow$ $x\mapsto\int_a^xf(t)\d t$ est une fonction majorée sur $\Q[a,b\W[$ $\Longleftrightarrow$ il existe $M\ge0$ tel que 
$$
\Q|\int_a^xf(t)\d t\W|\le M\qquad(a\le x<b)
$$ 


\Exemple. $\int_0^1\sin{1\F t}\d t$ est absolument convergente. 
\medskip

\Subsection Int, Intégration des relations de comparaison. 

\Propriete [$a\in\ob R$, $b>a$ un élément de $\ol R$] 
Soient $f:\Q[a,b\W[\to\ob R$ et $g:\Q[a,b\W[\to\ob R$ deux applications continues par morceaux sur~$\Q[a,b\W[$ telles que 
$$
0\le f(x)\le g(x)\qquad (a\le x<b). 
$$
Si $\int_a^bf(t)\d t$ diverge, alors $\int_a^bg(t)\d t$ diverge. \medskip\noindent
Si $\int_a^bg(t)\d t$ converge alors $\int_a^bf(t)\d t$ converge et l'on a 
\Equation [\bf Intégration des inégalités]
$$
0\le \int_a^bf(t)\d t\le \int_a^bg(t)\d t. 
$$ 

\Exemple. $\ds\int_0^1{\d t\F t+t^2}$ diverge car ${1\F t+t^2}\ge{1\F 2t}$ pour $0<t\le 1$. 

\Exemple. $\ds\int_0^\infty{\sin t\d t\F1+t^2}$ converge absolument et vérifie 
$\ds\Q|\int_0^\infty{\sin t\d t\F1+t^2}\W|\le {\pi\F2}$. 
\medskip

\Theoreme [$a\in\ob R$ et $b>a$ élément de $\ol R$]
Soient $f$ et $g$ des fonctions continues par morceaux et {\bf positives} sur $\Q[a,b\W[$ telles que 
$$
f(x)=o_b\b(g(x)\b).
$$ 
Si $\int_a^b f(t)\d t$ diverge, alors $\int_a^bg(t)\d t$ diverge et l'on a 
\Equation [\bf intégration des petits $o$]
$$
\underbrace{\int_a^x f(t)\d t}_{F(x)}=o_b\Bigg(\underbrace{\int_a^xg(t)\d t}_{G(x)}\Bigg). 
$$
Si $\int_a^b g(t)\d t$ converge, alors $\int_a^bf(t)\d t$ converge et l'on a 
\Equation [\bf intégration des petits $o$]
$$
\underbrace{\int_x^b f(t)\d t}_{F(b)-F(x)}.=o_b\Bigg(\underbrace{\int_x^bg(t)\d t}_{G(b)-G(x)}\Bigg). 
$$

\Remarque : Ce théorème permet notament d'intégrer les développements asymptotiques. 
\bigskip

\Application : $\ds \arccos(1-u)=\int_{1-u}^1{\d t\F\sqrt{1-t^2}}
=\int_0^u{\d x\F\sqrt{2x-x^2}}=\sqrt{2u}+{u\sqrt u\F6\sqrt2}+o(u^2)\qquad(u\to0^+)$. 

\Exercice{PTakx}%
\bigskip

\Remarque : Ecrire $\ds \int_1^\infty{\sin t\F \sqrt t}\d t$ converge 
et $\ds{\sin^2 t\F t}=o\Q({\sin t\F\sqrt t}\W)$ lorsque $t$ tends vers $+\infty$ 
donc l'intégrale $\ds \int_1^\infty{\sin^2 t\F t}\d t$ converge est une horreur...
\bigskip


\Theoreme [$a\in\ob R$ et $b>a$ un élément de $\ol R$] 
Soient $f$ et $g$ des fonctions continues par morceaux et {\bf positives} sur $\Q[a,b\W[$ telles que 
$$
f(x)\mathop{\sim}\limits_bg(x).
$$ 
Alors,~$\int_a^bf(t)\d t$ et $\int_a^bg(t)\d t$ ont même nature. 
De plus, en cas de divergence, on a 
\Equation [\bf intégration des équivalences]
$$
\underbrace{\int_a^xf(t)\d t}_{F(x)}\ \mathop{\sim}_b\ \underbrace{\int_a^xg(t)\d t}_{G(x)}. 
$$
et en cas de convergence, on a 
\Equation [\bf intégration des équivalences]
$$
\underbrace{\int_x^bf(t)\d t}_{F(b)-F(x)}\ \mathop{\sim}_b\ \underbrace{\int_x^bg(t)\d t}_{G(b)-G(x)}.
$$

\Exemple. Comme $\ds \int_0^1{\d t\F\arctan t}$ diverge, on a $\ds \int_x^1{\d t\F\arctan t}\ \mathop{\sim}\limits_{0^+}\ -\ln x$. 
\bigskip

\Exemple. Comme $\ds \int_4^\infty{\d t\F t^2-3t}$ converge, on a $\ds\int_x^\infty{\d t\F t^2-3t}\ \mathop{\sim}_{\infty}\ {1\F x}$. 
\bigskip

\Remarque : Si $f(t)\sim g(t)>0$ lorsque $t\to b$, alors $f(t)>0$ lorsque $t\to b$. 
\bigskip

\Section gah, Intégrales à un paramètre. 
\bigskip


\Subsection asf, Continuité des intégrales à un paramètre. 

\Theoreme [Index=Theoreme@Théorème!de continuite@de continuité des intégrales à un paramètre;Title=Théorème de continuité des intégrales {\it généralisées}à un paramètre;$a<b$ dans $\ol{\ob R}$, $I$ intervalle, $f:(t,x)\mapsto f(t,x)$ application] 
Si l'application $t\mapsto f(t,x)$ est continue par morceaux sur $\Q]a,b\W[$ pour chaque $x\in I$ fixé, \pn
si l'application $x\mapsto f(t,x)$ est continue sur $I$ pour chaque nombre réel $t\in\Q]a,b\W[$ fixé et \pn
s'il existe $g:\Q]a,b\W[\to\ob R^+$ continue par morceaux telle que $\int_a^bg(t)\d t$ converge et telle que 
\Equation [hypothèse de domination]
$$
\big|f(t,x)\big|\le g(t)\qquad(a<t<b, x\in I), 
$$
alors l'intégrale $F(x):=\int_a^bf(t,x)\d t$ converge absolument pour chaque nombre $x\in I$ et 
la fonction $F:I\to\ob C$ est continue sur $I$. En particulier, pour chaque $x_0\in I$, on a 
\Equation [Convergence dominée] 
$$
\lim_{x\to x_0}\underbrace{\int_a^bf(t,x)\d t}_{F(x)}=\underbrace{\int_a^b\lim_{x\to x_0}f(t,x)\d t}_{F(x_0)}. 
$$

\sidx{Theoreme@Théorème!de convergence dominee@de convergence dominée}%
\sidx{Theoreme@Théorème!de Lebesgue}%

\Remarque : ce théorème de continuité des intégrales {\it généralisées} à un pramètre est également appelé théorème de convergence dominé ou théorème de Lebesgue.

\Remarque : si l'application $(t,x)\mapsto f(t,x)$ est continue sur $\Q]a,b\W[\times I$, les deux premières hypothèses du théorème sont automatiquement vérifiées. 
\bigskip

\Remarque : Ce théorème permet d'intervertir limite et intégration. 
\bigskip

\Remarque : comme ce théorème ne permet pas de faire tendre $x$ vers $\pm\infty$ (certaines de ses variantes hors programme le permettent), nous devrons pour cela contourner la difficulté en posant $x={1/u}$ pour se ramener à une limite en $0^+$ ou en $0^-$. 
\bigskip

\Application : Calculer la limite $\ds\lim_{x\to+\infty}\int_0^1{x\F x\e^{it}+t}\d t$. 
\bigskip

\Concept Variante pour les intégrales non-impropres. 

\Propriete [$a<b$ dans $\ob R$, $I$ intervalle] Si $f:(t,x)\mapsto f(t,x)$ est continue sur $[a,b]\times I$ à valeurs dans $\ob C$, alors l'application
$$
\eqalign{F:&I\to\ob C\cr
x&\mapsto\int_a^bf(t,x)\d t}
$$ 
est définie et continue sur l'intervalle $I$. 

\Application : La fonction $\ds F:x\mapsto\int_0^1\sqrt{1+\cos(tx)}\d t$ est continue sur $\ob R$. 
\bigskip


\Remarque : Ce théorème est également vrai lorsque l'on remplace l'intervalle $I$ par un ouvert $U$ de $\ob R^n$. 
\bigskip

\Application : La fonction $\ds F:(x,y)\mapsto\int_0^1\e^{xt-yt^2}\d t$ est continue sur $\ob R^2$. 
\bigskip


\Subsection asf, Dérivation des intégrales à un paramètre. 

\Theoreme [Index=Theoreme@Théorème!de derivation des integrales a un parametre@de dérivation des intégrales à un paramètre;Title=Théorème de dérivation des intégrales {\it généralisées} à un paramètre;$a<b$ dans $\ol{\ob R}$, $I$ intervalle, $f:(t,x)\mapsto f(t,x)$ application] 
Si l'application $x\mapsto \ds{\partial f\F\partial x}(x,t)$ est définie et continue sur $I$ pour $t\in\Q]a,b\W[$ fixé, \pn
si $t\mapsto\ds{\partial f\F\partial x}(x,t)$ et $t\mapsto f(t,x)$ sont $\sc C^0$ par morceaux et intégrables sur~$\Q]a,b\W[$ pour~$x\in I$ et 
s'il existe $h:\Q]a,b\W[\to\ob R^+$ continue par morceaux telle que~$\int_a^bh(t)\d t$ converge et telle que 
\Equation [hypothèse de domination]
$$
\Q|{\partial f\F\partial x}(t,x)\W|\le h(t)\qquad(a<t<b, x\in I), 
$$
alors l'intégrale $F(x):=\int_a^bf(t,x)\d t$ converge absolument pour chaque nombre $x\in I$ et 
la fonction $F:I\to\ob C$ est de classe~$\sc C^1$ sur $I$ et vérifie
\Equation [Dérivation sous l'intégrale]
$$
\forall x\in I, \qquad {\d\F\d x}\underbrace{\int_a^bf(t,x)\d t}_{F(x)}=\int_a^b{\partial f\F\partial x}(t,x)\d t. 
$$

\Remarque : si l'application $t\mapsto\ds{\partial f\F\partial x}(x,t)$ est continue par morceaux et vérifie l'ypothèse de domination, elle est automatiquement intégrable sur $\Q]a,b\W[$. 
\bigskip

\Remarque : si l'application $(t,x)\mapsto f(t,x)$ est de classe $\sc C^1$ sur $\Q]a,b\W[\times I$, les trois premières hypothèses du théorème sont vérifiées. 
\bigskip

\Remarque : Ce théorème permet d'intervertir dérivation et intégration. 
\bigskip

\Remarque : le théorème est encore vrai si l'on enlève ``intégrable'' de la deuxième hypothèse et si l'on ajoute que l'intégrale $\int_a^bf(t,x)\d t$ doit converger pour au moins un $x_0\in I$. 
Cette~version plus puissante n'étant pas officiellement au programme, nous contournerons la difficulté en intégrant par partie. 
\bigskip

\Application : la fonction définie par $F(x)=\int_1^\infty{\sin t\F x+t}\d t$ est dérivable sur $\Q[0, +\infty\W[$. 

\Concept [Index=Theoreme@Théorème!de derivation des integrales a un parametre@de dérivation des intégrales à un paramètre] Théorème de dérivation multiple des intégrales {\it généralisées} à un paramètre

\Theoreme [$k\in\ob N^*$, $a<b$ dans $\ol{\ob R}$, $I$ intervalle, $f:(t,x)\mapsto f(t,x)$ application] 
Si l'application $x\mapsto f(x,t)$ est de classe $\sc C^k$ sur l'intervalle $I$ pour $t\in\Q]a,b\W[$ fixé, \pn
si $t\mapsto\ds{\partial f^n\F\partial x^n}(x,t)$ est continue par morceaux et intégrable sur $\Q]a,b\W[$ pour $x\in I$ et $0\le n\le k$ fixés et 
s'il~existe $g:\Q]a,b\W[\to\ob R^+$continue par morceaux tel que~$\int_a^bg(t)\d t$ converge, avec 
\Equation [hypothèse de domination]
$$
\Q|{\partial^kk\F\partial x^k}(t,x)\W|\le g(t)\qquad(a<t<b, x\in I),
$$
alors la fonction définie par $F(x):=\int_a^bf(t,x)\d t$ est de classe~$\sc C^k$ sur $I$ et vérifie
$$
\forall n\in\{0, \cdots, k\}, \quad \forall x\in I, \qquad {\d^n\F\d x^n}\underbrace{\int_a^bf(t,x)\d t}_{F(x)}=\int_a^b{\partial^n f\F\partial x^n}(t,x)\d t. 
$$


\Concept [Index=Theoreme@Théorème!de derivation des integrales a un parametre@de dérivation des intégrales à un paramètre] Théorème de dérivation multiple des intégrales à un paramètre

\Propriete [$k\in\ob N\cup\{\infty\}$, $a<b$ dans $\ob R$, $I$ intervalle]
Si la fonction $f:(t,x)\mapsto f(t,x)$ est de classe $\sc C^k$ sur $[a,b]\times I$, alors l'application 
$$
\eqalign{
F:&I\to\ob C\cr
x&\mapsto\int_a^bf(x,t)\d t}
$$
est de classe~$\sc C^k$ sur~l'intervalle~$I$. De plus, pour $0\le n\le k$, on a 
$$
\forall x\in I, \qquad {\d^n\F\d x^n}\underbrace{\int_a^bf(t,x)\d t}_{F(x)}=\int_a^b{\partial^n f\F\partial x^n}(t,x)\d t. 
$$

\Remarque : Ce théorème est également vrai lorsque l'on remplace l'intervalle $I$ par un ouvert $U$ de $\ob R^n$. 
\bigskip

\Application : La fonction $\ds F:(x,y)\mapsto\int_0^1\e^{xt-yt^2}\d t$ est de classe $\sc C^\infty$ sur $\ob R^2$. 
\bigskip



\Subsection rah, Intégration des intégrales à un paramètre. 
\medskip


\Concept Théorème de Fubini

\Theoreme [$a<b$ et $c<d$ dans $\ob R$] 
Si $f:(t,x)\mapsto f(t,x)$ est une application continue sur $[a,b]\times[c,d]$, alors on a 
$$
\int_a^b\bigg(\int_c^df(t,x)\d x\bigg)\d t=\int\limits_{[a,b]\times[c,d]}
\!\!f(t,x)\d t\d x=\int_c^d\bigg(\int_a^bf(t,x)\d t\bigg)\d x.
$$







\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Intégrales\Intégration\IntégralesGénéralisées\FonctionsDéfiniesParUneIntégrale}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}










\hautspages{Olus Livius Bindus}{Séries}%

\Chapter series, Séries. 

\Section seriess, Séries numériques. 

Etant donnée des nombres complexes $(u_n)_{n\in\ob N}$, nous étudions dans cette section la série $\sum_{u=0}^\infty u_n$, c'est-à-dire la suite des sommes partielles définie par  
$$
S_N:=\sum_{n=0}^Nu_n\qquad(N\in\ob N).
$$
Nous voulons essentiellement savoir si cette suite admet une limite, que  nous calculons lorsque c'est possible. 
\medskip

Pour étudier l'existence de la limite $\sum_{n=0}^\infty u_n$, nous nous assurons d'abord que  la condition nécéssaire de convergence   
$$
\lim_{n\to+\infty}u_n=0
$$
est satisfaite puis nous utilisons, au besoin, les outils suivants : 
\item{1.} Séries téléscopiques (permet le calcul de la limite)
\item{2.} Théorème spécial des séries alternées
\item{3.} Changement d'indice
\item{4.} Relation de Chasles
\item{5.} Convergence absolue
\item{6.} Sommation des équivalents
\item{7.} Sommation des inégalités
\item{8.} Sommation des $o$
\item{9.} Comparaison série-intégrale

\Subsection seriesdef, Définitions. 

Dans toute cette section, le symbole $\ob K$ désigne le corps $\ob R$ ou le corps $\ob C$. 
\bigskip

\Definition [$(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$]
On appelle suite des sommes partielles de $(u_n)$ la suite $(S_n)_{n\in\ob N}$ définie par 
$$
\forall n\in\ob N, \qquad S_n:=\sum_{k=0}^nu_k
$$ 
et on appelle série de terme général $(u_n)$ le couple $\b((u_n)_{n\in\ob N},(S_n)_{n\in\ob N}\b)$. 
\bigskip

\Remarque : on peut déduire la suite $u_n$ de sa suite $S_n$ des sommes partielles. Ainsi, on a 
$$
u_0=S_0\qquad \mbox{et}\qquad u_n=S_n-S_{n-1}\qquad(n\ge1). 
$$

\Definition [$(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$] 
La série de terme général $(u_n)$ converge vers $S\in\ob C\Leftrightarrow$ 
la suite des sommes partielles $(S_n)_{n\in\ob N}$ de $(u_n)$ converge vers $S$, i.e. 
$$
\forall\epsilon>0,\quad \exists N\in\ob N\quad \mbox{tel que} \quad \forall n\in\ob N, \quad 
\bigg|\underbrace{\sum_{k=0}^nu_k}_{S_n}-S\bigg|\le\epsilon. 
$$
Le nombre $S$ est alors appelé somme de la série et l'on écrit 
$$
\sum_{k=0}^\infty u_k=\sum_{k\ge0}u_k=\lim_{n\to\infty}\sum_{0\le k\le n}u_k=\lim_{n\to\infty}S_n=S. 
$$ 
Si la série de terme général $(u_n)$ ne converge vers aucun nombre $S\in\ob K$, i.e. si la suite~$(S_n)$ des sommes partielles de $(u_n)$ n'admet aucune limite finie, 
on dit que la série diverge. 
\medskip

\Exemple. Pour $a\in\ob C$, la série géométrique $\ds\sum_{n=0}^\infty a^n$ converge $\Leftrightarrow|a|<1$ et, dans ce cas, on a 
\Equation [\bf Sommes géométriques]
$$
\sum_{k=0}^\infty a^k={1\F1-a}\qquad\b(|a|<1\b). 
$$

\Propriete [ $(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$, $n_0\in\ob N$]
Les séries $\sum_{n=n_0}^\infty u_n:=\sum_{k=0}^\infty u_{k+n_0}$ et~$\sum_{n=0}^\infty u_n$ ont même nature. 
De plus, en cas de convergence, on a 
\Equation [\bf Relation de Chasles]
$$
\sum_{n=0}^\infty u_n=\sum_{n=0}^{n_0-1}u_n+\sum_{n=n_0}^\infty u_n.
$$
La suite de terme général $\ds r_n:=\sum_{k=n+1}^\infty u_k$ 
est appelée suite des restes de la série $\sum_{n=n_0}^\infty u_n$ et l'on a 
$$
\ds S_n=S-r_n=\sum_{k=0}^\infty u_k-r_n\qquad(n\ge0).
$$ 

\Definition [$(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$]
Si $\sum_{n=0}^\infty u_n$ converge, la suite de terme général $\ds r_n:=\sum_{k=n+1}^\infty u_k$ 
est appelée suite des restes de la série $\sum_{n=0}^\infty u_n$. Elle converge vers $0$ et satisfait 
\Equation [\bf suite des restes]
$$
\underbrace{\sum_{k=0}^nu_k}_{S_n}+r_n=\underbrace{\sum_{k=0}^\infty u_k}_{S}\qquad(n\ge0).
$$ 


\Subsection rah, Propriétés élémentaires.


\Propriete [$(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$] 
Si la série $\sum_{n=0}^\infty u_n$ converge, on a nécessairement $\ds\lim_{n\to\infty}u_n=0$. \medskip\noindent
Lorsque $(u_n)_{n\in\ob N}$ ne converge pas vers $0$, 
on dit que $\sum_{n=0}^\infty u_n$ diverge {\bf grossièrement}. 

\Exemple. La série $\sum_{n=0}^\infty a^n$ diverge grossièrement lorsque $|a|\ge1$. 
\bigskip

\Remarque : Attention, la réciproque est fausse : la série $\ds\sum_{n=1}^\infty{1\F n}$ diverge... 
\bigskip



\Propriete [$(u_n)_{n\in\ob N}$ suite d'éléments de $\ob K$, $\lambda\in\ob K^*$] 
Les séries $\sum_{n=0}^\infty \lambda u_n$ et $\sum_{n=0}^\infty u_n$ ont même nature. 
De plus, en cas de convergence, on a 
\Equation [\bf Multiplication]
$$
\sum_{n=0}^\infty\lambda u_n=\lambda\sum_{n=0}^\infty u_n. 
$$

\Propriete [$(u_n)$ et $(v_n)$ suites déléments de $K$]
Si $\sum_{n=0}^\infty u_n$ converge et si $\sum_{n=0}^\infty v_n$ diverge, 
alors la série $\sum_{n=0}^\infty(u_n+v_n)$ diverge. \medskip\noindent 
Si $\sum_{n=0}^\infty u_n$ et $\sum_{n=0}^\infty v_n$ convergent, 
alors la série $\sum_{n=0}^\infty(u_n+v_n)$ converge et l'on a 
\Equation[\bf Addition]
$$
\sum_{n=0}^\infty u_n+\sum_{n=0}^\infty v_n=\sum_{n=0}^\infty(u_n+v_n).
$$ 

\Remarque : $\sum_{n=0}^\infty\ch(n)$ et $-\sum_{n=0}^\infty\sh(n)$ divergent et pourtant $\sum_{n=0}^\infty\b(\ch (n)-\sh(n)\b)$ converge. 
\bigskip

\Propriete [$(u_n)$ et $(v_n)$ deux suites réelles]
La série $\sum_{n=0}^\infty(u_n+iv_n)$ converge $\ssi$ les séries 
$\sum_{n=0}^\infty u_n$ {\bf et} $\sum_{n=0}^\infty v_n$ convergent. 
En cas de convergence, on a 
\Equation [\bf Composantes]
$$
\sum_{n=0}^\infty(u_n+i v_n)=\sum_{n=0}^\infty u_n+i\sum_{n=0}^\infty v_n. 
$$

\Definition [$(u_n)_{n\in\ob N}$ suite complexe]
La série $\sum_{n=0}^\infty u_n$ converge {\bf absolument} $\ssi$ 
la série $\sum_{n=0}^\infty|u_n|$ converge. \medskip\noindent
La série $\sum_{n=0}^\infty u_n$ est {\bf semi-convergente} $\Longleftrightarrow$ 
$\sum_{n=0}^\infty u_n$ converge et $\sum_{n=0}^\infty|u_n|$ diverge. 


\Propriete [$(u_n)_{n\in\ob N}$ suite complexe] 
Si la série $\sum_{n=0}^\infty|u_n|$ converge, alors la série $\sum_{n=0}^\infty u_n$ converge et l'on a 
\Equation [\bf Convergence absolue]
$$
\Q|\sum_{n=0}^\infty u_n\W|\le \sum_{n=0}^\infty|u_n|. 
$$


\Remarque : Les ensembles $E_1$, $E_2$ et $E_3$ respectivement formés par les séries d'éléments de $\ob K$, 
les séries convergentes d'éléments de $\ob K$ et les séries absolument convergentes d'éléments de $\ob K$ 
forment trois $\ob K$-espaces vectoriels vérifiant 
$$
E_3\subset E_2\subset E_1.
$$ 

\Concept Sommes telescopiques

\Propriete [$(u_n)_{n\in\ob N}$ suite complexe]
La série $u_0+\sum_{n=1}^\infty(u_n-u_{n-1})$ converge $\ssi$ la suite 
$(u_n)_{n\in\ob N}$ converge. De plus, en~cas de convergence, on a 
\Equation [\bf Séries télescopiques]
$$
u_0+\sum_{n=1}^\infty(u_n-u_{n-1})=\lim_{n\to\infty}u_n. 
$$

\Exemple. La série $\ds\sum_{n=1}^\infty\ln\Q(1+{1\F n}\W)$ diverge et 
la série $\ds\sum_{n=1}^\infty{1\F n(n+1)}$ converge vers $1$. 
\bigskip

\Definition [$(u_n)_{n\in\ob N}$ suite réelle] 
La suite $(u_n)$ est alternée (resp. à partir du rang $n_0$) $\ssi$ le nombre $(-1)^nu_n$ est de signe constant pour $n\ge0$ 
(resp. pour $n\ge n_0$). 


\Exemple. la suite $u_n:=n(-1)^n+10$ est alternée à partir du rang $n_0=10$. 
\bigskip

\Concept Théorème spécial des séries alternées

\Theoreme [$(u_n)_{n\in\ob N}$ suite réelle alternée]
Si la suite $|u_n|$ est décroissante et de limite nulle, alors la série $\sum_{n=0}^\infty u_n$ converge et 
\Equation [\bf Théorème spécial]
$$
\forall k\ge0, \qquad \sum_{n=k}^\infty u_n \sbox { est entre } 0 \sbox{ et }u_k \sbox{ inclus}.
$$ 


\Remarque : on est capable d'encadrer la suite $r_k:=\sum_{n=k+1}^\infty u_n$ des restes, 
c'est très utile lorsque l'on veut approcher la série par une de ses sommes partielles. 
\bigskip

\Application : La série $\ds\sum_{n=1}^\infty{(-1)^n\F n}$ converge et sa somme $S$ est dans le segment $[-1,0]$. 
Plus précisément, 
$$\forall k\ge0, \qquad 
S=\sum_{n=1}^k{(-1)^n\F n}+r_k\quad \mbox{avec}\ r_k \ 
\mbox{entre $0$ et }\ {(-1)^{k+1}\F k+1}. 
$$

\Subsection rah, Séries de nombres positifs. 

\Concept Propriété fondamentale

\Propriete [$(u_n)_{n\in\ob N}$ suite de nombres positifs] 
La série $\sum_{n=0}^\infty u_n$ converge $\ssi$ il existe $M\in\ob R$ tel que 
$$
\forall k\in\ob N,\quad\sum_{n=0}^ku_n \le M. 
$$
En cas de divergence, on a $\lim\limits_{k\to\infty}\sum_{n=0}^ku_n=+\infty$ 
et l'on note alors $\ds\sum_{n=0}^\infty u_n=+\infty$. 

\Concept Sommation des inégalités

\Propriete [$(u_n)_{n\in\ob N}$ et $(v_n)_{n\in\ob N}$ 
suites réelles] 
Supposons que les suites $(u_n)$ et $(v_n)$ vérifient 
$$
0\le u_n\le v_n\qquad(n\ge0).
$$ 
Si la série $\sum_{n=0}^\infty u_n$ diverge, alors la série $\sum_{n=0}^\infty v_n$ diverge. \pn
Si la série $\sum_{n=0}^\infty v_n$ converge, alors la série $\sum_{n=0}^\infty u_n$ converge et l'on a 
\Equation [\bf Sommation des inégalités]
$$
0\le \sum_{n=0}^\infty u_n\le\sum_{n=0}^\infty v_n. 
$$ 

\Application : Comme $0\le \ln\Q(1+{1\F n}\W)\le {1\F n}$ pour $n\ge1$ 
et comme $\sum_{n=1}^\infty\ln\Q(1+{1\F n}\W)$ diverge, la série harmonique 
$\sum_{n=1}^\infty{1\F n}$ diverge. 
\bigskip


\Application : Comme $0\le \e^{-n^2}\le \e^{-n}$ pour $n\in\ob N$ et comme la série $\sum_{n=0}^\infty\e^{-n}$ converge, 
la série $\sum_{n=0}^\infty\e^{-n^2}$ converge. 
\bigskip

\Concept Sommation des o

\Propriete [$(u_n)_{n\in\ob N}$ et $(v_n)_{n\in\ob N}$ suites réelles] 
Supposons que les suites $(u_n)$ et $(v_n)$ soient positives et vérifient $u_n=o(v_n)$. \pn 
Si la série $\sum_{n=0}^\infty u_n$ diverge, alors la série $\sum_{n=0}^\infty v_n$ diverge et l'on a 
\Equation [\bf Sommation des o]
$$
\sum\limits_{n=0}^ku_n=o\Q(\sum\limits_{n=0}^kv_n\W)\qquad(k\to\infty). 
$$
Si la série $\sum_{n=0}^\infty v_n$ converge, alors la série $\sum_{n=0}^\infty u_n$ converge 
et l'on a 
\Equation [\bf Sommation des o]
$$
\sum\limits_{n=k}^\infty u_n=o\Q(\sum\limits_{n=k}^\infty v_n\W)\qquad(k\to\infty). 
$$ 

\Application : Comme ${\ln(n)\F n^3}=o\Q({1\F n(n+1)}\W)$ et comme $\sum_{n=1}^\infty{1\F n(n+1)}$ converge, 
$\sum\limits_{n=1}^\infty{\ln(n)\F n^3}$ converge et 
$$
\sum\limits_{k=n}^\infty{\ln(n)\F n^3}=o\Q(\sum\limits_{k=n}^\infty{1\F k(k+1)}\W)=o\Q({1\F n}\W)\qquad (n\to+\infty).
$$


\Application : Lorsque $\alpha<1$, on a ${1\F n}=o\Q({1\F n^\alpha}\W)$. Comme $\sum_{n=1}^\infty{1\F n}$ diverge, 
la série $\sum_{n=1}^\infty{1\F n^\alpha}$ diverge. 
\bigskip


\Exemple. La série $\sum_{n=1}^\infty{(-1)^{n-1}\F n+(-1)^{n-1}}$ converge car 
$$
{(-1)^{n-1}\F n+(-1)^{n-1}}={(-1)^{n-1}\F n}+o\Q(1\F n^{3/2}\W). 
$$

\Exemple. La série $\sum_{n=1}^\infty{(-1)^{n-1}\F\sqrt n+(-1)^{n-1}}$ diverge car 
$$
{(-1)^{n-1}\F\sqrt n+(-1)^{n-1}}={(-1)^{n-1}\F\sqrt n}-{1\F n}+o\Q({1\F n^{5/4}}\W). 
$$


\Concept Sommation des équivalents

\Propriete [$(u_n)_{n\in\ob N}$ et $(v_n)_{n\in\ob N}$ suites réelles positives] 
Si $u_n\sim v_n$, alors les séries $\sum_{n=0}^\infty u_n$ et $\sum_{n=0}^\infty v_n$ ont même nature. \pn
De plus, en cas de divergence, on a 
\Equation [\bf Sommation des équivalents]
$$
\sum\limits_{n=0}^ku_n\sim \sum\limits_{n=0}^kv_n\qquad(k\to\infty). 
$$
Et en cas de convergence, on a 
\Equation [\bf Sommation des équivalents]
$$
\sum\limits_{n=k}^\infty u_n\sim \sum\limits_{n=k}^\infty v_n\qquad(k\to\infty). 
$$

\Application : Comme ${1\F n^2}\sim{1\F n(n+1)}$ et comme $\sum_{n=1}^\infty{1\F n(n+1)}$ converge, la~série $\sum_{n=1}^\infty{1\F n^2}$ converge et  
$$
\sum\limits_{n=k}^\infty{1\F n^2}\sim\sum\limits_{n=k}^\infty{1\F n(n+1)}=\sum\limits_{n=k}^\infty\Q({1\F n}-{1\F n+1}\W)={1\F k}
\qquad(k\to\infty). 
$$

\Application : Comme $\ln(\ch n)\sim n$ et comme $\sum_{n=0}^\infty n$ diverge grossièrement, 
$\sum_{n=0}^\infty\ln(\ch n)$ diverge et 
$$
\sum\limits_{n=0}^k\ln(\ch n)\sim\sum\limits_{n=0}^kn={k(k+1)\F2}\sim{k^2\F 2}\qquad(k\to\infty). 
$$

\Concept Comparaison séries-intégrales

\Theoreme [{$f:\Q[0,\infty\W[\to\ob R$} continue par morceaux sur $\Q[0,\infty\W[$]
Si $f$ est positive et décroissante sur $\Q[0, \infty\W[$, alors l'intégrale $\int_0^\infty f(t)\d t$ 
et la série $\sum_{n=0}^\infty f(n)$ ont la même nature. 
\bigskip

\Remarque : On peut encadrer les sommes partielles de la fa\cced con suivante : 
$$
\int\limits_0^{N+1}f(t)\d t\le \sum\limits_{n=0}^N f(n)\le f(0)+\int\limits_0^Nf(t)\d t\qquad(N\in\ob N). 
$$
Inversement, on peut encadrer l'intégrale à l'aide des sommes partielles. 
\bigskip


\Propriete [$\alpha\in\ob C$]
\Equation [séries de Riemann]
$$
\ds\sum\limits_{n=1}^\infty{1\F n^\alpha} \sbox{ converge }\ssi |\alpha|>1.
$$


\Application : $\ds\sum\limits_{n=1}^N{1\F n}\sim\ln N\qquad(N\to\infty)$. 
\bigskip

\Concept Règle de Dalembert


\Propriete [$(u_n)_{n\in\ob N}$ suite de nombres complexes] 
Supposons que la limite $\ds\ell:=\lim_{n\to\infty}\Q|{u_{n+1}\F u_n}\W|$ existe. \medskip\noindent
Si $\ell<1$, la série $\sum_{n=1}^\infty u_n$ converge. \smallskip\noindent
Si $\ell>1$, la série $\sum_{n=1}^\infty u_n$ diverge grossièrement. \smallskip\noindent
Si $\ell=1$, on ne peut rien dire. 

\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Séries,\SériesNumériques}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}

\Section Rah, Séries Entières. 


\Subsection rah, Rayon de convergence.

\Definition [$(a_n)_{n\in\ob N}$ suite complexe] 
La série entière associée à la suite $(a_n)_{n\in\ob N}$ est la fonction 
$$
S:z\mapsto\sum_{n=0}^\infty a_nz^n.
$$ 


\Remarque : l'ensemble de définition $\{z\in\ob C:\sum_{n=0}^\infty a_nz^n\sbox{ converge}\}$ 
de la~série~entière est non vide car il contient $0$. 
\bigskip

\Exemple. $z\mapsto \ds\sum_{n=0}^\infty {z^n\F n^2+1}$ est une série entière. 

\Concept Lemme d'Abel

\Propriete [$(a_n)_{n\in\ob N}$ suite complexe]
S'il existe un nombre complexe $s\neq0$ tel que la suite $(a_ns^n)_{n\in\ob N}$ soit bornée, 
alors la série $\sum_{n=0}^\infty a_n z^n$ est absolument convergente pour chaque nombre complexe $z$ vérifiant $|z|<|s|$. 

\Concept Rayon et disque de convergence

\Definition[$(a_n)_{n\in\ob N}$ suite complexe]
Le rayon de convergence d'une série entière $S=\sum_{n=0}^\infty a_nz^n$ est 
le nombre $R\in\Q[0,\infty\W[\cup\{\infty\}$ défini par 
$$
R:=\sup\B\{r\ge0:\sum_{n=0}^\infty |a_n|r^n\mbox{ converge}\B\}. 
$$
L'ensemble $D:=\{z\in\ob C:|z|<R\}$ est appelé disque (ouvert) 
de convergence de $S$. 


\Propriete [$R$ rayon de convergence de $\sum_{n=0}^\infty a_nz^n$] 
Pour $|z|<R$, la série entière $\sum_{n=0}^\infty a_nz^n$ converge absolument. \pn
Pour $|z|>R$, la série entière $\sum_{n=0}^\infty a_nz^n$ diverge grossièrement. \pn
Pour $|z|=R$, on ne peut rien dire {\it a priori}. 

\Exemple. Pour $\ds\sum_{n=0}^\infty n^nz^n$, on a $\ds R=0$ et $D=\{0\}$. 

\Exemple. Pour $\ds\sum_{n=0}^\infty z^n={1\F1-z}$, on a $\ds R=1$ et $D=\{z\in\ob C:|z|<1\}$. 

\Exemple. Pour $\ds\sum_{n=0}^\infty {z^n\F n^n}$, on a $R=\infty$ et $D=\ob C$. 

\Propriete[$(a_n)_{n\in\ob N}$ suite complexe]
Le rayon de convergence $R$ de la série entière $\sum_{n=0}^\infty a_nz^n$ vérifie
$$
\eqalign{
R&=\sup\B\{|z|:\sum_{n=0}^\infty a_nz^n\mbox{ converge}\B\}
\cr
&=\sup\B\{r\ge0:\mbox{ la suite }(a_nr^n)_{n\in\ob N}\mbox{ est bornée}\B\}\cr
&=\sup\B\{r\ge0:\lim_{n\to\infty}a_nr^n=0\B\}. }
$$ 

\Remarque : cette propriété est extremement utile. C'est elle que l'on utilise pour calculer le rayon de convergence. 


\Subsection rah, Calcul du rayon de convergence. 


\Concept [Index=Rayon de convergence!Multiplication par un scalaire] Multiplication par un scalaire

\Propriete [$(a_n)_{n\in\ob N}$ suite complexe, $\lambda\in\ob C^*$]
Les séries entières $\sum_{n=0}^\infty\lambda a_nz^n$ et $\sum_{n=0}^\infty a_nz^n$ admettent le même rayon de convergence.  

\Concept [Index=Rayon de convergence!Addition] Addition

\Propriete [$R$ et $R'$ rayons de convergences de $\sum_{n=0}^\infty a_nz^n$ et $\sum_{n=0}^\infty b_nz^n$]
Le rayon de convergence $R''$ de la série entière $\sum_{n=0}^\infty(a_n+b_n)z^n$ vérifie 
$$
\eqalignno{&R''=\min\{R,R'\}\quad\sbox{ si }R\neq R', \cr
&R''\ge\min\{R,R'\}\quad\sbox{ si }R=R'} 
$$

\Application : Le rayon de convergence de la série entière $\ds\sum_{n=0}^\infty\ch(n)z^n$ est $\ds R={1\F\e}$. 

\Concept [Index=Rayon de convergence!Inegalites@Inégalités] Inégalités

\Propriete [$R$ et $R'$ rayons de convergences de $\sum_{n=0}^\infty a_nz^n$ et $\sum_{n=0}^\infty b_nz^n$]
Si $|a_n|\le |b_n|$ à partir d'un certain rang $n\ge N$, alors $R\ge R'$. 

\Application : Pour la série entière $\ds\sum_{n=0}^\infty\sin(n) z^n$, on a $R\ge 1$. 

\Concept [Index=Rayon de convergence!Equivalents] Equivalents

\Propriete [$R$ et $R'$ rayons de convergences de $\sum_{n=0}^\infty a_nz^n$ et $\sum_{n=0}^\infty b_nz^n$]
Si $|a_n|\sim|b_n|$ lorsque $n\to\infty$, alors $R=R'$. 

\Application : Pour la sèrie entière $\ds\sum_{n=0}^\infty\th{n}z^n$, on a $R=1$. 

\Concept [Index=Rayon de convergence!Integration terme a terme] Intégration terme à terme

\Propriete [$(a_n)_{n\in\ob N}$ suite complexe]
Les séries entièrent $\sum_{n=0}^\infty a_nz^n$ et 
$$
\sum\limits_{n=0}^\infty {a_n\F n+1}z^{n+1}=\sum_{n=1}^\infty{a_{n-1}\F n}z^n
$$
admettent le même rayon de convergence. 

\Exemple. Pour $\ds\sum_{n=0}^\infty z^n$, pour $\ds\sum_{n=0}^\infty{z^{n+1}\F n+1}$ et pour 
$\ds\sum_{n=0}^\infty{z^{(n+2)}\F (n+1)(n+2)}$, le rayon de convergence est $1$. 

\Concept [Index=Rayon de convergence!Derivation terme a terme@Dérivation terme à terme] Dérivation terme à terme

\Propriete [$(a_n)_{n\in\ob N}$ suite complexe]
Les séries entièrent $\sum_{n=0}^\infty a_nz^n$ et 
$$
\sum\limits_{n=1}^\infty na_nz^{n-1}=\sum\limits_{n=0}^\infty(n+1)a_{n+1}z^n
$$ 
admettent le même rayon de convergence. 

\Exemple. Pour $\ds \sum_{n=1}^\infty nz^{n-1}$ et pour $\ds \sum_{n=2}^\infty n(n-1)z^{n-2}$, 
le rayon de convergence est $1$. 


\Concept Règle de Dalembert pour les séries entières

\Propriete [$(u_n)_{n\in\ob N}$ suite de nombres complexes] 
Si la quantité $\Q|{u_{n+1}\F u_n}\W|$ converge vers $\ell\ge0$ ou $\ell=+\infty$ lorsque $n\to+\infty$, 
alors la série entière $\sum_{k=0}^\infty a_nz^n$ est de rayon de convergence $R={1\F \ell}$, avec la convention que $0={1\F+\infty}$ et que $+\infty=1\F 0$. 

\Remarque : l'intérêt de cette règle est qu'elle permet de calculer très simplement le rayon de convergence dans des cas simples, par exemple lorsque $u_n$ admet un équivalent simple (polynômes, etc...) ou possède une structure multiplicative (factorielle, puissances, etc..)

\Remarque : Les défauts de la règle de Dalemenbert sont les suivants : \pn
1) On ne peut pas l'appliquer aux séries ``lacunaires'' (i.e. pour lesquelles $u_n$ s'annule une infinité de fois) ou si la limite $\Q|{u_{n+1}\F u_n}\W|$ n'existe pas. \pn
2) Si on peut calculer $R$ via la règle de Dalembert, on peut trouver $R$ en comparant la série avec une série algébrique (c'est ce que fait la règle). \pn
3) il est souvent plus simple (moitié moins de calculs) de trouver le rayon de convergence via un équivalent de $u_n$. \pn
4) Certains étudiants ne savent utiliser que la règle de Dalembert (parce qu'ils ont la flemme d'apprendre à utiliser les autres outils), ils s'y accrochent comme à une bouée et sont perdus corps et âmes s'ils ne peuvent l'utiliser. 
A l'instar de l'utilisation systèmatique du discriminant pour résoudre les équations du second degré, 
la règle de Dalembert vous rend FAIBLES alors que les autres outils (équivalents, inégalités, etc..) vous rendent FORTS (à l'instar de l'usage de la forme canonique). 
\bigskip

\Remarque : si vous êtes en désaccord avec le point 4), prouvez que j'ai tort en resolvant les exercices suivants : 

\Exercice{aky}%

\Exercice{akz}%

\Exercice{ala}%

\Exercice{alb}%


\Subsection rah, Série entière d'une variable réelle. 

Dans cette section, $(a_n)_{n\in\ob N}$ désigne une suite de nombres réels ou complexes 
mais~$z$ ou $x$ désignent uniquement des nombres réels. 
\bigskip

\Definition [$\sum_{n=0}^\infty a_nx^n$ de rayon de convergence~$R>0$] 
L'intervalle $\Q]-R,R\W[$ est appelé intervalle ouvert de convergence de la série $S=\sum_{n=0}^\infty a_nx^n$. 

\Concept Théorème de continuité des séries entières

\Theoreme [$\sum_{n=0}^\infty a_n x^n$ de rayon de convergence $R>0$ ou~${R=\infty}$] 
L'application $S:x\mapsto\sum_{n=0}^\infty a_nx^n$ est continue sur l'intervalle $\Q]-R,R\W[$. 

\Application : La série entière $\ds S:x\mapsto\sum_{n=1}^{\infty}{(-1)^{n-1}\F n}x^n$ est continue sur $\Q]-1,1\W[$. 

\Concept Intégration terme à terme des séries entières

\Theoreme [$S=$ de rayon de convergence $R>0$ ou~$R=\infty$] 
Quels que soient $c$ et $d$ dans $\Q]-R,R\W[$, on a 
$$
\int_c^d\bigg(\underbrace{\sum\limits_{n=0}^\infty a_n x^n}_{S(x)}\bigg)\d x=\sum\limits_{n=0}^\infty\int_c^da_n x^n\d x=\sum\limits_{n=0}^\infty a_n{d^{n+1}-c^{n+1}\F n+1}
$$
En particulier, l'unique primitive de $S$ sur $\Q]-R,R\W[$ s'annulant en $0$ est l'application 
$$
x\mapsto\sum_{n=0}^\infty a_n{x^{n+1}\F n+1}=\sum_{n=1}^\infty a_{n-1}{x^n\F n}, 
$$
i.e. c'est une série entière de même rayon de convergence $R$. 

\Remarque : ce théorème permet d'intervertir intégration et sommation. 

\Application : Pour $x\in\Q]-1,1\W[$, on a $\ds \sum_{n=1}^\infty{(-1)^{n-1}\F n}x^n=\ln(1+x)$. 

\Concept Théorème d'Abel

\Theoreme [$S=\sum_{n=0}^\infty a_n x^n$ de rayon de convergence $R>0$] 
Si $S(x)$ converge pour $x=R$ (resp. $x=-R$) alors l'application $S$ est continue en $R$ (resp. en $-R$). 

\Application : $\ds\sum\limits_{n=1}^{\infty}{(-1)^{n-1}\F n}=\ln 2$ car $x\mapsto\ds \sum\limits_{n=1}^\infty{(-1)^{n-1}\F n}x^n$ est continue en $x=1$. 

\Remarque : La réciproque est fausse : $S(x)=\sum_{n=0}^\infty(-1)^nx^n$ est continue sur $\Q]-1,1\W[$ 
et prolongeable par continuité en $x=1$ mais ne converge pas en $x=1$. 

\Concept Dérivation terme à terme des séries entières

\Theoreme [$S=\sum_{n=0}^\infty a_n x^n$ de rayon de convergence $R>0$ ou $R=\infty$] 
La série entière $S$ est de classe $\sc C^1$ sur l'intervalle $\Q]-R,R\W[$ et vérifie 
$$
S'(x)=\sum\limits_{n=0}^\infty{\d\F\d x}\b(a_n x^n\b)=\sum\limits_{n=1}^\infty na_n x^{n-1}=\sum\limits_{n=0}^\infty(n+1)a_{n+1} x^n\qquad(-R<x<R). 
$$
En particulier, $S'$ est une série entière de même rayon de convergence $R$. 

\Remarque : ce théorème permet d'intervertir dérivation et sommation. 

\Application : Pour $x\in\Q]-1,1\W[$, on a $\ds{1\F(1-x)^2}=\sum_{n=1}^\infty nx^{n-1}=\sum_{n=0}^\infty(n+1)x^n$. 

\Theoreme [$S=\sum_{n=0}^\infty a_n x^n$ de rayon de convergence $R>0$ ou $R=\infty$] 
La série entière $S$ est de classe $\sc C^\infty$ sur l'intervalle $\Q]-R,R\W[$ et vérifie 
$$
{\d^k\F\d x^k}\underbrace{\sum\limits_{n=0}^\infty a_nx^n}_{S(x)}=\sum\limits_{n=0}^\infty{\d^k\F\d x^k}\b(a_nx^n\b)=\sum\limits_{n=k}^\infty {n!\F(n-k)!}a_n x^{n-k}
\qquad(k\in\ob N,-R<x<R). 
$$

\Application : La fonction $\ds x\mapsto {\ln(1+x)\F x}$ est de classe $\sc C^\infty$ sur $\Q]-1,\infty\W[$. 

En résumé, quand on dérive ou quand on intègre une série entière 
de rayon de convergence $R$, on peut procéder terme à terme sur l'intervalle ouvert de convergence $\Q]-R, R\W[$ 
et l'on obtient une série entière de même rayon de convergence $R$. 
\bigskip

\Subsection rah, Exponentielle complexe et autres fonctions développables en série entière. 


\Definition La fonction exponentielle est l'application $\exp:\ob C\to\ob C^*$ définie par 
$$
\forall z\in\ob C, \qquad \e^z=\exp(z):=\sum\limits_{n=0}^\infty{z^n\F n!}.
$$

\noindent
Rappel : Quelques propriétés (admises) de la fonction exponentielle complexe : 
$$
\eqalignno{
&\forall(s,z)\in\ob C^2, \quad\e^{s+z}=\e^s\times\e^z&(1)
\cr
&\forall z\in\ob C, \quad \exp z\neq0&(2)
\cr
&\forall z\in\ob C, \quad\ol{\exp z}=\exp{\ol z}&(3)
\cr
&\forall x\in\ob R, \quad \e^x\in\Q]0,\infty\W[& (4)
\cr
&\forall t\in\ob R,\quad |\e^{it}|=1&(5)
\cr
&\e^z=1\Longleftrightarrow z\in2\pi i\ob Z&(6)
\cr
&\exp \sbox{ est de classe }\sc C^\infty\sbox{ sur }\ob R&(7)
\cr
&\forall x\in\ob R,\quad {\d\F \d x}\b(\e^{zx}\b)=z\e^{zx}& (8)
\cr
&\forall x\in\ob R,\quad \ln(\e^x)=x&(9)
\cr
&\forall x>0,\quad x=\e{\ln (x)}&(10)
\cr
&\forall a>0, \forall z\in\ob C, \qquad a^z:=\e^{z\ln(a)}&(11)
}
$$

\Definition [$\alpha>0$, {$f:\Q]-\alpha,\alpha\W[\to\ob C$}]
$f$ est développable en série entière sur $\Q]-\alpha,\alpha\W[$ $\Leftrightarrow$ 
il existe une série entière $\sum_{n=0}^\infty a_nx^n$ de rayon de convergence $R\ge\alpha$ telle que 
$$
f(x)=\sum_{n=0}^\infty a_n x^n\qquad(-\alpha<x<\alpha). 
$$

\Propriete [$\alpha>0$]
Si $f$ est développable en série entière sur $\Q]-\alpha,\alpha\W[$, alors $f$ est de classe $\sc C^\infty$ 
sur $\Q]-\alpha, \alpha\W[$ et 
$$
a_n={f^{(n)}(0)\F n!}\qquad(n\in\ob N).
$$

\Remarque : en particulier, la suite $(a_n)_{n\in\ob N}$ des coefficients est uniquement déterminée par la fonction développable en série entière $f$. 

\Propriete 
Quelques sommes de séries entières à connaître impérativement : 
$$
\eqalign{
{1\F1-z}&=\sum\limits_{n=0}^\infty z^n\qquad(|z|<1), 
\cr
\e^z&=\sum\limits_{n=0}^\infty{z^n\F n!}\qquad(z\in\ob C), 
\cr
\cos(x)&=\sum\limits_{n=0}^\infty(-1)^n{x^{2n}\F(2n)!}\qquad(x\in\ob R),
\cr
\sin(x)&=\sum\limits_{n=0}^\infty(-1)^n{x^{2n+1}\F(2n+1)!}\qquad(x\in\ob R),
\cr
\ch(x)&=\sum\limits_{n=0}^\infty{x^{2n}\F(2n)!}\qquad(x\in\ob R),
\cr
\sh(x)&=\sum\limits_{n=0}^\infty{x^{2n+1}\F(2n+1)!}\qquad(x\in\ob R),
\cr 
\ln(1+x)&=\sum\limits_{n=0}^\infty(-1)^n{x^{n+1}\F n+1}=\sum\limits_{n=1}^\infty(-1)^{n-1}{x^n\F n}\qquad(x\in\ob R).
\cr
(1+x)^\alpha&=\sum\limits_{n=0}^\infty {x^k\F k!}\prod_{k=0}^{n-1}(\alpha-k)\qquad\big(x\in\Q]-1,1\W[\big).
}
$$

\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\SériesEntières}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}

\Section rah, Séries de Fourier.


Dans ce chapitre, $T$ désigne un nombre réel strictement positif et l'on pose $\ds \omega:={2\pi\F T}$. 
\medskip

\Subsection rah, Rappels.

\Definition[$c<d$, $f:{[c,d]}\to\ob C$]
$f$ est continue par morceaux sur $[c,d]\Longleftrightarrow$ il~existe $c=a_0<a_1<...<a_N=d$ tels que 
$$
\forall n\in\{1, \cdots, N\}\qquad \Q\{\eqalign{ 
&\mbox{$f$ est continue sur $\Q]a_{n-1},a_n\W[$}\cr
&\mbox{$f$ admet des limites en $a_{n-1}^+$ et en $a_n^-$. }}\W.
$$

\Definition[$c<d$, $f:{[c,d]}\to\ob C$]
$f$ est de classe $\sc C^1$ par morceaux sur $[c,d]\Leftrightarrow$ il~existe $c=a_0<a_1<...<a_N=d$ tels que 
$$
\forall n\in\{1, \cdots, N\}\qquad \Q\{\eqalign{ 
&\mbox{$f$ est de classe $\sc C^1$ sur $\Q]a_{n-1},a_n\W[$}\cr
&\mbox{$f$ admet des limites en $a_{n-1}^+$ et en $a_n^-$. },\cr
&\mbox{$f'$ admet des limites en $a_{n-1}^+$ et en $a_n^-$. }
}\W.
$$


\Definition [$T>0$]
La fonction $f:\ob R\to\ob C$ est $T$-périodique $\ssi$ 
$$
\forall x\in\ob R, \qquad f(x+T)=f(x). 
$$

\Remarque : si $f$ est $T$ périodique alors, $f$ est $kT$ périodique pour tout $k\in\ob N^*$. 
\bigskip

\Definition [$T>0$]
La fonction $f:\ob R\to\ob C$ est $T$-antipériodique $\ssi$ 
$$
\forall x\in\ob R, \qquad f(x+T)=-f(x). 
$$

\Remarque : si $f$ est $T$ anti-périodique alors, $f$ est $2T$ périodique. 
\bigskip

\Definition [$T>0$, $k\in\ob N\cup\{\infty\}$]
Une fonction $T$-périodique $f:\ob R\to\ob C$ est de classe $\sc C^k$ par morceaux sur $\ob R$ $\ssi$ elle est de classe $\sc C^k$ par morceau sur (au moins l'une de) ses périodes $[a, a+T]$. 

\Propriete [$T>0$]
L'ensemble des fonctions $f:\ob R\to\ob R$ continues par morceaux 
de période~$T$ 
est une $\ob R$-algèbre 
(un sous-ensemble non-vide de $\sc F(\ob R,\ob R)$ stable pour $+$, $.$ et $\times$). 

\Propriete [$T>0$]
Si $f$ est une fonction continue par morceaux et $T$ périodique sur $\ob R$, 
on a 
$$
\int_0^T f(x)\d x=\int_a^{a+T}f(x)\d x\qquad(a\in\ob R). 
$$

\Propriete [$a\in\ob R$]
Si $f:[-a,a]\to\ob C$ est continue par morceaux et impaire, alors 
$$
\int_{-a}^af(t)\d t=0.
$$

\Propriete [$a\in\ob R$]
Si $f:[-a,a]\to\ob C$ est continue par morceaux et paire, alors 
$$
\int_{-a}^a f(t)\d t=2\int_0^a g(t)\d t.
$$


\Subsection rah, Coefficients et série de Fourier. 


\Concept Coefficients $a_n$ et $b_n$ 

\Definition [$T>0$, $f:\ob R\to\ob R$ continue par morceaux et $T$-périodique] 
Les coefficient de fourier de l'application $f$ sont les nombres 
$$
\eqalign{
a_0(f)&:={1\F T}\int_0^Tf(x)\d x
\cr
a_n(f)&:={2\F T}\int_0^Tf(x)\cos(n\omega x)\d x\qquad(n\in\ob N^*)
\cr
b_n(f)&:={2\F T}\int_0^Tf(x)\sin(n\omega x)\d x\qquad(n\in\ob N^*)
}
$$

\Remarque : pour $a\in\ob R$, les formules suivantes sont également vraies : 
$$
\eqalign{
a_0(f)&:={1\F T}\int_a^{a+T}f(x)\d x
\cr
a_n(f)&:={2\F T}\int_a^{a+T}f(x)\cos(n\omega x)\d x\qquad(n\in\ob N^*)
\cr
b_n(f)&:={2\F T}\int_a^{a+T}f(x)\sin(n\omega x)\d x\qquad(n\in\ob N^*)
}
$$
Elles servent dans les exercices plus adaptés à un intervale du type $[a, a+T]$ qu'à $[0,T]$. 
\bigskip


\Exemple. Pour $T=1$, $\omega=2\pi$ et $f$ la fonction $1$-périodique définie par $f(x)=x$ pour $-{1\F2}<x\le{1\F2}$, 
on a 
\Equation exemplefourier
$$
\forall n\in\ob N, \quad a_n(f)=0\quad\mbox{et}\quad \forall n\in\ob N^*, \quad b_n(f)={(-1)^{n-1}\F n\pi}.
$$

\Definition [$f:\ob R\to\ob R$ continue par morceaux, de période $T$] 
La série de fourier de $f$ est la série de fonctions 
$$
S[f](x)=a_0(f)+\sum_{n=1}^\infty\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)
$$
et la $N^{\mbox{\sevenrm ième}}$ somme partielle de Fourier de $f$ est l'application 
$S_N[f]:\ob R\to\ob R$ définie par 
$$
S_N[f](x)=a_0(f)+\sum_{n=1}^N\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)\qquad(x\in\ob R).
$$


\Exemple. Pour la fonction de l'exemple \eqref{exemplefourier}, on a 
$$
S[f](x)=\sum_{n=1}^\infty(-1)^{n-1}{\sin (2\pi nx)\F\pi n}\quad\mbox{ et }\quad\forall N\ge1,\quad 
S_n[f](x)=\sum_{n=1}^N(-1)^{n-1}{\sin (2\pi nx)\F\pi n}. 
$$

\Concept Coefficients $c_n$

\Definition [$T>0$, $f:\ob R\to\ob R$ continue par morceaux et $T$-périodique] 
Pour $n\in\ob Z$, on pose 
$$
c_n(f):={1\F T}\int_0^Tf(x)\e^{-in\omega x}\d x
$$

\Propriete [$T>0$, $f:\ob R\to\ob R$ continue par morceaux et $T$-périodique] 
On a $c_0(f)=a_0(f)$ et
$$
\forall n\ge1, \qquad \Q\{\eqalign{c_n(f)={a_n(f)-ib_n(f)\F 2}\cr
c_{-n}(f)={a_n(f)+ib_n(f)\F 2}}\W.\qquad \mbox{et}\qquad \Q\{\eqalign{a_n(f)=c_n(f)+c_{-n}(f),\cr
b_n(f)={c_{-n}(f)-c_n(f)\F i}.}\W.
$$

\Propriete [$T>0$, $f:\ob R\to\ob R$ continue par morceaux et $T$-périodique] 
Pour $n\in\ob N$ et $x\in\ob R$, on a 
$$
S_n(f)(x)=\sum_{-N\le n\le N}c_n(f)\e^{i\omega x}. 
$$

\Subsection rah, La fonction normalisée $\tilde{f}$. 

\Definition [$f:\ob R\to\ob C$]
Si $f$ admet en $x$ une limite à gauche $f(x^-)$ et une limite à droite $f^{x^+}$, on pose 
$$
\tilde f(x):={f(x^-)+f(x^+)\F 2}. 
$$ 

\Propriete 
Si $f$ est continue en un point $x\in\ob R$ alors $f(x^-)=f(x)=f(x^+)$ et 
$$
\tilde f(x)=f(x). 
$$

\Subsection rah, Théorème fondamentaux. 

\Concept Théorème de Parseval

\Theoreme [$f:\ob R\to\ob R$ fonction $T$ périodique et continue par morceaux] 
Les séries $\sum_{n=1}^\infty\b|a_n(f)\b|^2$ et $\sum_{n=1}^\infty\b|b_n(f)\b|^2$ convergent et l'on a 
$$
{1\F T}\int_0^T\b|f(x)\b|^2\d x=\b|a_0(f)\b|^2+{1\F 2}\sum_{n=1}^\infty\b|a_n(f)\b|^2
+{1\F 2}\sum_{n=1}^\infty\b|b_n(f)\b|^2. 
$$

\Application : D'après l'exemple \eqref{exemplefourier}, on a 
$$
{\pi^2\F6}=\sum_{n=1}^\infty{1\F n^2}.
$$


\Concept Théorème de Dirichlet

\Theoreme [$T>0$, $f:\ob R\to\ob R$ fonction $T$-périodique de classe $\sc C^1$ par morceaux]
Pour chaque nombre réel $x$, la série numérique $S[f](x)$ converge et l'on a 
\Equation [\bf Dirichlet]
$$
\underbrace{f(x^-)+f(x^+)\F2}_{\tilde f(x)}=\underbrace{a_0+\sum_{n=1}^\infty\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)}
_{S[f](x)}
$$

\Remarque : Sous les hypothèses $f:\ob R\to\ob R$ $T$-périodique et continue par morceaux, 
les sommes partielles $x\mapsto S_N[f](x)$ sont définies sur $\ob R$ pour chaque entier $N\in\ob N$ 
mais la série $S[f](x)$ ne converge pas nécéssairement 
en un point quelconque $x\in\ob R$.
\bigskip

\Application : Pour la série de Fourier de l'exemple \eqref{exemplefourier}, on a 
$$
x=\sum_{n=1}^\infty(-1)^{n-1}{\sin (2\pi nx)\F\pi n}\quad\Q(-{1\F2}<x<{1\F2}\W).
$$

\Concept Théorème de convergence normale des séries de Fourier

\Theoreme [$f:\ob R\to\ob R$ fonction $T$-périodique, {\bf continue} et de classe $\sc C^1$ par morceaux] 
Pour chaque nombre réel $x$, la série numérique $S[f](x)$ converge et 
$$
f(x)=\underbrace{a_0+\sum_{n=1}^\infty\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)}
_{S[f](x)}.
$$
De plus, on peut intégrer terme à terme $S$ sur chaque segment. Ainsi
$$
\eqalign{
\int_c^df(x)\d x&=\int_c^d\Q(a_0+\sum_{n=1}^\infty\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)\W)\d x
\cr
&=a_0\int_c^d\d x+\sum_{n=1}^\infty\int_c^d\B(a_n(f)\cos(n\omega x)+b_n(f)\sin(n\omega x)\B)\d x\qquad(c,d\in\ob R)
}
$$
Enfin, les séries numériques $\ds\sum_{n=0}^\infty\b|a_n(f)\b|$ 
et $\ds\sum_{n=1}^\infty\b|b_n(f)\b|$ convergent. 
\bigskip

\Remarque : Sous ces hypothèses, les séries sont absolument 
convergentes et l'on a 
$$
f(x)=a_0+\sum_{n=1}^\infty a_n(f)\cos(n\omega x)+\sum_{n=1}^\infty b_n(f)\sin(n\omega x)\qquad(x\in\ob R).
$$

\Exemple. pour la fonction $1$-périodique $f$ définie par $f(x)=|x|$ pour $-1/2<x\le 1/2$, on a 
$$
|x|={1\F4}+\sum_{n=1}^\infty{(-1)^n-1\F\pi^2n^2}\cos(2n\pi x)\qquad\Q(-{1\F2}<x\le {1\F 2}\W). 
$$

\Subsection rah, Interprétation géométrique. 

\Definition [$E$ $\ob R$-espace vectoriel] 
Une application $\Phi:E\times E\to\ob R$ est un produit scalaire sur $E\ssi \Phi$ est une forme bilinéaire, symétrique, définie, positive 
$$
\eqalignno{\qquad\qquad\qquad
\forall(\lambda,\mu)\in\ob R^2,
\forall(x,y,z)\in &E^3\Q\{\eqalign{
\Phi(\lambda x+\mu y,z)=\lambda \Phi(x,z)+\mu\Phi(y,z)\cr
\quad\Phi(z,\lambda x+\mu y)=\lambda \Phi(z,x)
+\mu\Phi(z,y)\cr
}\W.&\mbox{(bilinéaire)}
\cr
\forall (x,y)\in &E^2, \qquad\Phi(x,y)=\Phi(y,x)\qquad&\mbox{(symétrique)}\cr
\forall x\in &E, \qquad\Phi(x,x)=0\Leftrightarrow x=0\qquad&\mbox{(définie)}\cr
\forall x\in &E, \qquad\Phi(x,x)\ge0\qquad\qquad&\mbox{(positive)}
}
$$

\Notation : En mathématiques, le produit scalaire de $x$ par $y$ 
est noté $\langle x,y\rangle$ ou $\langle x|y\rangle$ ou $(x|y)$ plutôt que $\vec x.\vec y$. 
\bigskip

\Propriete [$E$ $\ob R$-espace vectoriel muni d'un produit scalaire $\langle\cdot,\cdot\rangle$] 
L'application $x\mapsto \|x\|:=\sqrt{\langle x,x\rangle}$ est une norme sur $E$.  


\Propriete L'application $\ds (f,g)\mapsto\langle f,y\rangle:={1\F T}\int_0^Tf(t)g(t)\d t$ 
est un produit scalaire sur l'espace vectoriel $\sc C_T(\ob R)$ des fonctions $T$-périodiques et continues $f:\ob R\to\ob R$. 

\bigskip

\Remarque : Pour l'espace euclidien précédent, 
$\{g_0:x\mapsto 1\}\cup\{g_n:x\mapsto\sqrt2\cos (n\omega x)\}_{n\ge1}\cup \{h_n:x\mapsto\sqrt2\sin(n\omega x)\}_{n\ge1}$ 
est une famille (orthonormale) de fonctions orthogonales deux à deux et de norme $1$. 
\bigskip


\Remarque : Si l'on note $\sc E$ l'espace vectoriel des fonctions $T$-périodiques 
et continues par morceaux $f:\ob R\to\ob R$ vérifiant 
$$
f(x)={f(x^+)+f(x^-)\F2}\qquad(x\in\ob R)
$$
Alors, l'application $\ds (f,g)\mapsto\langle f,g\rangle:={1\F T}\int_0^Tf(t)g(t)\d t$ est un produit scalaire sur $\sc E$. 
\bigskip


\Concept Interprétation géométrique des coefficients de Fourier

\Remarque : les coefficients $a_n(f)$ et $b_n(f)$ sont les produits scalaires de $f$ avec les fonctions de la famille formée des $g_n$ et $h_n$. 
$$
\eqalignno{
a_0&=\langle f,1\rangle=\langle f,g_0\rangle
\cr
a_n&=2\langle f,\cos n\omega x\rangle=\sqrt2\langle f,g_n\rangle\qquad(n\in\ob N^*)
\cr
b_n&=2\langle f,\sin n\omega x\rangle=\sqrt2\langle f,h_n\rangle \qquad(n\in\ob N^*) 
}
$$

\Concept Interprétation géométrique des sommes partielles de Fourier


\Assertion [$f:\ob R\to\ob R$ continue et $T$-périodique, $n\in\ob N$] 
$$
S_n(f)=\langle f,g_0\rangle g_0+\sum_{k=1}^n\langle f,g_n\rangle g_n
+\sum_{k=1}^n\langle f,h_n\rangle h_n
$$ 
Autrement dit, $S_n(f)$ est la projection orthogonale de la fonction $f$ 
sur le sous-espace vectoriel $E_n$ 
de $\sc C_T(\ob R,\ob R)$ engendré par les fonctions $g_0,\cdots, g_n, h_1,\cdots, h_n$ : 
$$
E_n=\Q\{f:x\mapsto a_0+\sum_{k=1}^na_n\cos(n\omega x)+\sum_{k=1}^nb_n\sin(n\omega x):
a_0,\cdots,a_n,b_1,\cdots,b_n\in\ob R\W\}
$$ 



\Remarque : Etant donnée une fonction $f\in\sc C_T(\ob R)$, 
la fonction $S_n(f)$ est la fonction de l'espace $E_n$ la plus proche de $f$ pour la norme $||.||$. 
$$
\inf_{g\in E_n} \|g-f\|=\|S_n(f)-f\|=\mbox{distance de $f$ à $E_n$}. 
$$

\Exemple. Calculer la distance de $E_1$ à la fonction $1$-périodique $f$ 
définie par $f(x)=x$ pour $-{1\F2}<x\le{1\F2}$. 
\bigskip

\Rappel : Les vecteurs $\vec i=(1,0,0)$, $\vec j=(0,1,0)$ et $\vec k=(0,0,1)$ 
forment une base orthonormée de l'espace euclidien $\ob R^3$ et l'on a 
$$
\vec v=\underbrace{(\vec v.\vec i)}_{x}\vec i+\underbrace{(\vec v.\vec j)}_{y}\vec j
+\underbrace{(\vec v.\vec k)}_{z}\vec k\qquad(\vec v\in\ob R^3)
$$
En dimension $3$, le théorème de Pythagore s'écrit alors
$$
\|\vec v\|^2=\underbrace{(\vec v.\vec i)^2+(\vec v.\vec j)^2+(\vec v.\vec k)^2}_{x^2+y^2+z^2}
$$ 


\Concept Interprétation géométrique du théorème de Dirichlet+continuité
 
Soit $f\in\sc C_T(\ob R)$ de classe $\sc C^1$ par morceaux. Alors, on a 
$$
f=\langle f,g_0\rangle g_0+\sum_{k=1}^\infty\langle f,g_n\rangle g_n
+\sum_{k=1}^\infty\langle f,h_n\rangle h_n.
$$ 


\Concept Interprétation géométrique du théorème de Parseval. 

Pour $f\in\sc C_T(\ob R)$, on~a 
$$
\|f\|^2=\langle f,g_0\rangle^2+\sum_{n=1}^\infty\langle f,g_n\rangle^2
+\sum_{n=1}^\infty\langle f,h_n\rangle^2\qquad \mbox{(Théorème de Pythagore)}
$$




\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\SériesDeFourier}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{2}\LD@List{%
	\Exercices%,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}



\hautspages{Olus Livius Bindus}{Surfaces}%

\Chapter Surf, Surfaces.



On se place dans un espace euclidien $\sc E$ de dimension $3$ 
muni d'un repère orthonormé $(O,\vec i,\vec j,\vec k)$, que l'on identifie à $\ob R^3$. 
\bigskip

\Section null, Surfaces.

\Subsection null, Définitions.

\Definition [] Soit $k\in\ob N\cup\{\infty\}$. Une nappe paramétrée de classe $\sc C^k$ de $\ob R^3$ 
est le couple $(U,f)$ formé d'un ouvert $U$ non vide de $\ob R^2$ 
et de $f:U\to\ob R^3$ de classe $\sc C^k$. \smallskip\noindent
L'image $\{f(u,v):(u,v)\in U\}$ de $U$ par $f$ est appelé le support de la nappe $(U,f)$. 

\centerline{%
	\Image[Height=5cm,Box=hbox]{\imageFolder/Pdf/Surface2.pdf}%
	\Image[Height=5cm,Box=hbox]{\imageFolder/Pdf/Surface.pdf}%
}%
\Figure [Index=Surfaces] Surface $f:(u,v)\mapsto(u+v,\cos u+\cos v,\sin u+\sin v)$.

\Definition [] Soit $k\in\ob N\cup\{\infty\}$. Une partie $S$ de $\ob R^3$ 
est une nappe géométrique~de classe $\sc C^k$ 
s'il existe une nappe $(U,f)$ de classe $\sc C^k$ 
telle que $S$ soit le support de $(U,f)$. 
\bigskip

\Exemple. La sphère $S$ de rayon $1$ est une nappe géométrique de classe $\sc C^\infty$. 
\bigskip

\Definition [] Soient $k\in\ob N\cup\{\infty\}$ et $(O,\vec i,\vec j,\vec k)$ un repère de $\sc E$. 
Une partie $S$ de l'espace $\sc E$ est appelé surface de classe $\sc C^k$ 
si, et seuelement si, il existe un ouvert $U$ de $\ob R^3$ 
et une fonction $F:U\to\ob R$ de classe $\sc C^k$ telle que 
$$
M\in S\Longleftrightarrow F(x,y,z)=0, 
$$
où $(x,y,z)$ désignent les coordonnées 
du point $M$ dans le repère $(O,\vec i,\vec j,\vec k)$. 
\bigskip

\Remarque : On dit alors que la surface $S$ est d'équation cartésienne 
$$
\Q\{\eqalign{(x,y,z)\in U\cr F(x,y,z)=0\cr}\W. 
$$
dans le repère $(O,\vec i,\vec j,\vec k)$. 
{\it En pratique on a\/ $U=\ob R^3$ et on écrit $F(x,y,z)=0$. }
\bigskip

\Subsection null, Points singuliers/réguliers.
\bigskip

\Definition [] Soient $(U,f)$ une nappe paramétrée de classe $\sc C^1$ et $(u,v)\in U$. \pn
Alors le point $M=f(u,v)$ est régulier $\Longleftrightarrow$ 
$$
\Q\{{\partial f\F\partial u}(u,v),{\partial f\F\partial v}(u,v)\W\}\mbox{ est une famille libre} 
\Longleftrightarrow{\partial f\F\partial u}(u,v)\wedge{\partial f\F\partial v}(u,v)\neq(0,0,0).
$$
Le point $M=f(u,v)$ est dit régulier dans le cas contraire. 
\bigskip

\Definition [] Soient $(O,\vec i,\vec j,\vec k)$ un repère de $\sc E$, 
$S$ une surface de classe $\sc C^1$ d'équation cartésienne 
$$
\Q\{\eqalign{(x,y,z)\in U\cr F(x,y,z)=0\cr}\W. 
$$
et $M$ un point de la surface $S$ de coordoonnées $(x,y,z)$~dans 
le repère $(O,\vec i,\vec j,\vec k)$. \pn 
Alors, on dit que $M$ est un point régulier de $\sc S$ si, et seulement si, 
$$
\underbrace{\Q({\partial F\F\partial x}(x,y,z),{\partial F\F\partial y}(x,y,z), 
{\partial F\F\partial z}(x,y,z)\W)}_{\vec{\grad}\ F(x,y,z)}\neq(0,0,0).
$$ 
On dit que $M$ est un point singulier de $S$ dans le cas contraire. 
\bigskip

\Remarque : On dit que la nappe paramétrée (resp. la surface) est régulière si, 
et seulement si, elle est régulière en tous ses points. 

\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Surface3.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Surface4.pdf}%
}%
\Figure [Index=Surfaces!Cone@Cône]  Le cône $z^2=x^2+3y^2$.

\Remarque : On s'interessera en général à des surfaces régulières. 
\goodbreak

\Theoreme [] Soit $S$ une surface $\sc C^k$, avec $k\ge1$, 
d'équation cartésienne 
$$
(x,y,z)\in U\quad\mbox{et}\quad F(x,y,z)=0
$$ 
et $M_0$ un point régulier de $S$ de coordonnées $(x_0,y_0,z_0)$, 
dans un repère $(O,\vec i,\vec j,\vec k)$. \pn
Alors il existe un ouvert $V\subset U$ contenant $(x_0,y_0,z_0)$ tel que 
la portion de $S$ d'équation cartésienne 
$$
(x,y,z)\in V\quad\mbox{et}\quad F(x,y,z)=0
$$
soit le support d'une nappe régulière de classe $\sc C^k$. 
\bigskip

\Theoreme [] Soit $f:U\to\ob R^3$ une nappe de classe $\sc C^k$, avec $k\ge1$, soit $(u_0,v_0)\in U$ et soit 
$\sc R=(O,\vec i,\vec j,\vec k)$ un repère tels que 
$$
\mbox{la famille }\Q\{{\partial f\F\partial u}(u_0,v_0),{\partial f\F\partial v}(u_0,v_0),\vec k\W\}
\mbox{ soit libre.}
$$
Alors, il existe un ouvert $V\subset U$ de $\ob R^2$ contenant $(u_0,v_0)$ 
et une fonction $g:V\to\ob R$ de classe $\sc C^k$ telle que {\it (en notant $(x,y,z)$ les coordonnées de $M$ dans $\sc R$)}
$$
\mbox{$M$ appartient au support de la nappe }f:V\to\ob R^3
\Longleftrightarrow\Q\{\eqalign{(x,y)\in V\cr 
z=g(x,y)\cr}\W.
$$

\Remarque : On dit que la surface est à représentation cartésienne ou de paramétrage cartésien 
lorsqu'elle admet une équation du type 
$$
(x,y)\in U\quad\mbox{et}\quad z=g(x,y).
$$

\centerline{%
	\Image[Height=4cm]{\imageFolder/Pdf/Araignee.pdf}%
}%
\Figure [Index=Surfaces!Representation Cartesienne@Représentation Cartésienne] Graphe de $z=x\cos(xy)$ pour $(x,y)\in[-\pi,\pi]^2$.

\Subsection null, Aire.
\bigskip

\noindent{\bf Rappel\it (Aire d'une surface plane).} L'aire d'une surface $S$ de $\ob R^2$ 
est le nombre
$$
\sc A(S):=\int_S\d x\d y. 
$$

\noindent{\bf Rappel\it (Aire d'une nappe).} Soit $U$ un ouvert de $\ob R^2$ et 
$\vec f:U\to\ob R^3$ 
une nappe $\sc C^1$. Alors, l'aire du support $S:=\{f(u,v):(u,v)\in U\}$ 
de la nappe $(U,f)$ est 
$$
\sc A(S):=\int_U\Q|\!\Q|{\partial \vec f\F\partial u}(u,v)\wedge{\partial\vec f\F\partial v}(u,v)\W|\!\W|\d u\d v. 
$$
\smallskip

\Section null, Etude locale des surfaces.

\Subsection null, Plan tangent.
\bigskip

\Definition [] Soient $S$ une surface de $\sc E$ et $M$ un point de $S$. \pn
On dit qu'un plan $\sc P$ est tangent à $S$ en $M$ si, et seulement si, 
la tangente en $M$ de tout arc régulier $f:I\to S$, tracé sur $S$ 
et passant par $M$, est incluse dans $\sc P$. 
\bigskip

\Theoreme [] Une nappe paramétrée $f:U\to\ob R^3$ de classe $\sc C^1$ admet 
en un point régulièr $M_0=f(u_0,v_0)$ un unique plan tangent $\sc P$, 
déterminé par 
$$
\sc P=\Q(M_0,{\partial f\F\partial u}(u_0,v_0),{\partial f\F\partial v}(u_0,v_0)\W).
$$
La normale au plan $\sc P$ en $M_0$ est la droite $\Delta$ déterminée par 
$$
\Delta=\Q(M_0,{\partial f\F\partial u}(u_0,v_0)\wedge{\partial f\F\partial v}(u_0,v_0)\W).
$$


\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Plant1.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Plant2.pdf}%
}%
\Figure [Index=Surfaces!Plant tangent]  Vecteur normal et plan tangent à $z=\sqrt{1-x^2-y^2}$ en $(0,0,1)$.
\bigskip

\Theoreme [] Une surface $S$ de classe $\sc C^1$, d'équation cartésienne 
$$
(x,y,z)\in U\qquad\mbox{et}\qquad F(x,y,z)=0
$$
dans un repère $\sc R=(O,\vec i,\vec j,\vec k)$, 
admet en un point régulier $M_0=(x_0,y_0,z_0)\in S$ un unique plan tangent $\sc P$, 
d'équation cartésienne 
$$
{\partial F\F\partial x}(x_0,y_0,z_0)(x-x_0)+{\partial F\F\partial y}(x_0,y_0,z_0)(y-y_0)
+{\partial F\F\partial z}(x_0,y_0,z_0)(z-z_0)=0
$$
Si le repère $\sc R$ est orthonormé, la normale en $M_0$ à la surface $S$ 
est dirigée par
$$
\vec{\grad}\ F(x_0,y_0,z_0)={\partial F\F\partial x}(x_0,y_0,z_0)\vec i+{\partial F\F\partial x}(x_0,y_0,z_0)\vec j
+{\partial F\F\partial x}(x_0,y_0,z_0)\vec k
$$ 
\medskip

\Subsection null, Intersection de deux surfaces.

\Theoreme [] Soient $\sc R=(O,\vec i,\vec j,\vec k)$ un repère, $k\in\ob N^*\cup\{\infty\}$ et $S_1, S_2$ 
deux surfaces de classe $\sc C^k$ d'équations cartésiennes respectives 
$$
(S_1)\quad\Q\{\eqalign{(x,y,z)\in U_1\cr F_1(x,y,z)=0\cr}\W.\qquad\mbox{et}\qquad (S_2)\quad
\Q\{\eqalign{(x,y,z)\in U_2\cr F_2(x,y,z)=0\cr}\W.
$$
Alors l'intersection de $S_1$ et $S_2$ est d'équation 
$$
(S_1\cap S_2)\qquad\Q\{\eqalign{(x,y,z)\in U_1\cap U_2\cr F_1(x,y,z)=0\cr F_2(x,y,z)=0}\W.
$$
De plus, pour chaque point $M_0=(x_0,y_0,z_0)$ de $S_1\cap S_2$ 
tel que 
$$
\vec V=\vec{\grad }F_1(x_0,y_0,z_0)\wedge\vec{\grad }F_2(x_0,y_0,z_0)\neq 0, \eqdef{cond}
$$
il existe un ouvert $W$ de $\ob R^3$ contenant $M_0$ tel que $W\cap S_1\cap S_2$ 
soit le support d'un arc régulier de classe $\sc C^k$ et la tangente 
à cet arc en $M_0$ est la droite $\Delta=(M_0,\vec V)$. 
$$
	\vcenter{\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Inter2.pdf}}
	\bigcap\quad
	\vcenter{\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Inter1.pdf}}
	=
	\vcenter{\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Inter5.pdf}}
$$
\Figure [Index=Surfaces!Intersection]  Intersection de deux surfaces.
%	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Inter3.pdf}% reunion

\Remarques : a) La condition \eqref{cond} implique est équivalente à 
$$ 
\mbox{la famille }\Q\{\vec{\grad }F_1(x_0,y_0,z_0), \vec{\grad }F_2(x_0,y_0,z_0)\W\} \mbox{ est libre.}
$$ 
b) Localement à $M_0$, l'intersection $S_1\cap S_2$ est un arc géométrique $\Gamma$. \pn
c) La tangente à $\Gamma$ en $M_0$ est l'intersection 
des plans tangents $\sc P_1$, $\sc P_2$ en $M_0$ à $S_1$,~$S_2$. \pn
d) On a $\sc P_1\neq \sc P_2$. Lorsque cette condition n'est pas satisfaite, 
cela se complique...
\bigskip

\Subsection null, Surfaces réglées.

\Definition [] Une surface est dite réglée si, et seulement si, elle est la réunion 
d'une famille de droites. \pn
Une nappe paramétrée est dite réglée si, et seulement si, 
son support est une réunion d'une famille de droite. 
\bigskip

\Definition [] Si $S$ est une surface réglée, alors, les droites constituant $S$ 
sont appelées {\bf génératrices} de $S$. \pn
On appelle {\bf directrice} de $S$ toute courbe 
non réduite à un point qui rencontre toutes les génératrices de $S$. 
\bigskip

\Definition [] On dit qu'une surface réglée $S$ est développable si, et seulement si, 
le~plan tangent en un point régulier $M$ de $S$ ne varie pas quand $M$ 
décrit~une~génératrice. 
\bigskip

\Remarque : Soit $(I,\Gamma)$ une directrice d'une surface réglée $S$. 
Pour chaque génératrice~$\Delta$, il existe $t\in I$ telle que $\Delta$ 
passe par $\Gamma(t)$ et il existe un vecteur directeur $\vec u(t)$ de $\Delta$. 
On peut alors paramétrer $S$ par 
$$
\vec{OM}(t,\lambda)=\Gamma(t)+\lambda\vec u(t)\qquad\b(t\in I,\lambda\in\ob R\b).
$$ 
\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Regle1.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Regle2.pdf}%
}%
\Figure [Index=Surfaces!Intersection]  Surface réglée $(t,\lambda)\mapsto( \lambda\cos t,t+\lambda\sin t,
t+\lambda)$.

\Propriete [] Une génératrice d'une surface réglée $S$ qui passe 
par un point régulier~$M$ de $S$ est incluse 
dans le plan tangent à $S$ en $M$. 
\bigskip

\Theoreme [] La surface engendrée par les tangentes à une courbe gauche (arc paramétré) 
est développable. 
\bigskip

\Methode [Pour savoir si une surface $S$ est réglée] 
On regarde les droites $\Delta\subset S$ \pn(par chaque point de $S$, 
il passe au moins une droite 
$\Delta\subset S\Leftrightarrow$ la surface est réglée). \medskip\noindent 
En un point régulier, on peut déterminer le plan tangent $\sc P$ à $S$ 
et étudier $S\cap \sc P$... \medskip\pn
On adaptera selon que $S$ soit définie par un paramétrage ou une équation cartésienne. 
\bigskip

\Section null, Cylindres, cônes, surfaces de révolution.

\Subsection null, Cylindres.

\Definition [$\Delta$ droite] 
Une surface $S$ est un cylindre de direction~$\Delta$ si, et seulemen si, $S$ 
est réunion d'une famille de droites parallèles à $\Delta$.  
\bigskip

\Remarque : Un cylindre de direction $\Delta$ est une surface réglée. \pn 
Les droites qui la constituent sont appelées génératrices de $S$. \pn
Une courbe tracée sur un cylindre $S$ qui rencontre toutes les génératrices est appelée 
une directrice de $S$. 
\bigskip

\Definition [] L'intersection d'un cylindre $S$ avec un plan $\sc P$, qui n'est pas parallèle 
à sa direction $\Delta$, est appelé base de $S$ 
(resp. base droite de $S$ si $\sc P\perp\Delta$). 

\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Cylindre.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Cylindre2.pdf}%
}%
\Figure [Index=Surfaces!Cylindre]  Cylindre, base et direction.

\Remarque : On peut paramétrer un cylindre de directrice $(I,\Gamma)$ et de direction $\Delta=\mbox{Vect}(\vec v)$ par 
$$
\vec{OM}(t,\lambda)=\Gamma(t)+\lambda\vec v\qquad(t\in I,\lambda\in\ob R).
$$

\Propriete [] Soit $(O,\vec i,\vec j,\vec k)$ un repère. 
Alors, $S$ est un cylindre de direction $(O,\vec k)$ $\Longleftrightarrow$ 
$S$ admet une équation cartésienne dans le repère $(O,\vec i,\vec j,\vec k)$ du type 
$$
F(x,y)=0
$$

\Propriete [] Les cylindres sont des surfaces développables. 
\bigskip

\Remarque : Si $M$ est un point régulier du cylindre de directrion $\vec v$, alors 
tous les points de la génératrice $(M,\vec v)$ sont réguliers. 
\bigskip

\Methode [] Pour caractériser simplement un cylindre, on peut : \pn
1) chercher sa direction $\Delta$ (en cherchant les droites qu'il contient). \pn
2) procéder à un changement de repère $(O,\vec i',\vec j',\vec k')$ (orthonormé) où l'axe des $z$ 
est la direction $\Delta$ du cylindre. \pn
3) chercher une équation cartésienne/un paramétrage dans le nouveau repère
\bigskip\goodbreak

\Subsection null, Cônes.

\Definition [] Soit $A$ un point de $\sc E$. Une surface $S$ est 
un cône de sommet $A\Longleftrightarrow S$ est la réunion 
d'une famille de droite passant par $A$. 
\bigskip

\Remarque : Un cône de sommet $A$ est une surface réglée. \pn 
Les droites qui la constituent sont appelées génératrices de $S$. \pn
Une courbe tracée sur un cône $S$ de sommet $A$ 
qui rencontre toutes les génératrices en des points différents de $A$ 
est appelée une directrice de $S$. 
\bigskip

\Definition [] L'intersection d'un cône $S$ de sommet $A$ avec un plan $\sc P$, 
qui ne contient pas $A$ s'appelle une base de ce cône. 
\bigskip

\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Cone1.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Cone3.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Cone2.pdf}%
}%
\Figure [Index=Surfaces!Cone@Cône]  Cône 
$(t,\lambda)\mapsto (\lambda\cos^2t+\lambda\sin t, \lambda\sin t\cos t,2-2\lambda)$ et base.
\bigskip

\Remarque : On peut paramétrer un cône de sommet $A$ et de directrice $(I,\Gamma)$ par 
$$
\vec{OM}(t,\lambda)=\vec{OA}+\lambda\vec{A\Gamma}(t)\qquad(t\in I,\lambda\in\ob R).
$$

\Propriete [$(O,\vec i,\vec j,\vec k)$ repère affine] 
Une surface $S$, avec $S\neq\emptyset$ et $S\neq\{O\}$, est un cône de centre $O$ si, et seulement si, 
$$
(x,y,z)\in S\Longrightarrow \forall\lambda\in\ob R,\quad \lambda.(x,y,z)\in S.
$$

\Propriete [] Si une surface $S$ admet dans un repère $(O,\vec i,\vec j,\vec k)$ 
l'équation cartésienne  
$$
F(x,y,z)=0,
$$
alors $S$ est un cône de centre $O$ si, et seulement si, 
$$
F(x,y,z)=0\quad\Longrightarrow\quad \forall\lambda\in\ob R,\quad F(\lambda x,\lambda y,\lambda z)=0. 
$$

\Propriete 
Les cônes sont des surfaces développables. 

\Remarque : Si $M\neq A$ est un point régulier d'un cône de centre $A$, alors 
tous les points de la génératrice $(MA)$, distincts de $A$, sont réguliers. 
\bigskip
\noindent{\bf But : }Trouver le sommet (c'est l'intersection de deux droites contenues dans le cône). 
\bigskip

\Subsection null, Surfaces de révolution.

\Definition [] Une surface $S$ est de révolution autour d'une droite $\Delta$ si, 
et seulement~si, 
$S$ est globalement invariante par toutes les rotations d'axe $\Delta$. 


\Definition [] Si $S$ est de révolution autour d'une droite $\Delta$, 
les cercles obtenus par intersection de $S$ avec les plans othogonaux à $\Delta$ s'appellent les parallèles de $S$. 
\pn
L'intersection de $S$ avec un plan contenant $\Delta$ s'appelle une méridienne de $S$. \pn
L'intersection de $S$ avec un demi-plan de frontière $\Delta$ s'appelle 
une demi-méridienne~de~$S$.

\centerline{%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Revolution1.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Revolution2.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Revolution4.pdf}%
	\Image[Height=4cm,Box=hbox]{\imageFolder/Pdf/Revolution3.pdf}%
}%
\Figure [Index=Surfaces!de revolution@de révolution]  Surface de révolution, demi-méridienne et parallèle.
\bigskip

\Remarque : Soit $(O,\vec i,\vec j,\vec k)$ un repère orthonormé, 
$S$ une surface de révolution par rapport à $(O,\vec k)$ 
et une demi-méridienne $f:t\mapsto x(t)\vec i+y(t)\vec j+z(t)\vec k\ \,(t\in I)$. 
Alors on peut paramétrer $S$ par 
$$\Q\{\eqalign{X(t,\theta)&=x(t)\cos\theta- y(t)\sin\theta\cr
Y(t,\theta)&=x(t)\sin\theta+y(t)\cos\theta \cr
Z(t,\theta)&=z(t)}\W.\qquad(t\in I,\theta\in\ob R).
$$


\Propriete [$(O,\vec i,\vec j,\vec k)$ un repère orthonormé] 
Une surface $S$ est de~révolution par rapport à l'axe $(O,\vec k)$ si, et seulement si, 
$$
(x,y,z)\in S\Longrightarrow \forall\theta\in\ob R,\quad (r\cos\theta,r\sin\theta,z)\in S\mbox{ avec }r=\sqrt{x^2+y^2}.
$$

\Propriete [${\sc R=(O,\vec i,\vec j,\vec k)}$ un repère orthonormé] 
Une surface $S$ est de~révolution par rapport à l'axe $(O,\vec k)$ 
si, et seulement si, elle admet dans $\sc R$ une équation cartésienne 
du type 
$$
F\b(x^2+y^2,z\b)=0.
$$

\Remarque : On déduit facilement parallèles et méridiennes d'une telle équation. 
\bigskip

\Propriete [] Soit $M$ un point régulier d'une surface $S$ 
de révolution par rapport à $\Delta$. 
\pn
Si $M\in\Delta$, le plan tangent à $S$ est le plan perpendiculaire à $\Delta$ passant par $M$. 
\pn
Si $M\notin\Delta$ le plan tangent à $S$ en $M$ contient la tangente en $M$ 
à la parallèle de $S$ passant par $M$. 
\bigskip

\Section null, Quadriques.
\bigskip

\Subsection null, Définition et réduction.

\Definition [] Une surface $S$ de l'espace $\sc E$ est une quadrique $\Longleftrightarrow$ la surface 
$S$ admet dans un repère orthonormé $(O,\vec i,\vec j,\vec k)$ de $\sc E$ 
une équation cartésienne du type 
$$
\underbrace{a_1x^2+a_2y^2+a_3z^2+b_1yz+b_2xz+b_3xy}_{q(x,y,z)}+c_1x+c_2y+c_3z+d=0.
$$
avec $a_i,b_i,c_i,d\in\ob R$ pour $1\le i\le 3$ et $q\neq0$. 
\bigskip

\Remarque : Les coniques/quadriques sont aussi appelées courbes/surface algébriques du second degré. 
\bigskip

\noindent{\bf But. }On procède comme pour les coniques. On applique des rotations dans l'espace puis une translation 
au repère $(O,\vec i,\vec j,\vec k)$ pour obtenir un nouveau repère orthonormé 
$(C,\vec {e_1},\vec{e_2},\vec{e_3})$ dans lequelle la quadrique a une équation 
``réduite''. 
\bigskip 

\Propriete [] Soit $q:\ob R^n\to\ob R$ une forme quadratique 
de matrice associée $A$ et soit~$\sc B=\{e_1,\cdots,e_n\}$ 
une base orthonormale de vecteurs propres de la matrice $A$, associés 
aux valeurs propres $\{\lambda_1,\cdots,\lambda_n\}$. Alors, 
la matrice de $q$ dans $\sc B$ est 
$$
\pmatrix{\lambda_1&0&\cdots&0\cr
0&\lambda_2&\ddots&\vdots\cr
\vdots&\ddots&\ddots&0\cr
0&\cdots&0&\lambda_n\cr}.
$$
En particulier, pour chaque $x=\sum_{i=1}^nx_ie_i$ dans $\ob R^n$, on~a~alors 
$q(x)=\sum_{i=1}^n\lambda_ix_i^2$. 
\bigskip

\Methode [] 1) Réduire la forme quadratique $q$ : \vskip-.6em\noindent 
1a) Ecrire la matrice de la forme quadratique $q$, 
$\ds A=\pmatrix{a_1&{b_3\F2}& {b_2\F2}\cr {b_3\F2}&a_2&b_1\cr {b_2\F2}&{b_1\F2}& a_3\cr}$. 
\vskip-.6em\noindent 
1b) Trouver les valeurs propres $\lambda_1$, $\lambda_2$ et $\lambda_3$ de $A$. \pn
1c) Trouver une base orthonormale $\{\vec{\e_1},\vec{\e_2},\vec{e_3}\}$ 
de vecteurs propres associés aux $\lambda_i$. \pn
1d) Dans la nouvelle base ($x\vec i+y\vec j+z\vec k=X\vec{e_1}+Y\vec{e_2}+Z\vec{e_3}$), 
l'équation de $\sc C$ est 
$$
\lambda_1X^2+\lambda_2Y^2+\lambda_3Z^2+\alpha X+\beta Y+\delta Z+\gamma=0
$$ 
2) Faire une translation pour se débarasser (si possible) 
des termes en $X$, $Y$ et $Z$\pn
$$
X=X'+X_0, \qquad Y=Y'+Y_0\qquad\mbox{et}\qquad Z=Z'+Z_0
$$
3) Identifier la quadrique : ellipsoïde, paraboloïde, hyperboloïde 
(à $1$ ou $2$ nappes), cylindres, cônes, dégénérée 
($\emptyset$, $\{a\}$, droite, réunion de plan(s))... \pn
4) Identifier ses éléments caractéristiques 
\bigskip\goodbreak

\Subsection null, Ellipsoïde.

Soit $(a,b,c)\in\Q]0,\infty^2\W[$. Alors, la surface $\sc E$ d'équation cartésienne 
$$
{x^2\F a^2}+{y^2\F b^2}+{z^2\F c^2}=1 
$$
dans un repère orthonormé $(O,\vec i,\vec j, \vec k)$ 
est appelée ellipsoïde de centre $O$ et 
de~plans de symétries $(Oxy)$, $(Oxz)$ et $(Oyz)$. 

\centerline{%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Ellipsoide2.pdf}%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Ellipsoide1.pdf}%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Ellipsoide3.pdf}%
}%
\Figure [Index=Surfaces!Ellipsoide@Ellipsoïde]  Ellipsoïde.
\bigskip

\Remarque : Un paramètrage usuel de l'ellipsoïde $\sc E$ est 
$$
\Q\{\eqalign{x(\theta,\varphi)&=a\cos \varphi\cos\theta\cr y(\theta,\varphi)&=b\cos\varphi\sin\theta\cr 
z(\theta,\varphi)&=c\sin\varphi}\W.\qquad\Q(-\pi\le \theta\le\pi,-{\pi\F2}\le\varphi\le{\pi\F2}\W). 
$$

\Remarque : L'intersection de l'ellipsoïde avec un plan est une ellipse, 
un~point ou $\emptyset$. 
\bigskip

\Subsection null, Hyperboloïde à une nappe.

Soit $(a,b,c)\in\Q]0,\infty^2\W[$. Alors, la surface $\sc H$ d'équation cartésienne 
$$
{x^2\F a^2}+{y^2\F b^2}-{z^2\F c^2}=1 
$$
dans un repère orthonormé $(O,\vec i,\vec j, \vec k)$ 
est appelée hyperboloïde à une nappe de centre $O$ et 
de~plans de symétries $(Oxy)$, $(Oxz)$ et $(Oyz)$. 

\centerline{%
	\Image[Height=3.5cm,Box=hbox]{\imageFolder/Pdf/Hyperboloide2.pdf}%
	\Image[Height=3.5cm,Box=hbox]{\imageFolder/Pdf/Hyperboloide1.pdf}%
	\Image[Height=3.5cm,Box=hbox]{\imageFolder/Pdf/Hyperboloide3.pdf}%
}%
\Figure [Index=Surfaces!Hyperboloide@Hyperboloïde]  Hyperboloïde à une nappe.
\bigskip

\Remarque : L'hyperboloïde à une nappe admet pour les grandes valeurs de $z$ 
un cône asymptote d'équation cartésienne 
$$
{x^2\F a^2}+{y^2\F b^2}={z^2\F c^2}
$$
\smallskip

\Remarque : Quelques paramètrages usuels de l'hyperbolooïde à une nappe $\sc H$ : 
$$
\Q\{\eqalign{x(\theta,t)&=a\cos\theta\ch t\cr 
y(\theta,t)&=b\sin\theta\ch t\cr 
z(\theta,t)&=c\sh t}\W.\qquad\Q(-\pi\le \theta\le\pi,t\in\ob R\W). 
$$
$$
\Q\{\eqalign{x(\theta,\varphi)&=a{\cos\theta\F\cos\varphi}\cr 
y(\theta,\varphi)&=b{\sin\theta\F\cos\varphi}\cr 
z(\theta,\varphi)&=c\tan\varphi}\W.\qquad\Q(-\pi\le \theta\le\pi,-{\pi\F2}<\varphi<{\pi\F2}\W). 
$$


\Subsection null, Hyperboloïde à deux nappes.

Soit $(a,b,c)\in\Q]0,\infty^2\W[$. Alors, la surface $\sc H$ d'équation cartésienne 
$$
{x^2\F a^2}+{y^2\F b^2}-{z^2\F c^2}=-1 
$$
dans un repère orthonormé $(O,\vec i,\vec j, \vec k)$ 
est appelée hyperboloïde à deux nappes de centre $O$ et 
de~plans de symétries $(Oxy)$, $(Oxz)$ et $(Oyz)$. 

\centerline{%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Hyperboloided2.pdf}%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Hyperboloided3.pdf}%
	\Image[Height=3cm,Box=hbox]{\imageFolder/Pdf/Hyperboloided1.pdf}%
}%
\Figure [Index=Surfaces!Hyperboloide@Hyperboloïde]  Hyperboloïde à deux nappes.
\bigskip

\Remarque : L'hyperboloïde à deux nappes admet pour les grandes valeurs de $z$ 
un cône asymptote d'équation cartésienne 
$$
{x^2\F a^2}+{y^2\F b^2}={z^2\F c^2}
$$
\smallskip

\Remarque : Quelques paramètrages usuels de l'hyperbolooïde à deux nappes $\sc H$ : 
$$
\Q\{\eqalign{x(\theta,t)&=a\cos\theta\sh t\cr 
y(\theta,t)&=b\sin\theta\sh t\cr 
z(\theta,t)&=\pm c\ch t}\W.\qquad\Q(-\pi\le \theta\le\pi,t\in\ob R\W). 
$$
$$
\Q\{\eqalign{x(\theta,t)&=a{\cos\theta\F\sh t}\cr 
y(\theta,t)&=b{\sin\theta\F\sh t}\cr 
z(\theta,t)&=c\coth t}\W.\qquad\Q(-\pi\le \theta\le\pi,t\in\ob R^*\W). 
$$

\Subsection null, Paraboloïde elliptique.

Soit $(a,b)\in\Q]0,\infty^2\W[$. Alors, la surface $\sc P$ d'équation cartésienne 
$$
z={x^2\F a^2}+{y^2\F b^2} 
$$
dans un repère orthonormé $(O,\vec i,\vec j, \vec k)$ 
est appelée paraboloïde elliptique 
de~plans de symétries $(Oxz)$ et $(Oyz)$. 

\centerline{%
	\Image [Box=hbox,Width=3.2cm]{\imageFolder/Pdf/Paraboloide1.pdf}%
	\Image [Box=hbox,Width=3.2cm]{\imageFolder/Pdf/Paraboloide2.pdf}%
	\Image [Box=hbox,Width=3.2cm]{\imageFolder/Pdf/Paraboloide3.pdf}%
	\Image [Box=hbox,Width=3.2cm]{\imageFolder/Pdf/Paraboloide4.pdf}%
}%
\Figure [Index=Surfaces!Paraboloide elliptique@Paraboloïde elliptique] Paraboloïde elliptique.
\bigskip

\Remarque : Quelques paramètrages usuels du paraboloïde élliptique $\sc P$ : 
$$
\Q\{\eqalign{x(u,v)&=u\cr 
y(u,v)&=v\cr 
z(u,v)&={u^2\F a^2}+{v^2\F b^2}}\W.\qquad\Q(u\in\ob R,v\in\ob R\W). 
$$
$$
\Q\{\eqalign{x(\theta,t)&=at\cos\theta\cr 
y(\theta,t)&=bt\sin\theta\cr 
z(\theta,t)&=t^2}\W.\qquad\Q(-\pi<\theta\le\pi,t\ge0\W). 
$$

\Subsection null, Paraboloïde hyperbolique.

Soit $(a,b)\in\Q]0,\infty^2\W[$. Alors, la surface $\sc P$ d'équation cartésienne 
$$
z={x^2\F a^2}-{y^2\F b^2} 
$$
dans un repère orthonormé $(O,\vec i,\vec j, \vec k)$ 
est appelée paraboloïde hyperbolique 
de~plans de symétries $(Oxz)$ et $(Oyz)$. 

\centerline{%
	\Image [Box=hbox,Height=3.2cm]{\imageFolder/Pdf/Paraboloidee1.pdf}%
	\Image [Box=hbox,Height=3.2cm]{\imageFolder/Pdf/Paraboloidee2.pdf}%
	\Image [Box=hbox,Height=3.2cm]{\imageFolder/Pdf/Paraboloidee3.pdf}%
	\Image [Box=hbox,Height=3.2cm]{\imageFolder/Pdf/Paraboloidee4.pdf}%
}%
\Figure [Index=Surfaces!Paraboloide hyperbolique@Paraboloïde hyperbolique] Paraboloïde hyperbolique.
\bigskip
\Remarque : Quelques paramètrages usuels du paraboloïde élliptique $\sc P$ : 
$$
\Q\{\eqalign{x(u,v)&=u\cr 
y(u,v)&=v\cr 
z(u,v)&={u^2\F a^2}-{v^2\F b^2}}\W.\qquad\Q(u\in\ob R,v\in\ob R\W). 
$$
$$
\Q\{\eqalign{x(u,t)&=at\ch u\cr 
y(u,t)&=bt\sh u\cr 
z(u,t)&=t^2}\W.\qquad\mbox{et}\qquad\Q\{\eqalign{x(u,t)&=at\sh u\cr 
y(u,t)&=bt\ch u\cr 
z(u,t)&=-t^2}\W.\qquad\qquad\Q(u\in\ob R,t\in\ob R\W). 
$$




\Inferno{\input LD@Inferno@Macros.tex
\def\LD@List{\Surfaces\Aires\Quadriques}%
\def\LD@Font@Arial{}%

\Subsection go, Exercices.

\eightpts
\def\Chapter#1, #2.{}%
\LD@Exo@Theme@Display{1,2}\LD@List{%
	\Exercices,\Colles%,\Problèmes,\Others,\Mathematica,\Maple,\LD@Empty
}%
\eject
}


\hautspages{Olus Livius Bindus}{Méthodes et techniques}%

\Chapter cmet§, Méthodes et techniques. 


\Section cmet§rais, Raisonnements. 


\Subsection cmetǧrais, Récurrence.


Le raisonnement par récurrence consiste à établir la véracité d'une suite $\{\sc P_0, \sc P_1, \sc P_2, \sc P_3, \cdots\}$ de propositions logiques en procédant en deux étapes : 
\medskip
\noindent{\bf L'initialisation}. On établit la propriété au(x) premier(s) rang(s), de sorte à enclencher le mé\-cha\-ni\-sme de transmission. 
\medskip
\noindent{\bf La transmission}. On prouve que la propriété se transmet du (des) rang(s) précédent(s) au rang suivant. 
\medskip
\noindent
Le raisonnement par récurrence s'appuie donc sur l'un des schémas suivants (les variations sont infinies, ce qui est important est de bien en comprendre le fonctionnement) : 
\medskip
\Concept Récurrence forte

La proposition au rang $n$ induit la proposition au rang $n+1$. 

$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_6\cr
&\sbox{Transmission : }\sc P_n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad \sc P_6, \sc P_7, \sc P_8, \cdots
$$

\Concept Récurrence faible

Les propositions avant le rang $n$ induisent la proposition au rang $n+1$. 
$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_0\cr
&\sbox{Transmission : }\sc P_k \sbox{ pour }0\le k\le n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad \sc P_0, \sc P_1, \sc P_2, \cdots
$$

$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_4\cr
&\sbox{Transmission : }\sc P_k \sbox{ pour }4\le k\le n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad \sc P_4, \sc P_5, \sc P_6, \cdots
$$

\Concept Récurrence à $k$ pas

Les $k$ propositions consecutives avant le rang $n$ induisent la proposition au rang $n+1$ 
$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_3, \sc P_4\cr
&\sbox{Transmission : }\sc P_{n-1}\sbox{ et }\sc P_n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad\sc P_3, \sc P_4, \sc P_5, \sc P_6\cdots\leqno{(k=2)}
$$

$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_6, \sc P_7, \sc P_8\cr
&\sbox{Transmission : }\sc P_{n-2}, \sc P_{n-1}\sbox{ et }\sc P_n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad \sc P_6, \sc P_7, \sc P_8, \sc P_9\cdots\leqno{(k=3)}
$$


\Concept Récurrence finie

La récurence forte/faible/à plusieurs pas s'arrète, car la transmission ne se fait plus au dela d'un certain rang. 
$$\Q\{
\eqalign{
&\sbox{Initialisation : }\sc P_4\cr
&\sbox{Transmission : }\sbox{Pour }4\le n< 666, \sc P_n\Rightarrow\sc P_{n+1}}\W.
\qquad\sbox{induit}\qquad \sc P_4, \sc P_5, \cdots, \sc P_{665}, \sc P_{666}.
$$

\def\TipSection{Methodes@Méthodes}

\Section cmet§alglin, Ensembles et logique.        



\Tip{Ensembles!Prouver que $E\subset F$}%
\Methode[Pour prouver que $E\subset F$]
Prendre un élément quelconque de $E$ et montrer qu'il appartient à $F$. 
$$
\mbox{Fixons $x\in E$ et montrons que $x\in F$. }%\Pdf{\leqno{\LuciferStar}}
$$

\Invertedtrue
\Tip{Propositions!Prouver que $\sc P\Longrightarrow \sc Q$}
\Methode[Pour prouver que {$\sc P\Longrightarrow \sc Q$}]
Supposer la proposition $\sc P$ puis établir la proposition~$\sc Q$. 
$$
\mbox{Supposons $\sc P$ et montrons $\sc Q$.}\Pdf{\leqno{\LuciferStar}}
$$

\Application : si $f:\ob R\to\ob R$ est impaire, alors $f$ s'annule en $0$. 

\Demonstration. Supposons que $f$ soit impaire et montrons que $f(0)=0$. En remarquant que 
$$
\forall x\in\ob R, \qquad f(-x)=-f(x)
$$
et en appliquant cette relation pour $x=0$, nous obtenons que $f(-0)=-f(0)$ et donc que $f(0)=0$. \CQFD 

\Remarque : la valeur logique de l'implication $\sc P\Longrightarrow \sc Q$ ne dépend que des valeurs logiques de $\sc P$ et $\sc Q$ 
et non pas d'une éventuelle relation entre $\sc P$ et $\sc Q$. En particulier, les implications suivantes sont toutes vraies
$$
\eqalign{
&0=1\Longrightarrow 0=0\cr
&0\neq0\Longrightarrow \mbox{Mon prof de math est un extraterrestre}\cr
&0=0\Longrightarrow 1=1}
$$
il n'est pas toujours nécéssaire d'utiliser l'hypothèse $\sc P$ pour prouver $\sc Q$ ou de déduire $\sc Q$ de $\sc P$. 

\Tip{Ensembles!Prouver que $E=F$}
\Propriete [Pour prouver que $E=F$ (ensembles)]
On procède par double-inclusion
$$
E\subset F\qquad \sbox{et} \qquad F\subset E
$$

\Invertedtrue
\Tip{Propositions!Prouver que $A\Longleftrightarrow B$}
\Propriete[Pour prouver que $A\Longleftrightarrow B$]
On procède par double-implication
$$
A\Longrightarrow B\qquad\mbox{et}\qquad B\Longrightarrow A.
$$

\Exercice{PTaio}%

\Section cmet§fonc, Fonctions.  

\Tip{Fonctions!Prouver que $f$ est injective}\Tip{Fonctions!Prouver que $f$ est surjective}\Tip{Fonctions!Prouver que $f$ est bijective}
\Propriete [Pour savoir si $f:E\to F$ est injective (resp. surjective, resp. bijective)] 
On fixe un élément quelconque $y\in F$ et l'on regarde si l'équation $y=f(x)$ a au plus (resp au moins, resp. exactement) une solution $x\in E$. 


\Section cmet§nbcplxs, Nombres complexes. 

\Tip{Calculs!Factoriser $\e^{i\alpha}\pm\e^{i\varphi}$}

\Propriete [Pour factoriser $\e^{i\alpha}\pm\e^{i\varphi}$] 
On factorise l'exponentielle $\e^{i(\alpha+\beta)/2}$ de la moyenne des angles $\alpha$ et $\beta$. 
$$
\eqalign{
&\e^{i\alpha}+\e^{i\beta}=\Q(\e^{i(\alpha-\beta)/2}+\e^{i(\beta-\alpha)/2}\W)\e^{i(\alpha+\beta)/2}=2\cos\Q({\alpha-\beta\F2}\W)\e^{i(\alpha+\beta)/2},\cr
&\e^{i\alpha}-\e^{i\beta}=\Q(\e^{i(\alpha-\beta)/2}-\e^{i(\beta-\alpha)/2}\W)\e^{i(\alpha+\beta)/2}=2i\sin\Q({\alpha-\beta\F2}\W)\e^{i(\alpha+\beta)/2}.
}
$$

\Section cmet§alglin, Algèbre linéaire.  


\Tip{Ensembles!Prouver que $E$ est un $\ob K$-EV}
\Propriete [Pour prouver que $E$ est un $\ob K$-espace vectoriel]
On montre que $E$ est un sous-espace vectoriel d'un $\ob K$-espace vectoriel~connu
$$
E\neq\emptyset, \qquad\quad E\subset F\quad \ob K-\sbox{EV connu},\qquad\quad \forall(\lambda,\mu)\in \ob K^2, \quad \forall (x,y)\in E^2, \quad \lambda x+\mu y\in E.
$$

\Exercice{PTaim}%


\Tip{Ensembles!Prouver que $E=F$}
\Propriete [Pour prouver que $E=F$ ($\ob K$-espaces vectoriels {\bf de dimension finie})]
On montre une inclusion ainsi que l'égalité des dimensions
$$
 E\subset F \qquad \sbox{et}\qquad \dim E=\dim F
$$

\Exercice{PTaip}%


\Tip{Familles!Prouver que $\sc F$ est libre}
\Propriete [Pour prouver qu'une famille ${\sc F=\{e_1, \cdots e_n\}}$ de vecteurs est libre] 
On prend une combinaison linéaire nulle de ses vecteurs, puis l'on montre que ses coefficients sont tous nuls, 
$$\sc F\sbox{ libre }\quad\sbox{ se traduit symboliquement par }\quad\lambda_1 e_1+\cdots+\lambda_n e_n=0\Leftrightarrow\lambda_1=\cdots=\lambda_n=0.
$$

\Exercice{PTair}


\Tip{Fonctions!Prouver que $f$ est linéaire}
\Propriete [Pour prouver que $f:E\to F$ est une application linéaire] 
On prouve d'abord que $f$ est une application (que tout élément de $E$ a une image dans~$F$), 
entre espaces vectoriels, puis qu'elle est linéaire. 
$$
\eqalign{
&\forall x\in E, \quad f(x) \sbox{ est défini et appartient à }F, \qquad E\sbox{ et }F\sbox{ $\ob K$-EV}, \cr
&\forall(\lambda,\mu)\in \ob K^2, \quad \forall (x,y)\in E^2, \qquad f(\lambda x+\mu y)=\lambda f(x)+\mu f(y).
}
$$

\Exercice{PTain}

\Tip{Fonctions!Prouver que $f$ est injective}
\Propriete [Pour savoir si une application linéaire $f:E\to F$ est injective] 
On regarde son noyau
$$
f\sbox{ injective}\ssi \ker f=\{0\}.
$$

\Tip{Fonctions!Prouver que $f$ est surjective}

\Propriete [Pour savoir si une application linéaire $f:E\to F$ est surjective]
On se ramène, via la dimension, à une étude d'injectivité ou de noyau de $f$ : \pn
Si $E$ et $F$ ont la même dimension {\bf finie}, 
$$
f\sbox{ injective }\ssi f\sbox{ surjective }\ssi f \sbox{ bijective.}
$$ 
Si $E$ est de dimension finie, différente de celle de $F$, on utilise le théorème du rang
$$
\dim E=\dim\IM f+\dim\ker f.
$$
Si $E$ est de dimension infinie, on se ramène à la dimension finie, en utilisant des restrictions, ou l'on essaie une autre méthode. 

\Exercice{PTaiq}



\hautspages{Olus Livius Bindus}{Applications}%
\Chapter exos§, Applications. 
\bigskip
\Section exos§exos, Exercices.


\Exercice{PTais}


\Section exos§prob, Problèmes. 

\noindent Problème d'algèbre linéaire (nécéssite Espaces vectoriels, dimension et polynômes). 
\medskip
\def\LD@Maths@Exercice@Text{\underline{\bf Probleme}}%
\Exercice{PTait}%

\Exercice{PTSIxk}%

\Section exos§cor, Corrigés. 

\def\LD@Maths@Solution@Text{%
	{\bf Corrigé du problème \EA\refn\EA{labelexo\LD@Maths@Label@Internal}.}\PAR
}%
\Solution{PTSIxk}%
 
\vfill\null
\eject
\Chapter Annexes, Annexes.

\Section alphabet grec, Alphabet Grec.

\centerline{\vbox{
\halign{#\qquad&
$\displaystyle #$\qquad&$#$\qquad\cr
Lettre&\mbox{minuscule}&\mbox{Majuscule}\cr
Alpha&\alpha& A\cr
Beta& \beta& B\cr
Gamma&\gamma & \Gamma\cr
Delta&\delta&\Delta\cr
Epsilon&\epsilon \mbox{ ou }\varepsilon& E\cr
Zeta&\zeta&Z \cr
Eta&\eta& H\cr
Theta&\theta\mbox{ ou }\vartheta& \Theta\cr
Iota&\iota&I\cr
Kappa&\kappa&K\cr
Lambda&\lambda&\Lambda\cr
Mu&\mu&M\cr
Nu&\nu&N\cr
Xi&\xi&\Xi\cr
Omicron&o&O\cr
Pi&\pi&\Pi\cr
Rho&\rho\mbox{ ou }\varrho& P\cr
Sigma&\sigma\mbox{ ou }\varsigma&\Sigma\cr
Tau&\tau&T\cr
Upsilon&\upsilon&Y\cr
Phi&\phi\mbox{ ou }\varphi&\Phi\cr
Chi&\chi&X\cr
Psi&\psi&\Psi\cr
Omega&\omega&\Omega\cr
}}}



\eject
\Chapter Index, Index. 

\readindexfile{i}


\bye
